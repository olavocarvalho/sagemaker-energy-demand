{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Energy Demand\n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "The project consists of two data sets:\n",
    "* Hourly electricity demand data from the EIA;\n",
    "* Hourly observed weather data from LCD/NOAA. \n",
    "\n",
    "Additionally to demand and weather data, I'll create features based on time to see how the trends are impacted by day of week, hour, time of year, if is holiday, etc.\n",
    "\n",
    "To limit the scope of the project, I'll use data from Los Angeles exclusively to validate if is possible to improve electricity demand forecasting using weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket ='sagemaker-data-energy-demand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_CLIENT = boto3.client('s3')\n",
    "files_list = S3_CLIENT.list_objects_v2(Bucket=bucket, Prefix='raw_data/weather/')\n",
    "s3_files = files_list['Contents']        \n",
    "latest_weather_data = max(s3_files, key=lambda x: x['LastModified'])\n",
    "\n",
    "weather_data_location = 's3://{}/{}'.format(bucket, latest_weather_data['Key'])\n",
    "print(weather_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electricity data \n",
    "Electricity data were retrieved using EIA’s API and then unpacked into a dataframe. The API contain hourly entries from July 2015 to present.\n",
    "\n",
    "The electricity data required just simple cleaning. There were few null values in the set and a very small number of outliers. Removing outliers cut only ~.01% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIA__API_KEY = '1d48c7c8354cc4408732174250d3e8ff'\n",
    "REGION_CODE = 'LDWP'\n",
    "CITY = 'LosAngeles'\n",
    "\n",
    "def str_to_isodatetime(string):\n",
    "    year = string[:4]\n",
    "    month = string[4:6]\n",
    "    day =  string[6:8]\n",
    "    time = string[8:11] + ':00:00+0000'\n",
    "    return year + month + day + time\n",
    "\n",
    "def eia2dataframe(response):\n",
    "    '''\n",
    "    This function unpacks the JSON file from EIA API into a pandas dataframe.\n",
    "    '''\n",
    "    data = response['series'][0]['data']\n",
    "    dates = []\n",
    "    values = []\n",
    "    for date, demand in data:\n",
    "        if demand is None or demand <= 0:\n",
    "            dates.append(str_to_isodatetime(date))\n",
    "            values.append(np.nan)      \n",
    "            continue   \n",
    "        dates.append(str_to_isodatetime(date))\n",
    "        values.append(float(demand))\n",
    "    df = pd.DataFrame({'datetime': dates, 'demand': values})\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df['demand'] = df['demand'].interpolate()\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df.sort_index(ascending=True, inplace=True, kind='mergesort')\n",
    "    return df\n",
    "\n",
    "electricity_api_response = requests.get('http://api.eia.gov/series/?api_key=%s&series_id=EBA.%s-ALL.D.H' % (EIA__API_KEY, REGION_CODE)).json()\n",
    "electricity_df = eia2dataframe(electricity_api_response)\n",
    "electricity_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observed weather data\n",
    "LCD data are not available via NOAA’s API so I manually downloaded from the website as a CSV file which I imported to a pandas DataFrame. As common in data that come from physical sensors, LCD data required extensive cleansing.\n",
    "\n",
    "The main challenges in cleaning the LCD data was that there were in some cases multiple entries for the same hour. I wanted to have just one entry per hour such that I could eventually align LCD data with the hourly entries in the electricity data.\n",
    "\n",
    "I wrote a function that group weather data by hour and the mode of the entries for same hour. I performed the cleaning this way because either way, the values for multiple per-hour entries are very similar, so the choice of which entry to keep doesn’t make a real difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_calc(x):\n",
    "    try:\n",
    "        print(stats.mode(x.dropna())[1][0])\n",
    "        return stats.mode(x.dropna())[1][0]\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "\n",
    "def fix_date(df):\n",
    "    '''\n",
    "    This function goes through the dates in the weather dataframe and if there is more than one record for each\n",
    "    hour, we pick the record closest to the hour and drop the rows with the remaining records for that hour.\n",
    "    This is so we can align this dataframe with the one containing electricity data.'''\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize('UTC')\n",
    "    df['date_rounded'] = df['date'].dt.floor('H')\n",
    "    df.drop('date', axis=1, inplace=True)\n",
    "    df.rename({\"date_rounded\": \"datetime\"}, axis=1, inplace=True)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    # grouped_by_hour = df.groupby(['date_rounded']).head(-1)\n",
    "    grouped_by_hour = df[~df.index.duplicated(keep='last')]\n",
    "    print(grouped_by_hour.dtypes)\n",
    "    \n",
    "    grouped_by_hour.sort_index(ascending=True, inplace=True, kind='mergesort')\n",
    "    print(grouped_by_hour.shape)\n",
    "    \n",
    "    return grouped_by_hour\n",
    "    \n",
    "def clean_sky_condition(df):\n",
    "    '''\n",
    "    This function cleans the hourly sky condition column by assigning the hourly sky condition to be the one at the\n",
    "    top cloud layer, which is the best determination of the sky condition, as described by the documentation.\n",
    "    '''\n",
    "    conditions = df['hourlyskyconditions']\n",
    "    new_condition = []\n",
    "    for k, condition in enumerate(conditions):\n",
    "        if type(condition) != str and np.isnan(condition):\n",
    "            new_condition.append(np.nan)\n",
    "        else:\n",
    "            colon_indices = [i for i, char in enumerate(condition) if char == ':']\n",
    "            n_layers = len(colon_indices)\n",
    "            try:\n",
    "                colon_position = colon_indices[n_layers - 1]\n",
    "                if condition[colon_position - 1] == 'V':\n",
    "                    condition_code = condition[colon_position - 2 : colon_position]\n",
    "                else:\n",
    "                    condition_code = condition[colon_position - 3 : colon_position]\n",
    "                new_condition.append(condition_code)\n",
    "            except:\n",
    "                new_condition.append(np.nan)\n",
    "\n",
    "    df['hourlyskyconditions'] = new_condition\n",
    "    df['hourlyskyconditions'] = df['hourlyskyconditions'].astype('category')\n",
    "    return df\n",
    "\n",
    "def hourly_degree_days(df):\n",
    "    '''\n",
    "    This function adds hourly heating and cooling degree days to the weather DataFrame.\n",
    "    '''\n",
    "    df['hourlycoolingdegrees'] = df['hourlydrybulbtemperature'].apply(lambda x: x - 65. if x >= 65. else 0.)\n",
    "    df['hourlyheatingdegrees'] = df['hourlydrybulbtemperature'].apply(lambda x: 65. - x if x <= 65. else 0.)\n",
    "    return df\n",
    "\n",
    "# import csv\n",
    "weather_df = pd.read_csv(weather_data_location, usecols=['DATE', 'DailyCoolingDegreeDays', 'DailyHeatingDegreeDays', 'HourlyDewPointTemperature', 'HourlyPrecipitation', 'HourlyRelativeHumidity', 'HourlySeaLevelPressure', 'HourlySkyConditions', 'HourlyStationPressure', 'HourlyVisibility', 'HourlyDryBulbTemperature', 'HourlyWindSpeed'],\n",
    "                        dtype={\n",
    "                                'DATE': object,\n",
    "                                'DailyCoolingDegreeDays': object,\n",
    "                                'DailyHeatingDegreeDays': object,\n",
    "                                'HourlyDewPointTemperature': object,\n",
    "                                'HourlyPrecipitation': object,\n",
    "                                'HourlyRelativeHumidity': object,\n",
    "                                'HourlySeaLevelPressure': object,\n",
    "                                'HourlySkyConditions': object,\n",
    "                                'HourlyStationPressure': object,\n",
    "                                'HourlyVisibility': object,\n",
    "                                'HourlyDryBulbTemperature': object,\n",
    "                                'HourlyWindSpeed': object\n",
    "                        })\n",
    "\n",
    "# make columns lowercase for easier access\n",
    "weather_df.columns = [col.lower() for col in weather_df.columns]\n",
    "\n",
    "# clean dataframe so that there's only one record per hour\n",
    "weather_df = fix_date(weather_df)\n",
    "\n",
    "# fill the daily heating and cooling degree days such that each hour in an individual day has the same value\n",
    "weather_df['dailyheatingdegreedays'] = weather_df['dailyheatingdegreedays'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "weather_df.dailyheatingdegreedays.astype('float64')\n",
    "weather_df['dailycoolingdegreedays'] = weather_df['dailycoolingdegreedays'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "weather_df.dailycoolingdegreedays.astype('float64')\n",
    "weather_df['dailyheatingdegreedays'] = weather_df['dailyheatingdegreedays'].bfill()\n",
    "weather_df['dailycoolingdegreedays'] = weather_df['dailycoolingdegreedays'].bfill()\n",
    "\n",
    "weather_df = clean_sky_condition(weather_df)\n",
    "\n",
    "# clean other columns by replacing string based values with floats\n",
    "# values with an 's' following indicate uncertain measurments. we simply change those to floats and include them like normal\n",
    "weather_df['hourlyvisibility'] = weather_df['hourlyvisibility'].apply(lambda x: float(x) if str(x)[-1] != 'V' else float(str(x)[:-1]))\n",
    "\n",
    "weather_df['hourlydrybulbtemperature'] = weather_df['hourlydrybulbtemperature'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "\n",
    "weather_df['hourlydewpointtemperature'] = weather_df['hourlydewpointtemperature'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "\n",
    "# set trace amounts equal to zero and change data type\n",
    "weather_df['hourlyprecipitation'].where(weather_df['hourlyprecipitation'] != 'T', 0.0, inplace=True)\n",
    "weather_df['hourlyprecipitation'] = weather_df['hourlyprecipitation'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "weather_df['hourlystationpressure'] = weather_df['hourlystationpressure'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "weather_df['hourlywindspeed'] = weather_df['hourlywindspeed'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "weather_df['hourlyrelativehumidity'] = weather_df['hourlyrelativehumidity'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "weather_df['hourlysealevelpressure'] = weather_df['hourlysealevelpressure'].apply(lambda x: float(x) if str(x)[-1] != 's' else float(str(x)[:-1]))\n",
    "\n",
    "weather_df.hourlyprecipitation.astype('float64')\n",
    "weather_df.hourlyvisibility.astype('float64')\n",
    "weather_df.hourlyrelativehumidity.astype('float64')\n",
    "weather_df.hourlysealevelpressure.astype('float64')\n",
    "weather_df.hourlystationpressure.astype('float64')\n",
    "weather_df.hourlywindspeed.astype('float64')\n",
    "\n",
    "weather_df = hourly_degree_days(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.hourlyrelativehumidity.astype('float64')\n",
    "weather_df.hourlysealevelpressure.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** min ***')\n",
    "print(min(electricity_df.index))\n",
    "print(min(weather_df.index))\n",
    "print('*** max ***')\n",
    "print(max(electricity_df.index))\n",
    "print(max(weather_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cut dataframes based on date to align sources\n",
    "cut_electricity = electricity_df[:weather_df.index.max()]\n",
    "cut_weather = weather_df[cut_electricity.index.min():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*** min ***')\n",
    "print(min(cut_electricity.index))\n",
    "print(min(cut_weather.index))\n",
    "print(cut_weather.index.min() == cut_electricity.index.min())\n",
    "print('*** max ***')\n",
    "print(max(cut_electricity.index))\n",
    "print(max(cut_weather.index))\n",
    "print(cut_weather.index.max() == cut_electricity.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_set = set(cut_electricity.index)\n",
    "weather_set = set(cut_weather.index)\n",
    "print(len(weather_set.difference(electricity_set)))\n",
    "weather_set.difference(electricity_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with outliers and NaN values\n",
    "\n",
    "The plot distributions bof the features below is used to determine what columns should be filled by using the median\n",
    "and which should be filled according to ffill. The features whose ```medians``` and ```means``` are close together suggest that the ```median``` is a good choice for NaNs.Conversely features whose median and means are further apart suggest the presence of outliers and in this case I use ```ffill``` because we are dealing with time series and values in previous time steps are useful in predicting values for later time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms and violin plots as well as some stats for electricity data\n",
    "for col in electricity_df.columns:\n",
    "    fig,ax = plt.subplots(2, sharex=True)\n",
    "    plot_data = electricity_df[col][~electricity_df[col].isnull()]\n",
    "    ax[0].hist(plot_data, bins=15)\n",
    "    title_text = r'$\\sigma = %.2f$ MWh, $\\mu = %.2f$ MWh, median $= %.2f$ MWh' % (plot_data.std(), plot_data.mean(), plot_data.median())\n",
    "    sns.violinplot(plot_data, ax=ax[1])\n",
    "    ax[0].set_ylabel('Count', labelpad=5)\n",
    "    ax[1].set_xlabel('%s [MWh]' % col)\n",
    "    ax[0].set_title(title_text, size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# plot histograms and violin plots as well as some stats for weather data\n",
    "for col in weather_df.columns:\n",
    "    if col=='hourlyskyconditions': continue\n",
    "    elif col=='hourlyprecip':\n",
    "        title_text = r'$\\sigma = %.2f$, $\\mu = %.4f$, median $= %.4f$' % (plot_data.std(), plot_data.mean(), plot_data.median())\n",
    "    else:\n",
    "        title_text = r'$\\sigma = %.2f$, $\\mu = %.2f$, median $= %.2f$' % (plot_data.std(), plot_data.mean(), plot_data.median())\n",
    "    plot_data = weather_df[col][~weather_df[col].isnull()]\n",
    "    fig,ax = plt.subplots(2, sharex=True)\n",
    "    ax[0].hist(plot_data, bins=15)\n",
    "    sns.violinplot(plot_data, ax=ax[1])\n",
    "    ax[0].set_ylabel('Count', labelpad=5)\n",
    "    ax[1].set_xlabel('%s' % col)\n",
    "    ax[0].set_title(title_text, size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# plot bar plot for categorical value\n",
    "weather_df['hourlyskyconditions'].value_counts().plot(kind='bar')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_dict = {'median': ['dailyheatingdegreedays', 'hourlyaltimetersetting', 'hourlydrybulbtemperature', 'hourlyprecipitation', 'hourlysealevelpressure', 'hourlystationpressure', 'hourlywetbulbtempf', 'dailycoolingdegreedays', 'hourlyvisibility', 'hourlywindspeed', 'hourlycoolingdegrees', 'hourlyheatingdegrees'], 'ffill': ['demand', 'hourlydewpointtempf', 'hourlyrelativehumidity']}\n",
    "\n",
    "# fill electricity data NaNs\n",
    "for col in cut_electricity.columns:\n",
    "    if col in fill_dict['median']:\n",
    "        cut_electricity[col].fillna(cut_electricity[col].median(), inplace=True)\n",
    "    else:\n",
    "        cut_electricity[col].fillna(cut_electricity[col].ffill(), inplace=True)\n",
    "\n",
    "# fill weather data NaNs\n",
    "for col in cut_weather.columns:\n",
    "    if col == 'hourlyskyconditions':\n",
    "        cut_weather[col].fillna(cut_weather[col].value_counts().index[0], inplace=True) \n",
    "    elif col in fill_dict['median']:\n",
    "        cut_weather[col].fillna(cut_weather[col].median(), inplace=True)\n",
    "    else:\n",
    "        cut_weather[col].fillna(cut_weather[col].ffill(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally merge the data to get a complete dataframe for LA, ready for training\n",
    "merged_df = cut_weather.merge(cut_electricity, right_index=True, left_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.get_dummies(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(['hourlyskyconditions_', 'hourlyskyconditions_VV'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = calendar()\n",
    "holidays = cal.holidays(start=merged_df.index.min(), end=merged_df.index.max())\n",
    "\n",
    "def create_timeseries_features(df):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['date'] = df.index\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    df['isbusinessday']=np.where(df.date.dt.weekday_name.isin(['Saturday','Sunday']),0,1)\n",
    "    df['isholiday'] = df['date'].isin(holidays).astype(int)\n",
    "    df['daylight'] = [1 if (hour >= 6 and hour <= 18) else 0 for hour in df['date'].dt.hour]\n",
    "    return df\n",
    "\n",
    "\n",
    "merged_df = create_timeseries_features(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv file to continue in another notebook\n",
    "csv_buffer = io.StringIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "key = 'dataframes/%s_dataset.csv' % CITY\n",
    "\n",
    "merged_df.to_csv(csv_buffer, compression=None)\n",
    "s3_resource.Object(bucket, key).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
