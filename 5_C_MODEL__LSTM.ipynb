{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AWS and Sagemaker SDKs and get files access\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket ='sagemaker-data-energy-demand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "\n",
    "CITY = 'LosAngeles'\n",
    "train_key = 'dataframes/%s/train/data_train.csv' % CITY\n",
    "validation_key = 'dataframes/%s/test/data_validation.csv' % CITY\n",
    "\n",
    "train_location = 's3://{}/{}'.format(bucket, train_key)\n",
    "validation_location = 's3://{}/{}'.format(bucket, validation_key)\n",
    "\n",
    "df = pd.read_csv(train_location, index_col='datetime', parse_dates=True)\n",
    "df_test = pd.read_csv(validation_location, index_col='datetime', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predict_max_date = str(df_test.tail(1).index.values[0]).replace('T', ' ')        \n",
    "datetime_predict_max = datetime.strptime(dt_predict_max_date[:-10], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "dt_dataset_start = str(df.head(1).index.values[0]).replace('T', ' ')        \n",
    "datetime_dt_dataset_start = datetime.strptime(dt_dataset_start[:-10], '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "split_validation_date = datetime_predict_max - timedelta(days=15)\n",
    "split_validation_date_str = str(split_validation_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeseries consists hourly Data and we need to predict hourly rental count\n",
    "freq='H' \n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 15 days worth of hourly forecast \n",
    "prediction_length = 15 * 24 \n",
    "\n",
    "# AWS recommends setting context same as prediction length as a starting point\n",
    "# This controls how far in the past the network can see\n",
    "context_length = prediction_length\n",
    "\n",
    "dt_predict_max = pd.Timestamp(datetime_predict_max, freq=freq)\n",
    "\n",
    "dt_dataset_start_time = pd.Timestamp(datetime_dt_dataset_start, freq=freq)\n",
    "dt_dataset_end_time = pd.Timestamp(split_validation_date, freq=freq)\n",
    "\n",
    "# use for model training\n",
    "# Start time is the first row provided by dataset\n",
    "# Training TS end time ensures some data is withheld for model testing\n",
    "# 12 days worth of training data is withheld for testing\n",
    "dt_train_range = (dt_dataset_start_time,\n",
    "                  dt_dataset_end_time - timedelta(hours=15*24) )\n",
    "\n",
    "# Use entire data for testing\n",
    "# We can compare predicted values vs actual (i.e. last 12 days is withheld for testing and model hasn't seen that data)\n",
    "dt_test_range = (dt_dataset_start_time, \n",
    "                 dt_dataset_end_time) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_features = [\n",
    " 'dailycoolingdegreedays(t-1)',\n",
    " 'dailyheatingdegreedays(t-1)',\n",
    " 'hourlydewpointtemperature(t-1)',\n",
    " 'hourlydrybulbtemperature(t-1)',\n",
    " 'hourlyprecipitation(t-1)',\n",
    " 'hourlyrelativehumidity(t-1)',\n",
    " 'hourlyvisibility(t-1)',\n",
    " 'hourlywindspeed(t-1)',\n",
    " 'hourlycoolingdegrees(t-1)',\n",
    " 'hourlyheatingdegrees(t-1)',\n",
    " 'hourlyskyconditions_BKN(t-1)',\n",
    " 'hourlyskyconditions_FEW(t-1)',\n",
    " 'hourlyskyconditions_SCT(t-1)',\n",
    " 'hour(t-1)',\n",
    " 'hour(t)',\n",
    " 'dayofweek(t)',\n",
    " 'quarter(t)',\n",
    " 'month(t)',\n",
    " 'year(t)',\n",
    " 'dayofyear(t)',\n",
    " 'dayofmonth(t)',\n",
    " 'weekofyear(t)',\n",
    " 'isbusinessday(t)',\n",
    " 'isholiday(t)',\n",
    " 'daylight(t)']\n",
    "target_values = ['demand(t)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if there are gaps in timesteps\n",
    "def is_missing_steps(df,start,end,freq='D'):\n",
    "    dt_range = pd.date_range(start=start,end=end,freq=freq)\n",
    "    return not dt_range.equals(df[start:end].index)\n",
    "\n",
    "def get_missing_steps(df,start,end,freq='D'):\n",
    "    dt_range = pd.date_range(start=start,end=end,freq=freq)\n",
    "    return dt_range.difference(df[start:end].index)    \n",
    "\n",
    "# List timeseries with only NaNs\n",
    "# They can be removed\n",
    "def timeseries_with_only_nans(df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        if pd.isna(df[col].min()):\n",
    "            #print (col)\n",
    "            l.append(col)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex([], dtype='datetime64[ns, UTC]', freq='H')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_missing_steps(df,df.index.min(),df.index.max(),'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('1h').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_dict = {'median': ['dailyheatingdegreedays', 'hourlyaltimetersetting', 'hourlydrybulbtemperature', 'hourlyprecipitation', 'hourlysealevelpressure', 'hourlystationpressure', 'hourlywetbulbtempf', 'dailycoolingdegreedays', 'hourlyvisibility', 'hourlywindspeed', 'hourlycoolingdegrees', 'hourlyheatingdegrees'], 'ffill': ['demand', 'hourlydewpointtemperature', 'hourlyrelativehumidity']}\n",
    "\n",
    "# fill weather data NaNs\n",
    "for col in df.columns:\n",
    "    if col == 'hourlyskyconditions':\n",
    "        df[col].fillna(df[col].value_counts().index[0], inplace=True) \n",
    "    elif col in fill_dict['median']:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(df[col].ffill(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demand(t)                         0\n",
       "dailycoolingdegreedays(t-1)       0\n",
       "dailyheatingdegreedays(t-1)       0\n",
       "hourlydewpointtemperature(t-1)    0\n",
       "hourlydrybulbtemperature(t-1)     0\n",
       "hourlyprecipitation(t-1)          0\n",
       "hourlyrelativehumidity(t-1)       0\n",
       "hourlyvisibility(t-1)             0\n",
       "hourlywindspeed(t-1)              0\n",
       "hourlycoolingdegrees(t-1)         0\n",
       "hourlyheatingdegrees(t-1)         0\n",
       "hourlyskyconditions_BKN(t-1)      0\n",
       "hourlyskyconditions_FEW(t-1)      0\n",
       "hourlyskyconditions_SCT(t-1)      0\n",
       "hour(t-1)                         0\n",
       "hour(t)                           0\n",
       "dayofweek(t)                      0\n",
       "quarter(t)                        0\n",
       "month(t)                          0\n",
       "year(t)                           0\n",
       "dayofyear(t)                      0\n",
       "dayofmonth(t)                     0\n",
       "weekofyear(t)                     0\n",
       "isbusinessday(t)                  0\n",
       "isholiday(t)                      0\n",
       "daylight(t)                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dynamic_feat = df[dynamic_features]\n",
    "\n",
    "\n",
    "time_series_test = []\n",
    "time_series_training = []\n",
    "\n",
    "for t in target_values:\n",
    "    time_series_test.append(df[dt_test_range[0]:dt_test_range[1]][t])\n",
    "    time_series_training.append(df[dt_train_range[0]:dt_train_range[1]][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic features are the same for count, registered, casual\n",
    "dynamic_features_test = df_dynamic_feat [dt_test_range[0]:dt_test_range[1]]\n",
    "dynamic_features_training = df_dynamic_feat[dt_train_range[0]:dt_train_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_series_test[0].plot(label='test')\n",
    "time_series_training[0].plot(label='train')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]  \n",
    "\n",
    "def encode_dynamic_feat(dynamic_feat):  \n",
    "    l = []\n",
    "    for col in dynamic_feat:\n",
    "        assert (not dynamic_feat[col].isna().any()), col  + ' has NaN'             \n",
    "        l.append(dynamic_feat[col].tolist())\n",
    "    return l\n",
    "\n",
    "def series_to_obj(ts, cat=None, dynamic_feat=None):\n",
    "    obj = {\"start\": str(ts.index[0])[:-6], \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = encode_dynamic_feat(dynamic_feat)\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None, dynamic_feat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat, dynamic_feat))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"start\": \"2015-07-01 09:00:00\", \"target\": [3045.0, 2892.0, 2787.0, 2790.0, 2899.0], \"dynamic_feat\": [[11.0, 11.0, 11.0, 11.0, 11.0], [0.0, 0.0, 0.0, 0.0, 0.0], [63.0, 64.0, 63.0, 61.0, 61.0], [74.0, 77.0, 82.0, 84.0, 81.0], [0.0, 0.0, 0.0, 0.0, 0.0], [69.0, 64.0, 53.0, 46.0, 51.0], [10.0, 10.0, 10.0, 10.0, 10.0], [0.0, 0.0, 0.0, 3.0, 3.0], [9.0, 12.0, 17.0, 19.0, 16.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [8.0, 9.0, 10.0, 11.0, 12.0], [9.0, 10.0, 11.0, 12.0, 13.0], [2.0, 2.0, 2.0, 2.0, 2.0], [3.0, 3.0, 3.0, 3.0, 3.0], [7.0, 7.0, 7.0, 7.0, 7.0], [2015.0, 2015.0, 2015.0, 2015.0, 2015.0], [182.0, 182.0, 182.0, 182.0, 182.0], [1.0, 1.0, 1.0, 1.0, 1.0], [27.0, 27.0, 27.0, 27.0, 27.0], [1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0]]}'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_to_jsonline(time_series_training[0][:5], dynamic_feat=dynamic_features_training[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "with open(\"train_dynamic_feat.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts,dynamic_feat=dynamic_features_training).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_dynamic_feat.json\", 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts,dynamic_feat=dynamic_features_test).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data_dynamic_feat.csv',index=True,index_label='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39447 demand(t)\n"
     ]
    }
   ],
   "source": [
    "for ts in time_series_test:\n",
    "    print (len(ts),ts.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39087 demand(t)\n"
     ]
    }
   ],
   "source": [
    "for ts in time_series_training:\n",
    "    print (len(ts),ts.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_categories = False\n",
    "# Set a good base job name when building different models\n",
    "# It will help in identifying trained models and endpoints\n",
    "base_job_name = 'deepar-energy-demand-dynamic-features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'deepar/energy-demand'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "s3_data_path = \"{}/{}/data_dynamic\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sagemaker-data-energy-demand/deepar/energy-demand/data_dynamic',\n",
       " 'sagemaker-data-energy-demand/deepar/energy-demand/output')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path,s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across\n",
    "# three different availability zones in the region where the bucket was created.\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more training files and test files to S3\n",
    "write_to_s3('train_dynamic_feat.json',bucket,'deepar/energy-demand/data_dynamic/train/train_dynamic_feat.json')\n",
    "write_to_s3('test_dynamic_feat.json',bucket,'deepar/energy-demand/data_dynamic/test/test_dynamic_feat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "image_name = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c5.4xlarge',\n",
    "    base_job_name=base_job_name,\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 360, 360)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply referring to train path and test path\n",
    "# You can have multiple files in each path\n",
    "# SageMaker will use all the files\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 01:03:32 Starting - Starting the training job...\n",
      "2020-01-18 01:03:33 Starting - Launching requested ML instances......\n",
      "2020-01-18 01:04:36 Starting - Preparing the instances for training...\n",
      "2020-01-18 01:05:15 Downloading - Downloading input data...\n",
      "2020-01-18 01:05:52 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'360', u'epochs': u'400', u'time_freq': u'H', u'context_length': u'360', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'360', u'time_freq': u'H', u'context_length': u'360', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=25 from dataset.\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Training set statistics:\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Integer time series\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] number of time series: 1\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] number of observations: 39087\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] mean target length: 39087\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] min/mean/max target: 2019.0/3240.85327603/7095.0\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] mean abs(target): 3240.85327603\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] contains missing values: no\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Test set statistics:\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Integer time series\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] number of time series: 1\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] number of observations: 39447\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] mean target length: 39447\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] min/mean/max target: 2019.0/3237.86062311/7095.0\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] mean abs(target): 3237.86062311\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] contains missing values: no\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] nvidia-smi took: 0.025132894516 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:05:54 INFO 139837668939584] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 105749.27091598511, \"sum\": 105749.27091598511, \"min\": 105749.27091598511}}, \"EndTime\": 1579309660.236397, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309554.486248}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:07:40 INFO 139837668939584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 109456.70819282532, \"sum\": 109456.70819282532, \"min\": 109456.70819282532}}, \"EndTime\": 1579309663.943078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309660.236891}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:07:52 INFO 139837668939584] Epoch[0] Batch[0] avg_epoch_loss=9.636227\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:07:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.63622665405\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:07:58 INFO 139837668939584] Epoch[0] Batch[5] avg_epoch_loss=9.426425\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:07:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=9.42642482122\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:07:58 INFO 139837668939584] Epoch[0] Batch [5]#011Speed: 52.21 samples/sec#011loss=9.426425\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] Epoch[0] Batch[10] avg_epoch_loss=9.249744\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=9.03772602081\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] Epoch[0] Batch [10]#011Speed: 48.90 samples/sec#011loss=9.037726\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 21314.911127090454, \"sum\": 21314.911127090454, \"min\": 21314.911127090454}}, \"EndTime\": 1579309685.258241, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309663.943247}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1665378249 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=0, train loss <loss>=9.24974354831\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:05 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_12f071d4-4664-40c4-85d8-e7898645bf6d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 256.5748691558838, \"sum\": 256.5748691558838, \"min\": 256.5748691558838}}, \"EndTime\": 1579309685.515257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309685.25831}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:13 INFO 139837668939584] Epoch[1] Batch[0] avg_epoch_loss=8.954285\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=8.95428466797\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:19 INFO 139837668939584] Epoch[1] Batch[5] avg_epoch_loss=8.819240\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=8.81924009323\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:19 INFO 139837668939584] Epoch[1] Batch [5]#011Speed: 52.77 samples/sec#011loss=8.819240\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] Epoch[1] Batch[10] avg_epoch_loss=8.713863\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=8.58741130829\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] Epoch[1] Batch [10]#011Speed: 49.38 samples/sec#011loss=8.587411\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20299.921989440918, \"sum\": 20299.921989440918, \"min\": 20299.921989440918}}, \"EndTime\": 1579309705.815285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309685.515309}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.7585965279 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=1, train loss <loss>=8.7138633728\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:25 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:26 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_4ad7a81d-42ac-4fbe-81d7-4db4c2266a37-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 286.8218421936035, \"sum\": 286.8218421936035, \"min\": 286.8218421936035}}, \"EndTime\": 1579309706.102498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309705.815347}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:33 INFO 139837668939584] Epoch[2] Batch[0] avg_epoch_loss=8.540827\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=8.54082679749\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:39 INFO 139837668939584] Epoch[2] Batch[5] avg_epoch_loss=8.437055\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=8.43705542882\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:39 INFO 139837668939584] Epoch[2] Batch [5]#011Speed: 52.85 samples/sec#011loss=8.437055\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] Epoch[2] Batch[10] avg_epoch_loss=8.343469\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=8.23116569519\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] Epoch[2] Batch [10]#011Speed: 49.22 samples/sec#011loss=8.231166\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20172.268867492676, \"sum\": 20172.268867492676, \"min\": 20172.268867492676}}, \"EndTime\": 1579309726.274865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309706.102547}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.9659015259 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=2, train loss <loss>=8.34346918626\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:46 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_3a8212c8-68a1-421c-b716-0a1f2ac576cf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 271.03400230407715, \"sum\": 271.03400230407715, \"min\": 271.03400230407715}}, \"EndTime\": 1579309726.546283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309726.274927}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:53 INFO 139837668939584] Epoch[3] Batch[0] avg_epoch_loss=8.173269\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:08:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=8.17326927185\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:00 INFO 139837668939584] Epoch[3] Batch[5] avg_epoch_loss=8.112683\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=8.11268250148\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:00 INFO 139837668939584] Epoch[3] Batch [5]#011Speed: 52.88 samples/sec#011loss=8.112683\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:04 INFO 139837668939584] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18312.73579597473, \"sum\": 18312.73579597473, \"min\": 18312.73579597473}}, \"EndTime\": 1579309744.859116, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309726.546335}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:04 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=34.9482007576 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:04 INFO 139837668939584] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=3, train loss <loss>=8.06116876602\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:04 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:05 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_c7be115a-1c05-4550-baa0-8bf0241cf121-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 258.26001167297363, \"sum\": 258.26001167297363, \"min\": 258.26001167297363}}, \"EndTime\": 1579309745.117806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309744.859172}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:12 INFO 139837668939584] Epoch[4] Batch[0] avg_epoch_loss=7.944675\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=7.94467496872\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:18 INFO 139837668939584] Epoch[4] Batch[5] avg_epoch_loss=7.934950\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=7.9349501133\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:18 INFO 139837668939584] Epoch[4] Batch [5]#011Speed: 52.19 samples/sec#011loss=7.934950\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:23 INFO 139837668939584] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18684.288024902344, \"sum\": 18684.288024902344, \"min\": 18684.288024902344}}, \"EndTime\": 1579309763.802208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309745.11786}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:23 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.0757473673 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:23 INFO 139837668939584] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=4, train loss <loss>=7.90500874519\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:23 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:24 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_7cf534cf-aa0f-4541-bd14-02e78d5f4768-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 278.80287170410156, \"sum\": 278.80287170410156, \"min\": 278.80287170410156}}, \"EndTime\": 1579309764.081445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309763.802274}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:32 INFO 139837668939584] Epoch[5] Batch[0] avg_epoch_loss=7.968442\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=7.96844244003\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:38 INFO 139837668939584] Epoch[5] Batch[5] avg_epoch_loss=7.885411\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=7.88541078568\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:38 INFO 139837668939584] Epoch[5] Batch [5]#011Speed: 52.80 samples/sec#011loss=7.885411\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] Epoch[5] Batch[10] avg_epoch_loss=7.857136\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=7.82320604324\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] Epoch[5] Batch [10]#011Speed: 49.21 samples/sec#011loss=7.823206\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20493.797063827515, \"sum\": 20493.797063827515, \"min\": 20493.797063827515}}, \"EndTime\": 1579309784.575365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309764.08151}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.7902577751 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=5, train loss <loss>=7.85713590275\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:44 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_b1fbaadb-3ed0-4e15-835e-25fea08e0c6f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 282.5019359588623, \"sum\": 282.5019359588623, \"min\": 282.5019359588623}}, \"EndTime\": 1579309784.858265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309784.575429}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:52 INFO 139837668939584] Epoch[6] Batch[0] avg_epoch_loss=7.737412\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=7.73741197586\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:58 INFO 139837668939584] Epoch[6] Batch[5] avg_epoch_loss=7.829026\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=7.82902566592\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:09:58 INFO 139837668939584] Epoch[6] Batch [5]#011Speed: 52.72 samples/sec#011loss=7.829026\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:03 INFO 139837668939584] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18886.73496246338, \"sum\": 18886.73496246338, \"min\": 18886.73496246338}}, \"EndTime\": 1579309803.745112, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309784.858321}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:03 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.250679629 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:03 INFO 139837668939584] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=6, train loss <loss>=7.83487005234\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:03 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:04 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_6c237330-fbd8-42e6-9635-6a6cf6ba2099-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 273.79298210144043, \"sum\": 273.79298210144043, \"min\": 273.79298210144043}}, \"EndTime\": 1579309804.019325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309803.745179}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:11 INFO 139837668939584] Epoch[7] Batch[0] avg_epoch_loss=7.832253\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=7.83225297928\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:17 INFO 139837668939584] Epoch[7] Batch[5] avg_epoch_loss=7.859215\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=7.85921486219\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:17 INFO 139837668939584] Epoch[7] Batch [5]#011Speed: 52.74 samples/sec#011loss=7.859215\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:22 INFO 139837668939584] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18325.91199874878, \"sum\": 18325.91199874878, \"min\": 18325.91199874878}}, \"EndTime\": 1579309822.345335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309804.019375}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:22 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.0855719765 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:22 INFO 139837668939584] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=7, train loss <loss>=7.85440154076\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:22 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:29 INFO 139837668939584] Epoch[8] Batch[0] avg_epoch_loss=7.804295\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=7.80429458618\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:35 INFO 139837668939584] Epoch[8] Batch[5] avg_epoch_loss=7.852913\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=7.85291306178\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:35 INFO 139837668939584] Epoch[8] Batch [5]#011Speed: 52.79 samples/sec#011loss=7.852913\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:41 INFO 139837668939584] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18770.562171936035, \"sum\": 18770.562171936035, \"min\": 18770.562171936035}}, \"EndTime\": 1579309841.116287, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309822.345391}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:41 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=34.0424859624 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:41 INFO 139837668939584] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=8, train loss <loss>=7.84329638481\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:41 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:48 INFO 139837668939584] Epoch[9] Batch[0] avg_epoch_loss=7.870100\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=7.87009954453\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:54 INFO 139837668939584] Epoch[9] Batch[5] avg_epoch_loss=7.798624\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=7.79862427711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:10:54 INFO 139837668939584] Epoch[9] Batch [5]#011Speed: 52.55 samples/sec#011loss=7.798624\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] Epoch[9] Batch[10] avg_epoch_loss=7.841449\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=7.89283857346\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] Epoch[9] Batch [10]#011Speed: 48.89 samples/sec#011loss=7.892839\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20284.183979034424, \"sum\": 20284.183979034424, \"min\": 20284.183979034424}}, \"EndTime\": 1579309861.400847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309841.116355}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.8826161918 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=9, train loss <loss>=7.84144895727\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:09 INFO 139837668939584] Epoch[10] Batch[0] avg_epoch_loss=7.916643\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=7.91664266586\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:15 INFO 139837668939584] Epoch[10] Batch[5] avg_epoch_loss=7.858272\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=7.8582722346\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:15 INFO 139837668939584] Epoch[10] Batch [5]#011Speed: 52.77 samples/sec#011loss=7.858272\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] Epoch[10] Batch[10] avg_epoch_loss=7.833848\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=7.80453910828\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] Epoch[10] Batch [10]#011Speed: 49.05 samples/sec#011loss=7.804539\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20344.62809562683, \"sum\": 20344.62809562683, \"min\": 20344.62809562683}}, \"EndTime\": 1579309881.745834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309861.400908}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.6866183077 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=10, train loss <loss>=7.83384808627\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:21 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:22 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_dc47668c-6d3e-464d-a5e5-5a0821d27a07-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 268.6879634857178, \"sum\": 268.6879634857178, \"min\": 268.6879634857178}}, \"EndTime\": 1579309882.014906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309881.745895}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:29 INFO 139837668939584] Epoch[11] Batch[0] avg_epoch_loss=7.822695\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=7.82269525528\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:35 INFO 139837668939584] Epoch[11] Batch[5] avg_epoch_loss=7.852549\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=7.85254899661\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:35 INFO 139837668939584] Epoch[11] Batch [5]#011Speed: 53.00 samples/sec#011loss=7.852549\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] Epoch[11] Batch[10] avg_epoch_loss=7.823179\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=7.78793430328\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] Epoch[11] Batch [10]#011Speed: 49.23 samples/sec#011loss=7.787934\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20130.039930343628, \"sum\": 20130.039930343628, \"min\": 20130.039930343628}}, \"EndTime\": 1579309902.145047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309882.014956}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.240236885 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=11, train loss <loss>=7.82317868146\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:42 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_35e74039-7cfb-4727-a5c0-7f5260dd0d8d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 248.460054397583, \"sum\": 248.460054397583, \"min\": 248.460054397583}}, \"EndTime\": 1579309902.393887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309902.145104}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:49 INFO 139837668939584] Epoch[12] Batch[0] avg_epoch_loss=7.789207\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=7.78920698166\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:55 INFO 139837668939584] Epoch[12] Batch[5] avg_epoch_loss=7.796236\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=7.79623643557\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:11:55 INFO 139837668939584] Epoch[12] Batch [5]#011Speed: 52.99 samples/sec#011loss=7.796236\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:01 INFO 139837668939584] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18638.811111450195, \"sum\": 18638.811111450195, \"min\": 18638.811111450195}}, \"EndTime\": 1579309921.032802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309902.393938}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.5856548558 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=12, train loss <loss>=7.82634186745\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:08 INFO 139837668939584] Epoch[13] Batch[0] avg_epoch_loss=7.851917\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=7.85191679001\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:14 INFO 139837668939584] Epoch[13] Batch[5] avg_epoch_loss=7.824804\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=7.82480414708\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:14 INFO 139837668939584] Epoch[13] Batch [5]#011Speed: 52.87 samples/sec#011loss=7.824804\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] Epoch[13] Batch[10] avg_epoch_loss=7.793038\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=7.75491752625\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] Epoch[13] Batch [10]#011Speed: 48.95 samples/sec#011loss=7.754918\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20190.176963806152, \"sum\": 20190.176963806152, \"min\": 20190.176963806152}}, \"EndTime\": 1579309941.223361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309921.032871}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.7479830567 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=13, train loss <loss>=7.79303750125\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:21 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_8fb83a9a-9b8e-4e82-bb37-b45d812199c1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 245.27788162231445, \"sum\": 245.27788162231445, \"min\": 245.27788162231445}}, \"EndTime\": 1579309941.469065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309941.223416}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:28 INFO 139837668939584] Epoch[14] Batch[0] avg_epoch_loss=7.736150\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=7.73615026474\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:34 INFO 139837668939584] Epoch[14] Batch[5] avg_epoch_loss=7.792436\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=7.79243564606\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:34 INFO 139837668939584] Epoch[14] Batch [5]#011Speed: 52.52 samples/sec#011loss=7.792436\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:40 INFO 139837668939584] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18753.959894180298, \"sum\": 18753.959894180298, \"min\": 18753.959894180298}}, \"EndTime\": 1579309960.223137, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309941.46912}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:40 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.0061901174 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:40 INFO 139837668939584] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=14, train loss <loss>=7.81459579468\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:40 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:47 INFO 139837668939584] Epoch[15] Batch[0] avg_epoch_loss=7.924067\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=7.92406749725\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:53 INFO 139837668939584] Epoch[15] Batch[5] avg_epoch_loss=7.819870\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=7.81986999512\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:53 INFO 139837668939584] Epoch[15] Batch [5]#011Speed: 52.78 samples/sec#011loss=7.819870\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:58 INFO 139837668939584] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18581.089973449707, \"sum\": 18581.089973449707, \"min\": 18581.089973449707}}, \"EndTime\": 1579309978.804598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309960.223204}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.3445673661 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=15, train loss <loss>=7.84428009987\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:12:58 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:06 INFO 139837668939584] Epoch[16] Batch[0] avg_epoch_loss=7.818742\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=7.8187417984\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:12 INFO 139837668939584] Epoch[16] Batch[5] avg_epoch_loss=7.791555\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=7.79155453046\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:12 INFO 139837668939584] Epoch[16] Batch [5]#011Speed: 52.48 samples/sec#011loss=7.791555\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:17 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18912.573099136353, \"sum\": 18912.573099136353, \"min\": 18912.573099136353}}, \"EndTime\": 1579309997.717548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309978.804655}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:17 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.5225029263 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:17 INFO 139837668939584] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=16, train loss <loss>=7.77801980972\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:17 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:17 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_97e845f0-1cc3-441b-8740-13bbd9098e1f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 260.85400581359863, \"sum\": 260.85400581359863, \"min\": 260.85400581359863}}, \"EndTime\": 1579309997.978823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309997.717617}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:25 INFO 139837668939584] Epoch[17] Batch[0] avg_epoch_loss=7.862870\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=7.86287021637\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:31 INFO 139837668939584] Epoch[17] Batch[5] avg_epoch_loss=7.784019\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=7.78401947021\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:31 INFO 139837668939584] Epoch[17] Batch [5]#011Speed: 52.31 samples/sec#011loss=7.784019\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] Epoch[17] Batch[10] avg_epoch_loss=7.798507\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=7.81589126587\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] Epoch[17] Batch [10]#011Speed: 48.84 samples/sec#011loss=7.815891\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20348.04606437683, \"sum\": 20348.04606437683, \"min\": 20348.04606437683}}, \"EndTime\": 1579310018.326976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579309997.978879}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.4354019822 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=17, train loss <loss>=7.79850665006\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:38 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:45 INFO 139837668939584] Epoch[18] Batch[0] avg_epoch_loss=7.773651\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=7.77365064621\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:51 INFO 139837668939584] Epoch[18] Batch[5] avg_epoch_loss=7.808017\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=7.80801693598\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:51 INFO 139837668939584] Epoch[18] Batch [5]#011Speed: 52.31 samples/sec#011loss=7.808017\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] Epoch[18] Batch[10] avg_epoch_loss=7.742688\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=7.66429262161\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] Epoch[18] Batch [10]#011Speed: 49.02 samples/sec#011loss=7.664293\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20100.31008720398, \"sum\": 20100.31008720398, \"min\": 20100.31008720398}}, \"EndTime\": 1579310038.427641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310018.327037}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9894241959 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=18, train loss <loss>=7.74268770218\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:13:58 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_fdd8e7cc-e6e8-47b2-b2b5-970538800f94-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 257.22503662109375, \"sum\": 257.22503662109375, \"min\": 257.22503662109375}}, \"EndTime\": 1579310038.685229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310038.427697}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:06 INFO 139837668939584] Epoch[19] Batch[0] avg_epoch_loss=7.790799\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=7.79079866409\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:12 INFO 139837668939584] Epoch[19] Batch[5] avg_epoch_loss=7.783687\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=7.78368743261\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:12 INFO 139837668939584] Epoch[19] Batch [5]#011Speed: 52.30 samples/sec#011loss=7.783687\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] Epoch[19] Batch[10] avg_epoch_loss=7.773897\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=7.76214780807\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] Epoch[19] Batch [10]#011Speed: 49.25 samples/sec#011loss=7.762148\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20360.769987106323, \"sum\": 20360.769987106323, \"min\": 20360.769987106323}}, \"EndTime\": 1579310059.046102, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310038.685286}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.8571727368 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=19, train loss <loss>=7.77389669418\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:19 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:26 INFO 139837668939584] Epoch[20] Batch[0] avg_epoch_loss=7.766258\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=7.76625823975\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:32 INFO 139837668939584] Epoch[20] Batch[5] avg_epoch_loss=7.707822\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=7.70782200495\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:32 INFO 139837668939584] Epoch[20] Batch [5]#011Speed: 52.52 samples/sec#011loss=7.707822\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] Epoch[20] Batch[10] avg_epoch_loss=7.721884\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=7.73875846863\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] Epoch[20] Batch [10]#011Speed: 49.26 samples/sec#011loss=7.738758\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20063.15302848816, \"sum\": 20063.15302848816, \"min\": 20063.15302848816}}, \"EndTime\": 1579310079.109581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310059.046156}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.2480416049 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=20, train loss <loss>=7.7218840339\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:39 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_f0be2fc2-d50c-445f-a157-53984e915afa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 260.5409622192383, \"sum\": 260.5409622192383, \"min\": 260.5409622192383}}, \"EndTime\": 1579310079.370495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310079.109635}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:46 INFO 139837668939584] Epoch[21] Batch[0] avg_epoch_loss=7.690441\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=7.69044113159\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:52 INFO 139837668939584] Epoch[21] Batch[5] avg_epoch_loss=7.688741\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=7.68874136607\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:52 INFO 139837668939584] Epoch[21] Batch [5]#011Speed: 52.66 samples/sec#011loss=7.688741\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:58 INFO 139837668939584] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18820.569038391113, \"sum\": 18820.569038391113, \"min\": 18820.569038391113}}, \"EndTime\": 1579310098.191176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310079.37055}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.2081764544 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=21, train loss <loss>=7.66766562462\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:58 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:14:58 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_2aaeed22-142f-49e5-b8a0-0ce1ffea7f0a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 260.1590156555176, \"sum\": 260.1590156555176, \"min\": 260.1590156555176}}, \"EndTime\": 1579310098.45177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310098.191241}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:05 INFO 139837668939584] Epoch[22] Batch[0] avg_epoch_loss=7.617535\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=7.61753511429\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:11 INFO 139837668939584] Epoch[22] Batch[5] avg_epoch_loss=7.611977\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=7.61197733879\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:11 INFO 139837668939584] Epoch[22] Batch [5]#011Speed: 52.56 samples/sec#011loss=7.611977\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:17 INFO 139837668939584] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18791.550874710083, \"sum\": 18791.550874710083, \"min\": 18791.550874710083}}, \"EndTime\": 1579310117.24344, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310098.451831}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:17 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.631962614 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:17 INFO 139837668939584] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=22, train loss <loss>=7.61913433075\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:17 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:17 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_81d62294-c18d-420f-b65a-ebaaec736100-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 254.3928623199463, \"sum\": 254.3928623199463, \"min\": 254.3928623199463}}, \"EndTime\": 1579310117.498251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310117.243505}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:24 INFO 139837668939584] Epoch[23] Batch[0] avg_epoch_loss=7.603801\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=7.60380077362\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:30 INFO 139837668939584] Epoch[23] Batch[5] avg_epoch_loss=7.585872\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=7.58587193489\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:30 INFO 139837668939584] Epoch[23] Batch [5]#011Speed: 52.59 samples/sec#011loss=7.585872\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] Epoch[23] Batch[10] avg_epoch_loss=7.564077\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=7.53792352676\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] Epoch[23] Batch [10]#011Speed: 49.18 samples/sec#011loss=7.537924\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19984.18688774109, \"sum\": 19984.18688774109, \"min\": 19984.18688774109}}, \"EndTime\": 1579310137.482548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310117.498308}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.075183771 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=23, train loss <loss>=7.56407720392\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:37 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_09b030a0-c251-49c2-923c-6a2c9d5fffa8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 271.12603187561035, \"sum\": 271.12603187561035, \"min\": 271.12603187561035}}, \"EndTime\": 1579310137.754082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310137.482627}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:45 INFO 139837668939584] Epoch[24] Batch[0] avg_epoch_loss=7.542871\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=7.54287052155\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:51 INFO 139837668939584] Epoch[24] Batch[5] avg_epoch_loss=7.568890\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=7.56888961792\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:51 INFO 139837668939584] Epoch[24] Batch [5]#011Speed: 52.50 samples/sec#011loss=7.568890\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:56 INFO 139837668939584] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18719.45285797119, \"sum\": 18719.45285797119, \"min\": 18719.45285797119}}, \"EndTime\": 1579310156.473646, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310137.754141}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:56 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.746511771 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:56 INFO 139837668939584] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=24, train loss <loss>=7.55512919426\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:56 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:15:56 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_53c1c8cd-7164-4747-b35c-fb639ca806cd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 258.2058906555176, \"sum\": 258.2058906555176, \"min\": 258.2058906555176}}, \"EndTime\": 1579310156.732269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310156.473712}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:03 INFO 139837668939584] Epoch[25] Batch[0] avg_epoch_loss=7.539547\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=7.53954696655\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:10 INFO 139837668939584] Epoch[25] Batch[5] avg_epoch_loss=7.525794\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=7.52579387029\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:10 INFO 139837668939584] Epoch[25] Batch [5]#011Speed: 52.45 samples/sec#011loss=7.525794\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:15 INFO 139837668939584] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18655.138969421387, \"sum\": 18655.138969421387, \"min\": 18655.138969421387}}, \"EndTime\": 1579310175.387531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310156.732334}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.2882397232 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=25, train loss <loss>=7.53171916008\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:15 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:15 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_ad1ebd35-6d9d-4ea4-90b3-d9e276274d7b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 268.6779499053955, \"sum\": 268.6779499053955, \"min\": 268.6779499053955}}, \"EndTime\": 1579310175.656627, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310175.387599}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:23 INFO 139837668939584] Epoch[26] Batch[0] avg_epoch_loss=7.508455\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=7.50845479965\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:29 INFO 139837668939584] Epoch[26] Batch[5] avg_epoch_loss=7.496741\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=7.49674145381\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:29 INFO 139837668939584] Epoch[26] Batch [5]#011Speed: 52.32 samples/sec#011loss=7.496741\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:34 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18986.180067062378, \"sum\": 18986.180067062378, \"min\": 18986.180067062378}}, \"EndTime\": 1579310194.64292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310175.656685}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:34 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.3925675856 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:34 INFO 139837668939584] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=26, train loss <loss>=7.50288219452\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:34 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:34 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_ba2a533f-c52c-43e5-8580-17740fde70af-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 244.6308135986328, \"sum\": 244.6308135986328, \"min\": 244.6308135986328}}, \"EndTime\": 1579310194.887966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310194.642974}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:42 INFO 139837668939584] Epoch[27] Batch[0] avg_epoch_loss=7.455441\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=7.45544147491\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:48 INFO 139837668939584] Epoch[27] Batch[5] avg_epoch_loss=7.506099\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=7.50609874725\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:48 INFO 139837668939584] Epoch[27] Batch [5]#011Speed: 52.61 samples/sec#011loss=7.506099\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] Epoch[27] Batch[10] avg_epoch_loss=7.473138\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=7.43358573914\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] Epoch[27] Batch [10]#011Speed: 48.83 samples/sec#011loss=7.433586\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20351.970195770264, \"sum\": 20351.970195770264, \"min\": 20351.970195770264}}, \"EndTime\": 1579310215.240045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310194.888018}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9869260066 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=27, train loss <loss>=7.47313828902\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:16:55 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_25ff8918-069d-4af3-b0a8-2df511021a19-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 303.24792861938477, \"sum\": 303.24792861938477, \"min\": 303.24792861938477}}, \"EndTime\": 1579310215.543691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310215.240108}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:02 INFO 139837668939584] Epoch[28] Batch[0] avg_epoch_loss=7.483111\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=7.48311090469\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:09 INFO 139837668939584] Epoch[28] Batch[5] avg_epoch_loss=7.443418\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=7.4434179465\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:09 INFO 139837668939584] Epoch[28] Batch [5]#011Speed: 52.46 samples/sec#011loss=7.443418\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:14 INFO 139837668939584] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18775.494813919067, \"sum\": 18775.494813919067, \"min\": 18775.494813919067}}, \"EndTime\": 1579310234.31929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310215.543742}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.2346414625 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=28, train loss <loss>=7.4676112175\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:14 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:14 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_d6e9336e-66da-4372-9480-8961134b3851-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 256.52098655700684, \"sum\": 256.52098655700684, \"min\": 256.52098655700684}}, \"EndTime\": 1579310234.576229, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310234.319357}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:21 INFO 139837668939584] Epoch[29] Batch[0] avg_epoch_loss=7.451227\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=7.45122671127\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:27 INFO 139837668939584] Epoch[29] Batch[5] avg_epoch_loss=7.446614\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=7.44661355019\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:27 INFO 139837668939584] Epoch[29] Batch [5]#011Speed: 52.75 samples/sec#011loss=7.446614\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:33 INFO 139837668939584] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18641.368865966797, \"sum\": 18641.368865966797, \"min\": 18641.368865966797}}, \"EndTime\": 1579310253.217718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310234.576291}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:33 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.7956129434 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:33 INFO 139837668939584] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=29, train loss <loss>=7.42991666794\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:33 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:33 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_92469ecf-ceae-4705-9255-cb33808a8b6c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 261.4400386810303, \"sum\": 261.4400386810303, \"min\": 261.4400386810303}}, \"EndTime\": 1579310253.479594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310253.21779}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:40 INFO 139837668939584] Epoch[30] Batch[0] avg_epoch_loss=7.375766\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=7.37576580048\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:46 INFO 139837668939584] Epoch[30] Batch[5] avg_epoch_loss=7.406058\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=7.40605751673\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:46 INFO 139837668939584] Epoch[30] Batch [5]#011Speed: 52.38 samples/sec#011loss=7.406058\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:52 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18755.064010620117, \"sum\": 18755.064010620117, \"min\": 18755.064010620117}}, \"EndTime\": 1579310272.234759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310253.479644}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:52 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.8040268582 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:52 INFO 139837668939584] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=30, train loss <loss>=7.41799755096\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:52 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:17:52 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_a6112bd3-6ef7-4985-8f20-ca67b6e80bf9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 256.6959857940674, \"sum\": 256.6959857940674, \"min\": 256.6959857940674}}, \"EndTime\": 1579310272.491904, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310272.234827}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:00 INFO 139837668939584] Epoch[31] Batch[0] avg_epoch_loss=7.402676\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=7.4026761055\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:06 INFO 139837668939584] Epoch[31] Batch[5] avg_epoch_loss=7.408819\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=7.40881856283\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:06 INFO 139837668939584] Epoch[31] Batch [5]#011Speed: 52.13 samples/sec#011loss=7.408819\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] Epoch[31] Batch[10] avg_epoch_loss=7.413446\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=7.41899900436\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] Epoch[31] Batch [10]#011Speed: 48.56 samples/sec#011loss=7.418999\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20562.11018562317, \"sum\": 20562.11018562317, \"min\": 20562.11018562317}}, \"EndTime\": 1579310293.054114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310272.491953}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.7785886787 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=31, train loss <loss>=7.41344603625\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:13 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_6358c519-fccd-43f6-821d-a3bdd02e56da-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 268.15009117126465, \"sum\": 268.15009117126465, \"min\": 268.15009117126465}}, \"EndTime\": 1579310293.322659, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310293.054178}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:20 INFO 139837668939584] Epoch[32] Batch[0] avg_epoch_loss=7.409789\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=7.40978860855\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:26 INFO 139837668939584] Epoch[32] Batch[5] avg_epoch_loss=7.381079\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=7.38107864062\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:26 INFO 139837668939584] Epoch[32] Batch [5]#011Speed: 52.14 samples/sec#011loss=7.381079\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] Epoch[32] Batch[10] avg_epoch_loss=7.358743\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=7.33194112778\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] Epoch[32] Batch [10]#011Speed: 49.15 samples/sec#011loss=7.331941\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20111.257076263428, \"sum\": 20111.257076263428, \"min\": 20111.257076263428}}, \"EndTime\": 1579310313.434032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310293.322718}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9222662186 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=32, train loss <loss>=7.35874340751\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:33 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_d27c847c-0ceb-40b4-99be-1b838fc43b0e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 267.3659324645996, \"sum\": 267.3659324645996, \"min\": 267.3659324645996}}, \"EndTime\": 1579310313.701801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310313.434098}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:41 INFO 139837668939584] Epoch[33] Batch[0] avg_epoch_loss=7.404902\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=7.40490245819\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:47 INFO 139837668939584] Epoch[33] Batch[5] avg_epoch_loss=7.411258\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=7.41125798225\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:47 INFO 139837668939584] Epoch[33] Batch [5]#011Speed: 52.22 samples/sec#011loss=7.411258\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] Epoch[33] Batch[10] avg_epoch_loss=7.445000\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=7.48548994064\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] Epoch[33] Batch [10]#011Speed: 49.08 samples/sec#011loss=7.485490\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20259.644985198975, \"sum\": 20259.644985198975, \"min\": 20259.644985198975}}, \"EndTime\": 1579310333.961548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310313.701853}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.3795003676 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=33, train loss <loss>=7.44499978152\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:18:53 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:01 INFO 139837668939584] Epoch[34] Batch[0] avg_epoch_loss=7.465941\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=7.4659409523\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:07 INFO 139837668939584] Epoch[34] Batch[5] avg_epoch_loss=7.450956\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:07 INFO 139837668939584] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=7.45095578829\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:07 INFO 139837668939584] Epoch[34] Batch [5]#011Speed: 52.58 samples/sec#011loss=7.450956\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] Epoch[34] Batch[10] avg_epoch_loss=7.427424\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=7.39918584824\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] Epoch[34] Batch [10]#011Speed: 49.16 samples/sec#011loss=7.399186\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20416.34702682495, \"sum\": 20416.34702682495, \"min\": 20416.34702682495}}, \"EndTime\": 1579310354.37826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310333.961608}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.5717787577 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=34, train loss <loss>=7.42742399736\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:14 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:21 INFO 139837668939584] Epoch[35] Batch[0] avg_epoch_loss=7.373829\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=7.37382888794\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:28 INFO 139837668939584] Epoch[35] Batch[5] avg_epoch_loss=7.407272\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=7.40727186203\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:28 INFO 139837668939584] Epoch[35] Batch [5]#011Speed: 52.44 samples/sec#011loss=7.407272\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] Epoch[35] Batch[10] avg_epoch_loss=7.459116\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=7.52132787704\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] Epoch[35] Batch [10]#011Speed: 48.84 samples/sec#011loss=7.521328\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20257.513999938965, \"sum\": 20257.513999938965, \"min\": 20257.513999938965}}, \"EndTime\": 1579310374.63615, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310354.378329}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.2841667358 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=35, train loss <loss>=7.45911550522\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:34 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:42 INFO 139837668939584] Epoch[36] Batch[0] avg_epoch_loss=7.399990\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=7.39999008179\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:48 INFO 139837668939584] Epoch[36] Batch[5] avg_epoch_loss=7.380161\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=7.38016144435\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:48 INFO 139837668939584] Epoch[36] Batch [5]#011Speed: 52.05 samples/sec#011loss=7.380161\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] Epoch[36] Batch[10] avg_epoch_loss=7.374104\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=7.36683397293\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] Epoch[36] Batch [10]#011Speed: 48.92 samples/sec#011loss=7.366834\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20350.056886672974, \"sum\": 20350.056886672974, \"min\": 20350.056886672974}}, \"EndTime\": 1579310394.986556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310374.636213}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.432191682 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=36, train loss <loss>=7.37410350279\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:19:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:02 INFO 139837668939584] Epoch[37] Batch[0] avg_epoch_loss=7.450825\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=7.45082473755\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:08 INFO 139837668939584] Epoch[37] Batch[5] avg_epoch_loss=7.412394\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=7.41239420573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:08 INFO 139837668939584] Epoch[37] Batch [5]#011Speed: 52.29 samples/sec#011loss=7.412394\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:13 INFO 139837668939584] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18948.169946670532, \"sum\": 18948.169946670532, \"min\": 18948.169946670532}}, \"EndTime\": 1579310413.935105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310394.986624}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:13 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.617876797 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:13 INFO 139837668939584] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=37, train loss <loss>=7.39907155037\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:13 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:21 INFO 139837668939584] Epoch[38] Batch[0] avg_epoch_loss=7.322353\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=7.32235336304\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:27 INFO 139837668939584] Epoch[38] Batch[5] avg_epoch_loss=7.365076\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=7.36507558823\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:27 INFO 139837668939584] Epoch[38] Batch [5]#011Speed: 52.26 samples/sec#011loss=7.365076\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] Epoch[38] Batch[10] avg_epoch_loss=7.354399\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=7.34158792496\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] Epoch[38] Batch [10]#011Speed: 49.18 samples/sec#011loss=7.341588\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20109.843015670776, \"sum\": 20109.843015670776, \"min\": 20109.843015670776}}, \"EndTime\": 1579310434.045331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310413.935161}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9245188289 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=38, train loss <loss>=7.35439937765\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:34 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_d9d42e3d-dc2d-4fb9-b604-bd2a94082df9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 280.58505058288574, \"sum\": 280.58505058288574, \"min\": 280.58505058288574}}, \"EndTime\": 1579310434.326308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310434.045392}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:41 INFO 139837668939584] Epoch[39] Batch[0] avg_epoch_loss=7.355389\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=7.35538911819\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:47 INFO 139837668939584] Epoch[39] Batch[5] avg_epoch_loss=7.388032\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=7.38803156217\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:47 INFO 139837668939584] Epoch[39] Batch [5]#011Speed: 52.01 samples/sec#011loss=7.388032\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] Epoch[39] Batch[10] avg_epoch_loss=7.366665\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=7.34102420807\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] Epoch[39] Batch [10]#011Speed: 48.70 samples/sec#011loss=7.341024\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20244.510173797607, \"sum\": 20244.510173797607, \"min\": 20244.510173797607}}, \"EndTime\": 1579310454.570927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310434.326361}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.1073288072 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=39, train loss <loss>=7.36666458303\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:20:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:02 INFO 139837668939584] Epoch[40] Batch[0] avg_epoch_loss=7.344755\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=7.34475469589\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:08 INFO 139837668939584] Epoch[40] Batch[5] avg_epoch_loss=7.335031\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=7.33503142993\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:08 INFO 139837668939584] Epoch[40] Batch [5]#011Speed: 52.20 samples/sec#011loss=7.335031\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] Epoch[40] Batch[10] avg_epoch_loss=7.349563\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=7.36700000763\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] Epoch[40] Batch [10]#011Speed: 48.57 samples/sec#011loss=7.367000\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20374.130964279175, \"sum\": 20374.130964279175, \"min\": 20374.130964279175}}, \"EndTime\": 1579310474.945405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310454.570988}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.6392799899 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=40, train loss <loss>=7.34956260161\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:14 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:15 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_6ccd1f18-33a6-47ae-ad0a-629ade3b54d6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 264.34803009033203, \"sum\": 264.34803009033203, \"min\": 264.34803009033203}}, \"EndTime\": 1579310475.21014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310474.945468}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:22 INFO 139837668939584] Epoch[41] Batch[0] avg_epoch_loss=7.380214\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=7.38021373749\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:28 INFO 139837668939584] Epoch[41] Batch[5] avg_epoch_loss=7.375268\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=7.37526790301\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:28 INFO 139837668939584] Epoch[41] Batch [5]#011Speed: 52.17 samples/sec#011loss=7.375268\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:34 INFO 139837668939584] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18934.274911880493, \"sum\": 18934.274911880493, \"min\": 18934.274911880493}}, \"EndTime\": 1579310494.144516, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310475.210192}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:34 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.3784452039 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:34 INFO 139837668939584] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=41, train loss <loss>=7.357665205\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:34 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:41 INFO 139837668939584] Epoch[42] Batch[0] avg_epoch_loss=7.379056\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=7.37905550003\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:47 INFO 139837668939584] Epoch[42] Batch[5] avg_epoch_loss=7.335925\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=7.33592526118\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:47 INFO 139837668939584] Epoch[42] Batch [5]#011Speed: 51.82 samples/sec#011loss=7.335925\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] Epoch[42] Batch[10] avg_epoch_loss=7.347748\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=7.36193599701\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] Epoch[42] Batch [10]#011Speed: 48.48 samples/sec#011loss=7.361936\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20243.392944335938, \"sum\": 20243.392944335938, \"min\": 20243.392944335938}}, \"EndTime\": 1579310514.388291, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310494.144586}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.7633086108 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=42, train loss <loss>=7.34774832292\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:21:54 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_d8dfdd37-0162-4c5c-af90-667f675093fa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 277.4481773376465, \"sum\": 277.4481773376465, \"min\": 277.4481773376465}}, \"EndTime\": 1579310514.666124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310514.388352}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:02 INFO 139837668939584] Epoch[43] Batch[0] avg_epoch_loss=7.386661\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=7.38666057587\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:08 INFO 139837668939584] Epoch[43] Batch[5] avg_epoch_loss=7.365799\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=7.36579879125\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:08 INFO 139837668939584] Epoch[43] Batch [5]#011Speed: 51.45 samples/sec#011loss=7.365799\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] Epoch[43] Batch[10] avg_epoch_loss=7.341680\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=7.31273765564\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] Epoch[43] Batch [10]#011Speed: 48.21 samples/sec#011loss=7.312738\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20563.08603286743, \"sum\": 20563.08603286743, \"min\": 20563.08603286743}}, \"EndTime\": 1579310535.229311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310514.666176}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.5825125734 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=43, train loss <loss>=7.34168009325\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:15 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_4939168f-82d8-4586-954a-d8cfbfa86ea6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 258.7089538574219, \"sum\": 258.7089538574219, \"min\": 258.7089538574219}}, \"EndTime\": 1579310535.48841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310535.229375}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:23 INFO 139837668939584] Epoch[44] Batch[0] avg_epoch_loss=7.212125\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=7.21212530136\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:29 INFO 139837668939584] Epoch[44] Batch[5] avg_epoch_loss=7.323557\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=7.32355705897\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:29 INFO 139837668939584] Epoch[44] Batch [5]#011Speed: 51.69 samples/sec#011loss=7.323557\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] Epoch[44] Batch[10] avg_epoch_loss=7.324964\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=7.32665281296\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] Epoch[44] Batch [10]#011Speed: 48.11 samples/sec#011loss=7.326653\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20443.57991218567, \"sum\": 20443.57991218567, \"min\": 20443.57991218567}}, \"EndTime\": 1579310555.932089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310535.48846}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.5990325999 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=44, train loss <loss>=7.32496421987\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:35 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:36 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_cf94bd6f-ae7d-4438-a250-c429e879f243-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 250.44798851013184, \"sum\": 250.44798851013184, \"min\": 250.44798851013184}}, \"EndTime\": 1579310556.182935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310555.932144}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:43 INFO 139837668939584] Epoch[45] Batch[0] avg_epoch_loss=7.374527\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=7.37452745438\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:49 INFO 139837668939584] Epoch[45] Batch[5] avg_epoch_loss=7.357478\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=7.35747830073\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:49 INFO 139837668939584] Epoch[45] Batch [5]#011Speed: 52.10 samples/sec#011loss=7.357478\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:55 INFO 139837668939584] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19130.582809448242, \"sum\": 19130.582809448242, \"min\": 19130.582809448242}}, \"EndTime\": 1579310575.313626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310556.182989}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:55 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.0882401282 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:55 INFO 139837668939584] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=45, train loss <loss>=7.35160779953\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:22:55 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:02 INFO 139837668939584] Epoch[46] Batch[0] avg_epoch_loss=7.260615\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=7.26061534882\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:09 INFO 139837668939584] Epoch[46] Batch[5] avg_epoch_loss=7.337113\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=7.33711282412\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:09 INFO 139837668939584] Epoch[46] Batch [5]#011Speed: 51.74 samples/sec#011loss=7.337113\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:14 INFO 139837668939584] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19135.221004486084, \"sum\": 19135.221004486084, \"min\": 19135.221004486084}}, \"EndTime\": 1579310594.449188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310575.313681}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.1847110482 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=46, train loss <loss>=7.34471416473\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:14 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:22 INFO 139837668939584] Epoch[47] Batch[0] avg_epoch_loss=7.317340\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=7.31734037399\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:28 INFO 139837668939584] Epoch[47] Batch[5] avg_epoch_loss=7.342973\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=7.3429728349\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:28 INFO 139837668939584] Epoch[47] Batch [5]#011Speed: 51.37 samples/sec#011loss=7.342973\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:33 INFO 139837668939584] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19200.856924057007, \"sum\": 19200.856924057007, \"min\": 19200.856924057007}}, \"EndTime\": 1579310613.650426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310594.449255}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:33 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.2275120297 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:33 INFO 139837668939584] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=47, train loss <loss>=7.33776974678\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:33 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:40 INFO 139837668939584] Epoch[48] Batch[0] avg_epoch_loss=7.421476\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=7.42147636414\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:47 INFO 139837668939584] Epoch[48] Batch[5] avg_epoch_loss=7.351130\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=7.35113048553\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:47 INFO 139837668939584] Epoch[48] Batch [5]#011Speed: 51.82 samples/sec#011loss=7.351130\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:52 INFO 139837668939584] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18957.93104171753, \"sum\": 18957.93104171753, \"min\": 18957.93104171753}}, \"EndTime\": 1579310632.608749, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310613.650493}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:52 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.4928566156 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:52 INFO 139837668939584] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=48, train loss <loss>=7.36066179276\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:52 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:59 INFO 139837668939584] Epoch[49] Batch[0] avg_epoch_loss=7.321485\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:23:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=7.32148456573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:06 INFO 139837668939584] Epoch[49] Batch[5] avg_epoch_loss=7.336711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=7.33671069145\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:06 INFO 139837668939584] Epoch[49] Batch [5]#011Speed: 51.45 samples/sec#011loss=7.336711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:11 INFO 139837668939584] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18995.403051376343, \"sum\": 18995.403051376343, \"min\": 18995.403051376343}}, \"EndTime\": 1579310651.604532, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310632.608804}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.8498841105 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=49, train loss <loss>=7.37041034698\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:18 INFO 139837668939584] Epoch[50] Batch[0] avg_epoch_loss=7.340208\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=7.34020805359\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:25 INFO 139837668939584] Epoch[50] Batch[5] avg_epoch_loss=7.350753\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=7.35075306892\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:25 INFO 139837668939584] Epoch[50] Batch [5]#011Speed: 51.78 samples/sec#011loss=7.350753\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:30 INFO 139837668939584] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19013.586044311523, \"sum\": 19013.586044311523, \"min\": 19013.586044311523}}, \"EndTime\": 1579310670.618494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310651.604602}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:30 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.2918331727 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:30 INFO 139837668939584] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=50, train loss <loss>=7.33727579117\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:30 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:38 INFO 139837668939584] Epoch[51] Batch[0] avg_epoch_loss=7.251661\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=7.25166130066\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:44 INFO 139837668939584] Epoch[51] Batch[5] avg_epoch_loss=7.271994\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=7.27199403445\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:44 INFO 139837668939584] Epoch[51] Batch [5]#011Speed: 51.75 samples/sec#011loss=7.271994\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] Epoch[51] Batch[10] avg_epoch_loss=7.291383\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=7.31464958191\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] Epoch[51] Batch [10]#011Speed: 47.81 samples/sec#011loss=7.314650\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20429.765939712524, \"sum\": 20429.765939712524, \"min\": 20429.765939712524}}, \"EndTime\": 1579310691.048645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310670.618555}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9140725572 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=51, train loss <loss>=7.29138291966\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:51 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_6fe7977e-7b28-4ad7-a72c-d61b21a2aa57-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 267.7450180053711, \"sum\": 267.7450180053711, \"min\": 267.7450180053711}}, \"EndTime\": 1579310691.316783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310691.048708}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:59 INFO 139837668939584] Epoch[52] Batch[0] avg_epoch_loss=7.277877\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:24:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=7.27787685394\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:05 INFO 139837668939584] Epoch[52] Batch[5] avg_epoch_loss=7.334489\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=7.33448894819\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:05 INFO 139837668939584] Epoch[52] Batch [5]#011Speed: 51.44 samples/sec#011loss=7.334489\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:11 INFO 139837668939584] Epoch[52] Batch[10] avg_epoch_loss=7.323999\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=7.3114107132\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:11 INFO 139837668939584] Epoch[52] Batch [10]#011Speed: 51.52 samples/sec#011loss=7.311411\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:13 INFO 139837668939584] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22373.96478652954, \"sum\": 22373.96478652954, \"min\": 22373.96478652954}}, \"EndTime\": 1579310713.690854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310691.316835}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:13 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.8225633609 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:13 INFO 139837668939584] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=52, train loss <loss>=7.30447963874\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:13 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:20 INFO 139837668939584] Epoch[53] Batch[0] avg_epoch_loss=7.308656\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=7.30865573883\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:26 INFO 139837668939584] Epoch[53] Batch[5] avg_epoch_loss=7.331065\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=7.33106525739\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:26 INFO 139837668939584] Epoch[53] Batch [5]#011Speed: 51.23 samples/sec#011loss=7.331065\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:32 INFO 139837668939584] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18788.039922714233, \"sum\": 18788.039922714233, \"min\": 18788.039922714233}}, \"EndTime\": 1579310732.479269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310713.690919}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:32 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.0947422579 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:32 INFO 139837668939584] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=53, train loss <loss>=7.3216173172\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:32 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:39 INFO 139837668939584] Epoch[54] Batch[0] avg_epoch_loss=7.383257\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=7.38325691223\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:46 INFO 139837668939584] Epoch[54] Batch[5] avg_epoch_loss=7.347917\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=7.34791668256\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:46 INFO 139837668939584] Epoch[54] Batch [5]#011Speed: 51.41 samples/sec#011loss=7.347917\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:51 INFO 139837668939584] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18999.6440410614, \"sum\": 18999.6440410614, \"min\": 18999.6440410614}}, \"EndTime\": 1579310751.479294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310732.479326}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:51 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.3162363837 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:51 INFO 139837668939584] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=54, train loss <loss>=7.34284305573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:51 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:59 INFO 139837668939584] Epoch[55] Batch[0] avg_epoch_loss=7.303864\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:25:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=7.30386400223\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:05 INFO 139837668939584] Epoch[55] Batch[5] avg_epoch_loss=7.309191\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=7.3091905117\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:05 INFO 139837668939584] Epoch[55] Batch [5]#011Speed: 50.80 samples/sec#011loss=7.309191\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] Epoch[55] Batch[10] avg_epoch_loss=7.331168\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=7.35754175186\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] Epoch[55] Batch [10]#011Speed: 47.52 samples/sec#011loss=7.357542\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20992.595911026, \"sum\": 20992.595911026, \"min\": 20992.595911026}}, \"EndTime\": 1579310772.472274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310751.479365}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.0591502902 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=55, train loss <loss>=7.33116834814\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:12 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:19 INFO 139837668939584] Epoch[56] Batch[0] avg_epoch_loss=7.351609\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=7.3516087532\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:26 INFO 139837668939584] Epoch[56] Batch[5] avg_epoch_loss=7.318744\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=7.31874402364\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:26 INFO 139837668939584] Epoch[56] Batch [5]#011Speed: 51.15 samples/sec#011loss=7.318744\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:31 INFO 139837668939584] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19072.214126586914, \"sum\": 19072.214126586914, \"min\": 19072.214126586914}}, \"EndTime\": 1579310791.544784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310772.472326}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:31 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.7175886483 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:31 INFO 139837668939584] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=56, train loss <loss>=7.32917294502\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:31 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:39 INFO 139837668939584] Epoch[57] Batch[0] avg_epoch_loss=7.315951\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=7.31595087051\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:45 INFO 139837668939584] Epoch[57] Batch[5] avg_epoch_loss=7.314885\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=7.31488513947\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:45 INFO 139837668939584] Epoch[57] Batch [5]#011Speed: 50.60 samples/sec#011loss=7.314885\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] Epoch[57] Batch[10] avg_epoch_loss=7.343841\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=7.37858839035\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] Epoch[57] Batch [10]#011Speed: 47.55 samples/sec#011loss=7.378588\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20632.95602798462, \"sum\": 20632.95602798462, \"min\": 20632.95602798462}}, \"EndTime\": 1579310812.178125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310791.544851}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.3574870994 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=57, train loss <loss>=7.34384116259\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:52 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:59 INFO 139837668939584] Epoch[58] Batch[0] avg_epoch_loss=7.257717\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:26:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=7.25771665573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:06 INFO 139837668939584] Epoch[58] Batch[5] avg_epoch_loss=7.311464\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=7.31146430969\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:06 INFO 139837668939584] Epoch[58] Batch [5]#011Speed: 50.86 samples/sec#011loss=7.311464\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:11 INFO 139837668939584] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19370.450019836426, \"sum\": 19370.450019836426, \"min\": 19370.450019836426}}, \"EndTime\": 1579310831.548869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310812.178176}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.9366073263 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=58, train loss <loss>=7.3243188858\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:19 INFO 139837668939584] Epoch[59] Batch[0] avg_epoch_loss=7.316443\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=7.31644296646\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:25 INFO 139837668939584] Epoch[59] Batch[5] avg_epoch_loss=7.318896\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=7.31889565786\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:25 INFO 139837668939584] Epoch[59] Batch [5]#011Speed: 50.37 samples/sec#011loss=7.318896\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] Epoch[59] Batch[10] avg_epoch_loss=7.340370\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=7.36613903046\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] Epoch[59] Batch [10]#011Speed: 48.07 samples/sec#011loss=7.366139\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20756.87003135681, \"sum\": 20756.87003135681, \"min\": 20756.87003135681}}, \"EndTime\": 1579310852.306113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310831.548935}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.5075008607 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=59, train loss <loss>=7.34036991813\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:32 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:39 INFO 139837668939584] Epoch[60] Batch[0] avg_epoch_loss=7.321026\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=7.32102632523\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:45 INFO 139837668939584] Epoch[60] Batch[5] avg_epoch_loss=7.313480\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=7.313479503\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:45 INFO 139837668939584] Epoch[60] Batch [5]#011Speed: 50.90 samples/sec#011loss=7.313480\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:51 INFO 139837668939584] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19001.8310546875, \"sum\": 19001.8310546875, \"min\": 19001.8310546875}}, \"EndTime\": 1579310871.308293, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310852.306176}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:51 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.4177698411 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:51 INFO 139837668939584] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=60, train loss <loss>=7.33988118172\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:51 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:58 INFO 139837668939584] Epoch[61] Batch[0] avg_epoch_loss=7.313182\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:27:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=7.31318235397\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:05 INFO 139837668939584] Epoch[61] Batch[5] avg_epoch_loss=7.307598\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=7.30759827296\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:05 INFO 139837668939584] Epoch[61] Batch [5]#011Speed: 50.57 samples/sec#011loss=7.307598\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] Epoch[61] Batch[10] avg_epoch_loss=7.380942\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=7.4689540863\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] Epoch[61] Batch [10]#011Speed: 47.44 samples/sec#011loss=7.468954\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20749.634981155396, \"sum\": 20749.634981155396, \"min\": 20749.634981155396}}, \"EndTime\": 1579310892.058306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310871.308359}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.1811556855 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=61, train loss <loss>=7.38094182448\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:12 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:19 INFO 139837668939584] Epoch[62] Batch[0] avg_epoch_loss=7.309882\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=7.30988168716\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:26 INFO 139837668939584] Epoch[62] Batch[5] avg_epoch_loss=7.280506\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=7.28050573667\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:26 INFO 139837668939584] Epoch[62] Batch [5]#011Speed: 50.56 samples/sec#011loss=7.280506\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] Epoch[62] Batch[10] avg_epoch_loss=7.287666\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=7.29625864029\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] Epoch[62] Batch [10]#011Speed: 47.57 samples/sec#011loss=7.296259\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20702.27098464966, \"sum\": 20702.27098464966, \"min\": 20702.27098464966}}, \"EndTime\": 1579310912.760869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310892.058359}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.5905981205 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=62, train loss <loss>=7.28766614741\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:32 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:33 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_6595041e-3527-43ac-8970-763489a07bcb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 267.35997200012207, \"sum\": 267.35997200012207, \"min\": 267.35997200012207}}, \"EndTime\": 1579310913.028622, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310912.760932}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:40 INFO 139837668939584] Epoch[63] Batch[0] avg_epoch_loss=7.370263\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=7.37026262283\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:46 INFO 139837668939584] Epoch[63] Batch[5] avg_epoch_loss=7.316074\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=7.31607397397\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:46 INFO 139837668939584] Epoch[63] Batch [5]#011Speed: 50.59 samples/sec#011loss=7.316074\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:52 INFO 139837668939584] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19150.901079177856, \"sum\": 19150.901079177856, \"min\": 19150.901079177856}}, \"EndTime\": 1579310932.17965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310913.028689}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:52 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.060968987 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:52 INFO 139837668939584] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=63, train loss <loss>=7.3359855175\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:52 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:59 INFO 139837668939584] Epoch[64] Batch[0] avg_epoch_loss=7.280447\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:28:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=7.28044748306\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:05 INFO 139837668939584] Epoch[64] Batch[5] avg_epoch_loss=7.316538\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=7.31653825442\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:05 INFO 139837668939584] Epoch[64] Batch [5]#011Speed: 50.32 samples/sec#011loss=7.316538\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:11 INFO 139837668939584] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19240.570068359375, \"sum\": 19240.570068359375, \"min\": 19240.570068359375}}, \"EndTime\": 1579310951.420613, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310932.17973}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9115754489 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=64, train loss <loss>=7.2967148304\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:18 INFO 139837668939584] Epoch[65] Batch[0] avg_epoch_loss=7.394917\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=7.39491701126\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:25 INFO 139837668939584] Epoch[65] Batch[5] avg_epoch_loss=7.340299\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=7.34029944738\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:25 INFO 139837668939584] Epoch[65] Batch [5]#011Speed: 50.21 samples/sec#011loss=7.340299\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:30 INFO 139837668939584] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19187.479972839355, \"sum\": 19187.479972839355, \"min\": 19187.479972839355}}, \"EndTime\": 1579310970.608471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310951.42068}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:30 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.8956358736 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:30 INFO 139837668939584] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=65, train loss <loss>=7.32167596817\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:30 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:38 INFO 139837668939584] Epoch[66] Batch[0] avg_epoch_loss=7.322294\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=7.32229423523\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:44 INFO 139837668939584] Epoch[66] Batch[5] avg_epoch_loss=7.307506\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=7.30750648181\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:44 INFO 139837668939584] Epoch[66] Batch [5]#011Speed: 49.89 samples/sec#011loss=7.307506\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:49 INFO 139837668939584] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19105.844020843506, \"sum\": 19105.844020843506, \"min\": 19105.844020843506}}, \"EndTime\": 1579310989.7147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310970.608538}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:49 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=33.4974390385 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:49 INFO 139837668939584] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=66, train loss <loss>=7.27837939262\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:49 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:49 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_67126eeb-4122-4fb4-910c-997264f010f2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 267.28200912475586, \"sum\": 267.28200912475586, \"min\": 267.28200912475586}}, \"EndTime\": 1579310989.982409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310989.714766}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:57 INFO 139837668939584] Epoch[67] Batch[0] avg_epoch_loss=7.273174\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:29:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=7.27317428589\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:03 INFO 139837668939584] Epoch[67] Batch[5] avg_epoch_loss=7.268678\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=7.26867779096\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:03 INFO 139837668939584] Epoch[67] Batch [5]#011Speed: 50.00 samples/sec#011loss=7.268678\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:09 INFO 139837668939584] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19539.67308998108, \"sum\": 19539.67308998108, \"min\": 19539.67308998108}}, \"EndTime\": 1579311009.522199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579310989.982468}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:09 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.2931312798 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:09 INFO 139837668939584] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=67, train loss <loss>=7.28807067871\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:09 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:17 INFO 139837668939584] Epoch[68] Batch[0] avg_epoch_loss=7.264325\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=7.26432514191\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:23 INFO 139837668939584] Epoch[68] Batch[5] avg_epoch_loss=7.273958\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=7.27395828565\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:23 INFO 139837668939584] Epoch[68] Batch [5]#011Speed: 49.79 samples/sec#011loss=7.273958\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:29 INFO 139837668939584] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19592.1950340271, \"sum\": 19592.1950340271, \"min\": 19592.1950340271}}, \"EndTime\": 1579311029.114735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311009.522255}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:29 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.6148836643 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:29 INFO 139837668939584] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=68, train loss <loss>=7.27073516846\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:29 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:29 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_d4bbdb88-24ca-4a5d-bc45-5b8a96e1bd30-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 238.55018615722656, \"sum\": 238.55018615722656, \"min\": 238.55018615722656}}, \"EndTime\": 1579311029.353694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311029.114796}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:36 INFO 139837668939584] Epoch[69] Batch[0] avg_epoch_loss=7.298276\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=7.29827642441\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:43 INFO 139837668939584] Epoch[69] Batch[5] avg_epoch_loss=7.257779\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=7.25777888298\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:43 INFO 139837668939584] Epoch[69] Batch [5]#011Speed: 49.72 samples/sec#011loss=7.257779\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:48 INFO 139837668939584] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19514.686107635498, \"sum\": 19514.686107635498, \"min\": 19514.686107635498}}, \"EndTime\": 1579311048.868492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311029.353749}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:48 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.5906741133 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:48 INFO 139837668939584] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=69, train loss <loss>=7.28010592461\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:48 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:56 INFO 139837668939584] Epoch[70] Batch[0] avg_epoch_loss=7.230984\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:30:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=7.23098421097\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:03 INFO 139837668939584] Epoch[70] Batch[5] avg_epoch_loss=7.280828\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=7.28082752228\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:03 INFO 139837668939584] Epoch[70] Batch [5]#011Speed: 49.81 samples/sec#011loss=7.280828\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] Epoch[70] Batch[10] avg_epoch_loss=7.330770\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=7.3907002449\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] Epoch[70] Batch [10]#011Speed: 46.79 samples/sec#011loss=7.390700\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21067.997217178345, \"sum\": 21067.997217178345, \"min\": 21067.997217178345}}, \"EndTime\": 1579311069.936868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311048.868559}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0422005679 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=70, train loss <loss>=7.33076966893\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:09 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:17 INFO 139837668939584] Epoch[71] Batch[0] avg_epoch_loss=7.337392\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=7.33739233017\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:24 INFO 139837668939584] Epoch[71] Batch[5] avg_epoch_loss=7.308032\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=7.30803219477\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:24 INFO 139837668939584] Epoch[71] Batch [5]#011Speed: 49.80 samples/sec#011loss=7.308032\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] Epoch[71] Batch[10] avg_epoch_loss=7.338125\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=7.37423686981\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] Epoch[71] Batch [10]#011Speed: 46.78 samples/sec#011loss=7.374237\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21183.254957199097, \"sum\": 21183.254957199097, \"min\": 21183.254957199097}}, \"EndTime\": 1579311091.120479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311069.936928}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.9205164744 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=71, train loss <loss>=7.33812522888\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:31 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:38 INFO 139837668939584] Epoch[72] Batch[0] avg_epoch_loss=7.415632\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=7.41563177109\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:44 INFO 139837668939584] Epoch[72] Batch[5] avg_epoch_loss=7.382740\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=7.38274025917\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:44 INFO 139837668939584] Epoch[72] Batch [5]#011Speed: 49.58 samples/sec#011loss=7.382740\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:50 INFO 139837668939584] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19458.91809463501, \"sum\": 19458.91809463501, \"min\": 19458.91809463501}}, \"EndTime\": 1579311110.579749, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311091.120543}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:50 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.4507181314 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=72, train loss <loss>=7.3535554409\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:50 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:58 INFO 139837668939584] Epoch[73] Batch[0] avg_epoch_loss=7.339816\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:31:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=7.33981609344\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:04 INFO 139837668939584] Epoch[73] Batch[5] avg_epoch_loss=7.301086\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=7.30108634631\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:04 INFO 139837668939584] Epoch[73] Batch [5]#011Speed: 48.78 samples/sec#011loss=7.301086\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] Epoch[73] Batch[10] avg_epoch_loss=7.313946\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=7.32937850952\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] Epoch[73] Batch [10]#011Speed: 45.96 samples/sec#011loss=7.329379\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21364.459991455078, \"sum\": 21364.459991455078, \"min\": 21364.459991455078}}, \"EndTime\": 1579311131.944602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311110.579817}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.5943947586 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=73, train loss <loss>=7.3139464205\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:19 INFO 139837668939584] Epoch[74] Batch[0] avg_epoch_loss=7.335176\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=7.33517551422\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:26 INFO 139837668939584] Epoch[74] Batch[5] avg_epoch_loss=7.347558\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=7.34755818049\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:26 INFO 139837668939584] Epoch[74] Batch [5]#011Speed: 48.91 samples/sec#011loss=7.347558\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] Epoch[74] Batch[10] avg_epoch_loss=7.295821\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=7.23373708725\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] Epoch[74] Batch [10]#011Speed: 46.48 samples/sec#011loss=7.233737\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21069.804906845093, \"sum\": 21069.804906845093, \"min\": 21069.804906845093}}, \"EndTime\": 1579311153.014755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311131.944663}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.4700272673 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=74, train loss <loss>=7.29582131993\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:33 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:40 INFO 139837668939584] Epoch[75] Batch[0] avg_epoch_loss=7.339559\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=7.33955907822\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:47 INFO 139837668939584] Epoch[75] Batch[5] avg_epoch_loss=7.328273\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=7.3282734553\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:47 INFO 139837668939584] Epoch[75] Batch [5]#011Speed: 48.81 samples/sec#011loss=7.328273\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] Epoch[75] Batch[10] avg_epoch_loss=7.287811\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=7.23925647736\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] Epoch[75] Batch [10]#011Speed: 46.01 samples/sec#011loss=7.239256\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21274.08003807068, \"sum\": 21274.08003807068, \"min\": 21274.08003807068}}, \"EndTime\": 1579311174.289196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311153.014808}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.6474956072 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=75, train loss <loss>=7.2878111926\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:32:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:01 INFO 139837668939584] Epoch[76] Batch[0] avg_epoch_loss=7.251965\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=7.25196456909\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:08 INFO 139837668939584] Epoch[76] Batch[5] avg_epoch_loss=7.328405\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=7.32840498288\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:08 INFO 139837668939584] Epoch[76] Batch [5]#011Speed: 48.83 samples/sec#011loss=7.328405\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:13 INFO 139837668939584] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19549.497842788696, \"sum\": 19549.497842788696, \"min\": 19549.497842788696}}, \"EndTime\": 1579311193.839042, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311174.289258}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:13 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.9469313419 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:13 INFO 139837668939584] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=76, train loss <loss>=7.33192749023\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:13 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:21 INFO 139837668939584] Epoch[77] Batch[0] avg_epoch_loss=7.257850\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=7.25785017014\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:28 INFO 139837668939584] Epoch[77] Batch[5] avg_epoch_loss=7.232496\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=7.23249586423\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:28 INFO 139837668939584] Epoch[77] Batch [5]#011Speed: 47.76 samples/sec#011loss=7.232496\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] Epoch[77] Batch[10] avg_epoch_loss=7.288080\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=7.35478010178\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] Epoch[77] Batch [10]#011Speed: 45.60 samples/sec#011loss=7.354780\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21572.39294052124, \"sum\": 21572.39294052124, \"min\": 21572.39294052124}}, \"EndTime\": 1579311215.411823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311193.839111}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.9190128977 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=77, train loss <loss>=7.28807960857\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:35 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:42 INFO 139837668939584] Epoch[78] Batch[0] avg_epoch_loss=7.315051\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=7.31505060196\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:49 INFO 139837668939584] Epoch[78] Batch[5] avg_epoch_loss=7.278322\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=7.27832158407\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:49 INFO 139837668939584] Epoch[78] Batch [5]#011Speed: 48.36 samples/sec#011loss=7.278322\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:55 INFO 139837668939584] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19874.522924423218, \"sum\": 19874.522924423218, \"min\": 19874.522924423218}}, \"EndTime\": 1579311235.286703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311215.411887}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:55 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.0509223279 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:55 INFO 139837668939584] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=78, train loss <loss>=7.26973342896\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:55 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:33:55 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_9fe84686-683f-4664-8a92-8db6c615f709-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 275.2718925476074, \"sum\": 275.2718925476074, \"min\": 275.2718925476074}}, \"EndTime\": 1579311235.56242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311235.286773}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:02 INFO 139837668939584] Epoch[79] Batch[0] avg_epoch_loss=7.360136\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=7.3601360321\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:09 INFO 139837668939584] Epoch[79] Batch[5] avg_epoch_loss=7.341788\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=7.34178781509\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:09 INFO 139837668939584] Epoch[79] Batch [5]#011Speed: 48.22 samples/sec#011loss=7.341788\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:15 INFO 139837668939584] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19717.11802482605, \"sum\": 19717.11802482605, \"min\": 19717.11802482605}}, \"EndTime\": 1579311255.279653, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311235.562475}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.3288220036 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=79, train loss <loss>=7.31944046021\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:15 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:23 INFO 139837668939584] Epoch[80] Batch[0] avg_epoch_loss=7.174625\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=7.17462491989\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:29 INFO 139837668939584] Epoch[80] Batch[5] avg_epoch_loss=7.277569\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=7.27756897608\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:29 INFO 139837668939584] Epoch[80] Batch [5]#011Speed: 48.51 samples/sec#011loss=7.277569\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] Epoch[80] Batch[10] avg_epoch_loss=7.290435\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=7.30587339401\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] Epoch[80] Batch [10]#011Speed: 44.56 samples/sec#011loss=7.305873\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21795.12310028076, \"sum\": 21795.12310028076, \"min\": 21795.12310028076}}, \"EndTime\": 1579311277.075154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311255.279721}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.3830394972 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=80, train loss <loss>=7.2904346206\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:37 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:44 INFO 139837668939584] Epoch[81] Batch[0] avg_epoch_loss=7.234337\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=7.23433685303\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:51 INFO 139837668939584] Epoch[81] Batch[5] avg_epoch_loss=7.259306\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=7.25930587451\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:51 INFO 139837668939584] Epoch[81] Batch [5]#011Speed: 47.99 samples/sec#011loss=7.259306\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:57 INFO 139837668939584] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20063.621997833252, \"sum\": 20063.621997833252, \"min\": 20063.621997833252}}, \"EndTime\": 1579311297.139115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311277.075215}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:57 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.7488561117 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:57 INFO 139837668939584] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=81, train loss <loss>=7.26915249825\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:57 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:34:57 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_54b60773-b96f-40a2-808a-6de47b1bb0b0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 257.69591331481934, \"sum\": 257.69591331481934, \"min\": 257.69591331481934}}, \"EndTime\": 1579311297.397217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311297.13918}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:04 INFO 139837668939584] Epoch[82] Batch[0] avg_epoch_loss=7.315217\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=7.31521749496\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:11 INFO 139837668939584] Epoch[82] Batch[5] avg_epoch_loss=7.261610\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=7.26160987218\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:11 INFO 139837668939584] Epoch[82] Batch [5]#011Speed: 48.36 samples/sec#011loss=7.261610\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:17 INFO 139837668939584] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19625.914096832275, \"sum\": 19625.914096832275, \"min\": 19625.914096832275}}, \"EndTime\": 1579311317.023235, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311297.397269}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:17 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.8264342202 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:17 INFO 139837668939584] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=82, train loss <loss>=7.24100751877\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:17 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:17 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_ed9fce91-6fd9-485f-9e97-36b6716f50e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 255.44404983520508, \"sum\": 255.44404983520508, \"min\": 255.44404983520508}}, \"EndTime\": 1579311317.279094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311317.023304}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:25 INFO 139837668939584] Epoch[83] Batch[0] avg_epoch_loss=7.339476\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=7.33947563171\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:31 INFO 139837668939584] Epoch[83] Batch[5] avg_epoch_loss=7.292735\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=7.29273454348\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:31 INFO 139837668939584] Epoch[83] Batch [5]#011Speed: 47.43 samples/sec#011loss=7.292735\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] Epoch[83] Batch[10] avg_epoch_loss=7.264958\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=7.23162717819\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] Epoch[83] Batch [10]#011Speed: 44.96 samples/sec#011loss=7.231627\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21596.01092338562, \"sum\": 21596.01092338562, \"min\": 21596.01092338562}}, \"EndTime\": 1579311338.875216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311317.279152}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.2369394652 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=83, train loss <loss>=7.26495846835\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:38 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:46 INFO 139837668939584] Epoch[84] Batch[0] avg_epoch_loss=7.378041\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=7.37804079056\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:53 INFO 139837668939584] Epoch[84] Batch[5] avg_epoch_loss=7.299199\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=7.29919926325\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:53 INFO 139837668939584] Epoch[84] Batch [5]#011Speed: 47.20 samples/sec#011loss=7.299199\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:59 INFO 139837668939584] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20132.411003112793, \"sum\": 20132.411003112793, \"min\": 20132.411003112793}}, \"EndTime\": 1579311359.007968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311338.875277}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:59 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.3920108189 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:59 INFO 139837668939584] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=84, train loss <loss>=7.26557145119\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:35:59 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:06 INFO 139837668939584] Epoch[85] Batch[0] avg_epoch_loss=7.226312\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=7.22631168365\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:12 INFO 139837668939584] Epoch[85] Batch[5] avg_epoch_loss=7.287621\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=7.28762070338\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:12 INFO 139837668939584] Epoch[85] Batch [5]#011Speed: 47.73 samples/sec#011loss=7.287621\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:18 INFO 139837668939584] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19703.476905822754, \"sum\": 19703.476905822754, \"min\": 19703.476905822754}}, \"EndTime\": 1579311378.711829, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311359.008039}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:18 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.2483180413 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:18 INFO 139837668939584] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=85, train loss <loss>=7.30521936417\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:18 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:26 INFO 139837668939584] Epoch[86] Batch[0] avg_epoch_loss=7.255405\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=7.25540542603\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:33 INFO 139837668939584] Epoch[86] Batch[5] avg_epoch_loss=7.247573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=7.24757321676\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:33 INFO 139837668939584] Epoch[86] Batch [5]#011Speed: 47.89 samples/sec#011loss=7.247573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] Epoch[86] Batch[10] avg_epoch_loss=7.267172\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=7.29068994522\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] Epoch[86] Batch [10]#011Speed: 44.39 samples/sec#011loss=7.290690\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21888.036012649536, \"sum\": 21888.036012649536, \"min\": 21888.036012649536}}, \"EndTime\": 1579311400.600239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311378.711898}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0213801528 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=86, train loss <loss>=7.26717172969\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:40 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:48 INFO 139837668939584] Epoch[87] Batch[0] avg_epoch_loss=7.219059\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=7.21905899048\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:55 INFO 139837668939584] Epoch[87] Batch[5] avg_epoch_loss=7.271664\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=7.2716644605\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:36:55 INFO 139837668939584] Epoch[87] Batch [5]#011Speed: 47.19 samples/sec#011loss=7.271664\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] Epoch[87] Batch[10] avg_epoch_loss=7.248043\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=7.2196972847\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] Epoch[87] Batch [10]#011Speed: 44.42 samples/sec#011loss=7.219697\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21918.792009353638, \"sum\": 21918.792009353638, \"min\": 21918.792009353638}}, \"EndTime\": 1579311422.519376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311400.600302}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6091742318 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=87, train loss <loss>=7.24804301695\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:02 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:10 INFO 139837668939584] Epoch[88] Batch[0] avg_epoch_loss=7.297351\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=7.29735088348\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:16 INFO 139837668939584] Epoch[88] Batch[5] avg_epoch_loss=7.271914\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=7.2719142437\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:16 INFO 139837668939584] Epoch[88] Batch [5]#011Speed: 47.16 samples/sec#011loss=7.271914\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] Epoch[88] Batch[10] avg_epoch_loss=7.286909\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=7.30490236282\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] Epoch[88] Batch [10]#011Speed: 45.07 samples/sec#011loss=7.304902\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21459.686994552612, \"sum\": 21459.686994552612, \"min\": 21459.686994552612}}, \"EndTime\": 1579311443.97941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311422.519437}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0562301505 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=88, train loss <loss>=7.2869088433\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:23 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:31 INFO 139837668939584] Epoch[89] Batch[0] avg_epoch_loss=7.267337\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=7.2673368454\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:38 INFO 139837668939584] Epoch[89] Batch[5] avg_epoch_loss=7.262802\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=7.26280212402\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:38 INFO 139837668939584] Epoch[89] Batch [5]#011Speed: 47.18 samples/sec#011loss=7.262802\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] Epoch[89] Batch[10] avg_epoch_loss=7.240889\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=7.21459302902\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] Epoch[89] Batch [10]#011Speed: 43.67 samples/sec#011loss=7.214593\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21803.781986236572, \"sum\": 21803.781986236572, \"min\": 21803.781986236572}}, \"EndTime\": 1579311465.783534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311443.979471}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0864156327 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=89, train loss <loss>=7.24088889902\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:45 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:46 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_2b8d7d0e-4ad3-4688-9f5c-10249d75c848-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 240.85593223571777, \"sum\": 240.85593223571777, \"min\": 240.85593223571777}}, \"EndTime\": 1579311466.02472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311465.783587}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:53 INFO 139837668939584] Epoch[90] Batch[0] avg_epoch_loss=7.374615\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:37:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=7.37461519241\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:00 INFO 139837668939584] Epoch[90] Batch[5] avg_epoch_loss=7.305179\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=7.30517919858\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:00 INFO 139837668939584] Epoch[90] Batch [5]#011Speed: 46.74 samples/sec#011loss=7.305179\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] Epoch[90] Batch[10] avg_epoch_loss=7.356331\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=7.41771411896\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] Epoch[90] Batch [10]#011Speed: 44.39 samples/sec#011loss=7.417714\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21765.886068344116, \"sum\": 21765.886068344116, \"min\": 21765.886068344116}}, \"EndTime\": 1579311487.790724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311466.024784}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8171720918 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] #quality_metric: host=algo-1, epoch=90, train loss <loss>=7.35633143512\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:07 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:15 INFO 139837668939584] Epoch[91] Batch[0] avg_epoch_loss=7.253299\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=7.2532992363\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:22 INFO 139837668939584] Epoch[91] Batch[5] avg_epoch_loss=7.295880\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=7.29587960243\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:22 INFO 139837668939584] Epoch[91] Batch [5]#011Speed: 46.89 samples/sec#011loss=7.295880\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] Epoch[91] Batch[10] avg_epoch_loss=7.265511\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=7.22906847\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] Epoch[91] Batch [10]#011Speed: 44.51 samples/sec#011loss=7.229068\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21893.368005752563, \"sum\": 21893.368005752563, \"min\": 21893.368005752563}}, \"EndTime\": 1579311509.684446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311487.790789}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0089781985 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=91, train loss <loss>=7.26551090587\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:29 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:37 INFO 139837668939584] Epoch[92] Batch[0] avg_epoch_loss=7.181393\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=7.18139266968\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:43 INFO 139837668939584] Epoch[92] Batch[5] avg_epoch_loss=7.248665\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=7.24866469701\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:43 INFO 139837668939584] Epoch[92] Batch [5]#011Speed: 46.61 samples/sec#011loss=7.248665\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:49 INFO 139837668939584] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20251.598834991455, \"sum\": 20251.598834991455, \"min\": 20251.598834991455}}, \"EndTime\": 1579311529.936423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311509.684496}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:49 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.4172025219 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:49 INFO 139837668939584] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=92, train loss <loss>=7.25581445694\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:49 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:57 INFO 139837668939584] Epoch[93] Batch[0] avg_epoch_loss=7.218224\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:38:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=7.21822404861\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:04 INFO 139837668939584] Epoch[93] Batch[5] avg_epoch_loss=7.234276\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=7.23427565893\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:04 INFO 139837668939584] Epoch[93] Batch [5]#011Speed: 46.60 samples/sec#011loss=7.234276\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:09 INFO 139837668939584] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20038.131952285767, \"sum\": 20038.131952285767, \"min\": 20038.131952285767}}, \"EndTime\": 1579311549.974935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311529.93649}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:09 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.9427641358 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:09 INFO 139837668939584] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=93, train loss <loss>=7.22341799736\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:09 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:10 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_b8c0dd36-559f-40c7-9ba7-8e577df69441-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 270.10607719421387, \"sum\": 270.10607719421387, \"min\": 270.10607719421387}}, \"EndTime\": 1579311550.245455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311549.975001}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:17 INFO 139837668939584] Epoch[94] Batch[0] avg_epoch_loss=7.213120\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=7.21312046051\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:24 INFO 139837668939584] Epoch[94] Batch[5] avg_epoch_loss=7.248339\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=7.24833909671\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:24 INFO 139837668939584] Epoch[94] Batch [5]#011Speed: 46.86 samples/sec#011loss=7.248339\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] Epoch[94] Batch[10] avg_epoch_loss=7.207215\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=7.15786561966\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] Epoch[94] Batch [10]#011Speed: 44.27 samples/sec#011loss=7.157866\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21658.241987228394, \"sum\": 21658.241987228394, \"min\": 21658.241987228394}}, \"EndTime\": 1579311571.903798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311550.245508}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6421883821 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=94, train loss <loss>=7.20721478896\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:31 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:32 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_90561f35-250a-431e-badd-6f6435fde199-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 251.71709060668945, \"sum\": 251.71709060668945, \"min\": 251.71709060668945}}, \"EndTime\": 1579311572.155854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311571.90385}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:39 INFO 139837668939584] Epoch[95] Batch[0] avg_epoch_loss=7.230656\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=7.23065567017\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:46 INFO 139837668939584] Epoch[95] Batch[5] avg_epoch_loss=7.301498\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=7.30149817467\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:46 INFO 139837668939584] Epoch[95] Batch [5]#011Speed: 46.97 samples/sec#011loss=7.301498\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] Epoch[95] Batch[10] avg_epoch_loss=7.255562\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=7.20043783188\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] Epoch[95] Batch [10]#011Speed: 43.62 samples/sec#011loss=7.200438\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21973.910093307495, \"sum\": 21973.910093307495, \"min\": 21973.910093307495}}, \"EndTime\": 1579311594.129873, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311572.155907}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.4438828313 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=95, train loss <loss>=7.25556165522\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:39:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:02 INFO 139837668939584] Epoch[96] Batch[0] avg_epoch_loss=7.164165\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=7.16416549683\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:08 INFO 139837668939584] Epoch[96] Batch[5] avg_epoch_loss=7.254378\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=7.25437839826\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:08 INFO 139837668939584] Epoch[96] Batch [5]#011Speed: 46.64 samples/sec#011loss=7.254378\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] Epoch[96] Batch[10] avg_epoch_loss=7.281980\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=7.315102005\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] Epoch[96] Batch [10]#011Speed: 44.01 samples/sec#011loss=7.315102\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22117.7659034729, \"sum\": 22117.7659034729, \"min\": 22117.7659034729}}, \"EndTime\": 1579311616.248005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311594.129936}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1566286744 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=96, train loss <loss>=7.28198003769\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:16 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:23 INFO 139837668939584] Epoch[97] Batch[0] avg_epoch_loss=7.223015\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=7.22301530838\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:30 INFO 139837668939584] Epoch[97] Batch[5] avg_epoch_loss=7.289324\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=7.28932404518\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:30 INFO 139837668939584] Epoch[97] Batch [5]#011Speed: 46.86 samples/sec#011loss=7.289324\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:36 INFO 139837668939584] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20369.504928588867, \"sum\": 20369.504928588867, \"min\": 20369.504928588867}}, \"EndTime\": 1579311636.617858, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311616.248067}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:36 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.2720872638 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:36 INFO 139837668939584] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=97, train loss <loss>=7.24246411324\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:36 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:44 INFO 139837668939584] Epoch[98] Batch[0] avg_epoch_loss=7.340152\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=7.34015226364\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:51 INFO 139837668939584] Epoch[98] Batch[5] avg_epoch_loss=7.276146\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=7.2761464119\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:51 INFO 139837668939584] Epoch[98] Batch [5]#011Speed: 46.44 samples/sec#011loss=7.276146\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] Epoch[98] Batch[10] avg_epoch_loss=7.242328\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=7.20174694061\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] Epoch[98] Batch [10]#011Speed: 43.65 samples/sec#011loss=7.201747\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21937.409162521362, \"sum\": 21937.409162521362, \"min\": 21937.409162521362}}, \"EndTime\": 1579311658.555646, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311636.617925}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.3105425847 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=98, train loss <loss>=7.2423284704\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:40:58 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:06 INFO 139837668939584] Epoch[99] Batch[0] avg_epoch_loss=7.241532\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=7.24153232574\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:13 INFO 139837668939584] Epoch[99] Batch[5] avg_epoch_loss=7.251903\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=7.25190321604\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:13 INFO 139837668939584] Epoch[99] Batch [5]#011Speed: 46.79 samples/sec#011loss=7.251903\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] Epoch[99] Batch[10] avg_epoch_loss=7.229247\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=7.20206012726\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] Epoch[99] Batch [10]#011Speed: 43.94 samples/sec#011loss=7.202060\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22004.553079605103, \"sum\": 22004.553079605103, \"min\": 22004.553079605103}}, \"EndTime\": 1579311680.560576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311658.555707}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.5390207273 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=99, train loss <loss>=7.2292472666\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:20 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:28 INFO 139837668939584] Epoch[100] Batch[0] avg_epoch_loss=7.239313\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=7.23931312561\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:34 INFO 139837668939584] Epoch[100] Batch[5] avg_epoch_loss=7.209028\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=7.20902808507\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:34 INFO 139837668939584] Epoch[100] Batch [5]#011Speed: 46.70 samples/sec#011loss=7.209028\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:40 INFO 139837668939584] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20270.86114883423, \"sum\": 20270.86114883423, \"min\": 20270.86114883423}}, \"EndTime\": 1579311700.831795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311680.560629}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:40 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0296183549 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:40 INFO 139837668939584] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=100, train loss <loss>=7.21950035095\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:40 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:48 INFO 139837668939584] Epoch[101] Batch[0] avg_epoch_loss=7.257820\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=7.25782012939\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:55 INFO 139837668939584] Epoch[101] Batch[5] avg_epoch_loss=7.247589\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=7.24758879344\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:41:55 INFO 139837668939584] Epoch[101] Batch [5]#011Speed: 46.68 samples/sec#011loss=7.247589\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] Epoch[101] Batch[10] avg_epoch_loss=7.233682\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=7.21699285507\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] Epoch[101] Batch [10]#011Speed: 44.02 samples/sec#011loss=7.216993\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21731.632947921753, \"sum\": 21731.632947921753, \"min\": 21731.632947921753}}, \"EndTime\": 1579311722.563798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311700.831859}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6801310745 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=101, train loss <loss>=7.23368154873\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:02 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:10 INFO 139837668939584] Epoch[102] Batch[0] avg_epoch_loss=7.213753\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=7.21375274658\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:17 INFO 139837668939584] Epoch[102] Batch[5] avg_epoch_loss=7.180695\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=7.18069473902\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:17 INFO 139837668939584] Epoch[102] Batch [5]#011Speed: 46.83 samples/sec#011loss=7.180695\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] Epoch[102] Batch[10] avg_epoch_loss=7.167285\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=7.15119256973\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] Epoch[102] Batch [10]#011Speed: 43.99 samples/sec#011loss=7.151193\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21821.696996688843, \"sum\": 21821.696996688843, \"min\": 21821.696996688843}}, \"EndTime\": 1579311744.385835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311722.56385}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8325733115 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=102, train loss <loss>=7.16728466207\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:24 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_f98834cf-f74d-4cf9-adf4-4d26694e5116-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 273.70595932006836, \"sum\": 273.70595932006836, \"min\": 273.70595932006836}}, \"EndTime\": 1579311744.659926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311744.385896}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:32 INFO 139837668939584] Epoch[103] Batch[0] avg_epoch_loss=7.214451\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=7.21445131302\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:39 INFO 139837668939584] Epoch[103] Batch[5] avg_epoch_loss=7.219688\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=7.21968841553\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:39 INFO 139837668939584] Epoch[103] Batch [5]#011Speed: 46.34 samples/sec#011loss=7.219688\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] Epoch[103] Batch[10] avg_epoch_loss=7.199346\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=7.17493543625\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] Epoch[103] Batch [10]#011Speed: 43.83 samples/sec#011loss=7.174935\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22205.425024032593, \"sum\": 22205.425024032593, \"min\": 22205.425024032593}}, \"EndTime\": 1579311766.865475, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311744.659992}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.677308126 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=103, train loss <loss>=7.19934615222\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:46 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:54 INFO 139837668939584] Epoch[104] Batch[0] avg_epoch_loss=7.174100\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:42:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=7.17409992218\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:01 INFO 139837668939584] Epoch[104] Batch[5] avg_epoch_loss=7.226882\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=7.22688245773\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:01 INFO 139837668939584] Epoch[104] Batch [5]#011Speed: 46.37 samples/sec#011loss=7.226882\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] Epoch[104] Batch[10] avg_epoch_loss=7.205854\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=7.18062009811\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] Epoch[104] Batch [10]#011Speed: 43.62 samples/sec#011loss=7.180620\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21991.148948669434, \"sum\": 21991.148948669434, \"min\": 21991.148948669434}}, \"EndTime\": 1579311788.856971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311766.865538}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.7846046805 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=104, train loss <loss>=7.20585411245\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:08 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:16 INFO 139837668939584] Epoch[105] Batch[0] avg_epoch_loss=7.199678\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=7.19967794418\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:23 INFO 139837668939584] Epoch[105] Batch[5] avg_epoch_loss=7.238747\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=7.23874656359\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:23 INFO 139837668939584] Epoch[105] Batch [5]#011Speed: 46.68 samples/sec#011loss=7.238747\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] Epoch[105] Batch[10] avg_epoch_loss=7.235636\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=7.23190326691\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] Epoch[105] Batch [10]#011Speed: 44.05 samples/sec#011loss=7.231903\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21995.30577659607, \"sum\": 21995.30577659607, \"min\": 21995.30577659607}}, \"EndTime\": 1579311810.852568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311788.857023}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.5063869399 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=105, train loss <loss>=7.23563597419\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:30 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:38 INFO 139837668939584] Epoch[106] Batch[0] avg_epoch_loss=7.336700\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=7.33670043945\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:45 INFO 139837668939584] Epoch[106] Batch[5] avg_epoch_loss=7.239111\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=7.23911062876\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:45 INFO 139837668939584] Epoch[106] Batch [5]#011Speed: 46.92 samples/sec#011loss=7.239111\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] Epoch[106] Batch[10] avg_epoch_loss=7.207100\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=7.16868629456\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] Epoch[106] Batch [10]#011Speed: 43.86 samples/sec#011loss=7.168686\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21816.422939300537, \"sum\": 21816.422939300537, \"min\": 21816.422939300537}}, \"EndTime\": 1579311832.669339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311810.852629}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0689691108 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=106, train loss <loss>=7.20709956776\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:43:52 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:00 INFO 139837668939584] Epoch[107] Batch[0] avg_epoch_loss=7.268994\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=7.26899385452\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:06 INFO 139837668939584] Epoch[107] Batch[5] avg_epoch_loss=7.258983\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=7.25898313522\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:06 INFO 139837668939584] Epoch[107] Batch [5]#011Speed: 46.49 samples/sec#011loss=7.258983\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:12 INFO 139837668939584] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20268.81694793701, \"sum\": 20268.81694793701, \"min\": 20268.81694793701}}, \"EndTime\": 1579311852.938498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311832.669398}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:12 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.6873895174 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:12 INFO 139837668939584] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=107, train loss <loss>=7.25700297356\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:12 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:21 INFO 139837668939584] Epoch[108] Batch[0] avg_epoch_loss=7.343165\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=7.34316539764\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:27 INFO 139837668939584] Epoch[108] Batch[5] avg_epoch_loss=7.286325\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=7.28632473946\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:27 INFO 139837668939584] Epoch[108] Batch [5]#011Speed: 46.90 samples/sec#011loss=7.286325\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] Epoch[108] Batch[10] avg_epoch_loss=7.260577\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=7.2296792984\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] Epoch[108] Batch [10]#011Speed: 43.35 samples/sec#011loss=7.229679\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22439.83006477356, \"sum\": 22439.83006477356, \"min\": 22439.83006477356}}, \"EndTime\": 1579311875.378727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311852.938563}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.8378911523 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=108, train loss <loss>=7.2605768117\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:35 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:42 INFO 139837668939584] Epoch[109] Batch[0] avg_epoch_loss=7.248014\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=7.24801445007\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:49 INFO 139837668939584] Epoch[109] Batch[5] avg_epoch_loss=7.242352\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=7.24235224724\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:49 INFO 139837668939584] Epoch[109] Batch [5]#011Speed: 46.65 samples/sec#011loss=7.242352\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:55 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20296.40483856201, \"sum\": 20296.40483856201, \"min\": 20296.40483856201}}, \"EndTime\": 1579311895.675496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311875.378791}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:55 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.2369065803 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:55 INFO 139837668939584] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=109, train loss <loss>=7.22877588272\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:44:55 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:03 INFO 139837668939584] Epoch[110] Batch[0] avg_epoch_loss=7.219208\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=7.21920776367\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:10 INFO 139837668939584] Epoch[110] Batch[5] avg_epoch_loss=7.170818\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=7.17081816991\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:10 INFO 139837668939584] Epoch[110] Batch [5]#011Speed: 46.56 samples/sec#011loss=7.170818\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:16 INFO 139837668939584] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20426.414012908936, \"sum\": 20426.414012908936, \"min\": 20426.414012908936}}, \"EndTime\": 1579311916.102293, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311895.675565}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:16 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.7443624183 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:16 INFO 139837668939584] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=110, train loss <loss>=7.2019194603\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:16 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:23 INFO 139837668939584] Epoch[111] Batch[0] avg_epoch_loss=7.190739\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=7.19073915482\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:29 INFO 139837668939584] Epoch[111] Batch[5] avg_epoch_loss=7.194494\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=7.19449408849\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:29 INFO 139837668939584] Epoch[111] Batch [5]#011Speed: 46.83 samples/sec#011loss=7.194494\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:34 INFO 139837668939584] processed a total of 573 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18324.921131134033, \"sum\": 18324.921131134033, \"min\": 18324.921131134033}}, \"EndTime\": 1579311934.427602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311916.102359}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:34 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.268724272 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:34 INFO 139837668939584] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=111, train loss <loss>=7.20328452852\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:34 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:41 INFO 139837668939584] Epoch[112] Batch[0] avg_epoch_loss=7.235560\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=7.23555994034\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:48 INFO 139837668939584] Epoch[112] Batch[5] avg_epoch_loss=7.238787\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=7.23878685633\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:48 INFO 139837668939584] Epoch[112] Batch [5]#011Speed: 46.83 samples/sec#011loss=7.238787\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:54 INFO 139837668939584] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20220.79300880432, \"sum\": 20220.79300880432, \"min\": 20220.79300880432}}, \"EndTime\": 1579311954.648777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311934.427672}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0569909928 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=112, train loss <loss>=7.19840755463\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:45:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:02 INFO 139837668939584] Epoch[113] Batch[0] avg_epoch_loss=7.322632\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=7.32263183594\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:08 INFO 139837668939584] Epoch[113] Batch[5] avg_epoch_loss=7.229458\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=7.22945753733\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:09 INFO 139837668939584] Epoch[113] Batch [5]#011Speed: 46.53 samples/sec#011loss=7.229458\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:14 INFO 139837668939584] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20184.669971466064, \"sum\": 20184.669971466064, \"min\": 20184.669971466064}}, \"EndTime\": 1579311974.833824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311954.648843}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.8648610704 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=113, train loss <loss>=7.22533445358\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:14 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:22 INFO 139837668939584] Epoch[114] Batch[0] avg_epoch_loss=7.199594\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=7.19959402084\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:29 INFO 139837668939584] Epoch[114] Batch[5] avg_epoch_loss=7.188776\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=7.1887755394\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:29 INFO 139837668939584] Epoch[114] Batch [5]#011Speed: 46.09 samples/sec#011loss=7.188776\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] Epoch[114] Batch[10] avg_epoch_loss=7.183215\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=7.17654209137\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] Epoch[114] Batch [10]#011Speed: 44.30 samples/sec#011loss=7.176542\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22184.06391143799, \"sum\": 22184.06391143799, \"min\": 22184.06391143799}}, \"EndTime\": 1579311997.018262, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311974.83389}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.6525042483 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=114, train loss <loss>=7.1832148812\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:37 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:44 INFO 139837668939584] Epoch[115] Batch[0] avg_epoch_loss=7.156045\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=7.15604543686\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:51 INFO 139837668939584] Epoch[115] Batch[5] avg_epoch_loss=7.205782\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=7.20578169823\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:51 INFO 139837668939584] Epoch[115] Batch [5]#011Speed: 46.63 samples/sec#011loss=7.205782\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] Epoch[115] Batch[10] avg_epoch_loss=7.225120\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=7.24832582474\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] Epoch[115] Batch [10]#011Speed: 43.68 samples/sec#011loss=7.248326\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22004.085063934326, \"sum\": 22004.085063934326, \"min\": 22004.085063934326}}, \"EndTime\": 1579312019.022724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579311997.018326}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.4489540439 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=115, train loss <loss>=7.22511993755\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:46:59 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:06 INFO 139837668939584] Epoch[116] Batch[0] avg_epoch_loss=7.182151\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=7.18215084076\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:13 INFO 139837668939584] Epoch[116] Batch[5] avg_epoch_loss=7.165372\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=7.16537173589\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:13 INFO 139837668939584] Epoch[116] Batch [5]#011Speed: 46.71 samples/sec#011loss=7.165372\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] Epoch[116] Batch[10] avg_epoch_loss=7.159419\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=7.15227680206\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] Epoch[116] Batch [10]#011Speed: 43.78 samples/sec#011loss=7.152277\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21944.477081298828, \"sum\": 21944.477081298828, \"min\": 21944.477081298828}}, \"EndTime\": 1579312040.967555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312019.022787}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8023623383 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=116, train loss <loss>=7.15941949324\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:20 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:21 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_9e2f1f9c-f123-4b46-9a3e-1c8de6579b34-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 261.20495796203613, \"sum\": 261.20495796203613, \"min\": 261.20495796203613}}, \"EndTime\": 1579312041.22915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312040.967619}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:28 INFO 139837668939584] Epoch[117] Batch[0] avg_epoch_loss=7.247578\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=7.24757766724\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:35 INFO 139837668939584] Epoch[117] Batch[5] avg_epoch_loss=7.191168\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=7.191167593\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:35 INFO 139837668939584] Epoch[117] Batch [5]#011Speed: 46.20 samples/sec#011loss=7.191168\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:41 INFO 139837668939584] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20150.72202682495, \"sum\": 20150.72202682495, \"min\": 20150.72202682495}}, \"EndTime\": 1579312061.379982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312041.229203}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:41 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1724647421 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:41 INFO 139837668939584] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=117, train loss <loss>=7.18248133659\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:41 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:49 INFO 139837668939584] Epoch[118] Batch[0] avg_epoch_loss=7.302410\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=7.30241012573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:55 INFO 139837668939584] Epoch[118] Batch[5] avg_epoch_loss=7.260300\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=7.26030015945\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:47:55 INFO 139837668939584] Epoch[118] Batch [5]#011Speed: 46.74 samples/sec#011loss=7.260300\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:01 INFO 139837668939584] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20422.09506034851, \"sum\": 20422.09506034851, \"min\": 20422.09506034851}}, \"EndTime\": 1579312081.80246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312061.380052}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.4570858276 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=118, train loss <loss>=7.24648094177\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:09 INFO 139837668939584] Epoch[119] Batch[0] avg_epoch_loss=7.093330\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=7.0933303833\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:16 INFO 139837668939584] Epoch[119] Batch[5] avg_epoch_loss=7.164201\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=7.16420094172\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:16 INFO 139837668939584] Epoch[119] Batch [5]#011Speed: 45.86 samples/sec#011loss=7.164201\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:22 INFO 139837668939584] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20488.71088027954, \"sum\": 20488.71088027954, \"min\": 20488.71088027954}}, \"EndTime\": 1579312102.291549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312081.802517}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:22 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.7973046215 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:22 INFO 139837668939584] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=119, train loss <loss>=7.11794672012\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:22 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:22 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_091e3e6e-34a4-4114-8cdc-08c9ff04450e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 250.86021423339844, \"sum\": 250.86021423339844, \"min\": 250.86021423339844}}, \"EndTime\": 1579312102.542825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312102.291615}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:29 INFO 139837668939584] Epoch[120] Batch[0] avg_epoch_loss=7.183197\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=7.18319702148\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:36 INFO 139837668939584] Epoch[120] Batch[5] avg_epoch_loss=7.197097\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=7.19709698359\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:36 INFO 139837668939584] Epoch[120] Batch [5]#011Speed: 45.96 samples/sec#011loss=7.197097\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:42 INFO 139837668939584] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20274.178981781006, \"sum\": 20274.178981781006, \"min\": 20274.178981781006}}, \"EndTime\": 1579312122.817132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312102.542892}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:42 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.5941471714 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:42 INFO 139837668939584] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=120, train loss <loss>=7.16842665672\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:42 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:50 INFO 139837668939584] Epoch[121] Batch[0] avg_epoch_loss=7.352707\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=7.35270690918\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:57 INFO 139837668939584] Epoch[121] Batch[5] avg_epoch_loss=7.211644\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=7.21164433161\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:48:57 INFO 139837668939584] Epoch[121] Batch [5]#011Speed: 46.24 samples/sec#011loss=7.211644\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] Epoch[121] Batch[10] avg_epoch_loss=7.197348\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=121, batch=10 train loss <loss>=7.18019132614\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] Epoch[121] Batch [10]#011Speed: 43.58 samples/sec#011loss=7.180191\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22200.690031051636, \"sum\": 22200.690031051636, \"min\": 22200.690031051636}}, \"EndTime\": 1579312145.018196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312122.8172}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1340968737 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=121, train loss <loss>=7.19734751094\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:05 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:12 INFO 139837668939584] Epoch[122] Batch[0] avg_epoch_loss=7.158692\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=7.15869235992\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:19 INFO 139837668939584] Epoch[122] Batch[5] avg_epoch_loss=7.191578\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=7.19157799085\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:19 INFO 139837668939584] Epoch[122] Batch [5]#011Speed: 45.68 samples/sec#011loss=7.191578\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:25 INFO 139837668939584] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20547.16992378235, \"sum\": 20547.16992378235, \"min\": 20547.16992378235}}, \"EndTime\": 1579312165.565692, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312145.018246}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:25 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.9043505536 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:25 INFO 139837668939584] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=122, train loss <loss>=7.17950220108\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:25 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:32 INFO 139837668939584] Epoch[123] Batch[0] avg_epoch_loss=7.228766\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=7.22876644135\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:39 INFO 139837668939584] Epoch[123] Batch[5] avg_epoch_loss=7.213711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=7.21371094386\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:39 INFO 139837668939584] Epoch[123] Batch [5]#011Speed: 45.87 samples/sec#011loss=7.213711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:45 INFO 139837668939584] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20087.27502822876, \"sum\": 20087.27502822876, \"min\": 20087.27502822876}}, \"EndTime\": 1579312185.653353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312165.56576}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:45 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.4214706711 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:45 INFO 139837668939584] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=123, train loss <loss>=7.22105112076\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:45 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:52 INFO 139837668939584] Epoch[124] Batch[0] avg_epoch_loss=7.223935\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=7.22393465042\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:59 INFO 139837668939584] Epoch[124] Batch[5] avg_epoch_loss=7.189293\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=7.18929306666\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:49:59 INFO 139837668939584] Epoch[124] Batch [5]#011Speed: 45.82 samples/sec#011loss=7.189293\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:05 INFO 139837668939584] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20247.42293357849, \"sum\": 20247.42293357849, \"min\": 20247.42293357849}}, \"EndTime\": 1579312205.90115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312185.653418}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:05 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.7320405639 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:05 INFO 139837668939584] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=124, train loss <loss>=7.17852606773\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:05 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:13 INFO 139837668939584] Epoch[125] Batch[0] avg_epoch_loss=7.194257\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=7.19425725937\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:20 INFO 139837668939584] Epoch[125] Batch[5] avg_epoch_loss=7.182692\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=7.18269181252\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:20 INFO 139837668939584] Epoch[125] Batch [5]#011Speed: 45.88 samples/sec#011loss=7.182692\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:26 INFO 139837668939584] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20507.614135742188, \"sum\": 20507.614135742188, \"min\": 20507.614135742188}}, \"EndTime\": 1579312226.40914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312205.901215}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:26 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.9399605881 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:26 INFO 139837668939584] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=125, train loss <loss>=7.16357541084\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:26 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:34 INFO 139837668939584] Epoch[126] Batch[0] avg_epoch_loss=7.211833\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=7.21183347702\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:41 INFO 139837668939584] Epoch[126] Batch[5] avg_epoch_loss=7.185282\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=7.1852821509\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:41 INFO 139837668939584] Epoch[126] Batch [5]#011Speed: 46.29 samples/sec#011loss=7.185282\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] Epoch[126] Batch[10] avg_epoch_loss=7.139709\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=7.08502111435\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] Epoch[126] Batch [10]#011Speed: 44.03 samples/sec#011loss=7.085021\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21950.14476776123, \"sum\": 21950.14476776123, \"min\": 21950.14476776123}}, \"EndTime\": 1579312248.359656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312226.409206}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.5213414947 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=126, train loss <loss>=7.13970895247\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:48 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:56 INFO 139837668939584] Epoch[127] Batch[0] avg_epoch_loss=7.100856\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:50:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=7.10085582733\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:03 INFO 139837668939584] Epoch[127] Batch[5] avg_epoch_loss=7.151701\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=7.15170081457\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:03 INFO 139837668939584] Epoch[127] Batch [5]#011Speed: 45.91 samples/sec#011loss=7.151701\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] Epoch[127] Batch[10] avg_epoch_loss=7.125118\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=7.09321889877\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] Epoch[127] Batch [10]#011Speed: 44.28 samples/sec#011loss=7.093219\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22203.34506034851, \"sum\": 22203.34506034851, \"min\": 22203.34506034851}}, \"EndTime\": 1579312270.56333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312248.359709}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.5449768684 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=127, train loss <loss>=7.12511812557\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:10 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:18 INFO 139837668939584] Epoch[128] Batch[0] avg_epoch_loss=7.185433\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=7.18543291092\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:25 INFO 139837668939584] Epoch[128] Batch[5] avg_epoch_loss=7.159810\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=7.15980950991\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:25 INFO 139837668939584] Epoch[128] Batch [5]#011Speed: 46.40 samples/sec#011loss=7.159810\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] Epoch[128] Batch[10] avg_epoch_loss=7.172607\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=7.18796396255\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] Epoch[128] Batch [10]#011Speed: 43.45 samples/sec#011loss=7.187964\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22085.232973098755, \"sum\": 22085.232973098755, \"min\": 22085.232973098755}}, \"EndTime\": 1579312292.648908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312270.563394}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0652334304 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=128, train loss <loss>=7.17260698839\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:32 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:40 INFO 139837668939584] Epoch[129] Batch[0] avg_epoch_loss=7.220747\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=7.22074651718\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:47 INFO 139837668939584] Epoch[129] Batch[5] avg_epoch_loss=7.203294\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=7.20329403877\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:47 INFO 139837668939584] Epoch[129] Batch [5]#011Speed: 46.38 samples/sec#011loss=7.203294\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] Epoch[129] Batch[10] avg_epoch_loss=7.223454\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=7.24764661789\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] Epoch[129] Batch [10]#011Speed: 43.22 samples/sec#011loss=7.247647\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22231.95481300354, \"sum\": 22231.95481300354, \"min\": 22231.95481300354}}, \"EndTime\": 1579312314.881151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312292.64896}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0017411429 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=129, train loss <loss>=7.22345430201\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:51:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:02 INFO 139837668939584] Epoch[130] Batch[0] avg_epoch_loss=7.097759\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=7.09775876999\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:09 INFO 139837668939584] Epoch[130] Batch[5] avg_epoch_loss=7.136089\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=7.13608940442\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:09 INFO 139837668939584] Epoch[130] Batch [5]#011Speed: 46.12 samples/sec#011loss=7.136089\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:15 INFO 139837668939584] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20581.6810131073, \"sum\": 20581.6810131073, \"min\": 20581.6810131073}}, \"EndTime\": 1579312335.463175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312314.881212}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.9496994628 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=130, train loss <loss>=7.13970184326\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:15 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:23 INFO 139837668939584] Epoch[131] Batch[0] avg_epoch_loss=7.121134\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=7.12113380432\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:30 INFO 139837668939584] Epoch[131] Batch[5] avg_epoch_loss=7.130615\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=7.13061451912\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:30 INFO 139837668939584] Epoch[131] Batch [5]#011Speed: 46.36 samples/sec#011loss=7.130615\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] Epoch[131] Batch[10] avg_epoch_loss=7.134232\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=7.13857269287\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] Epoch[131] Batch [10]#011Speed: 43.63 samples/sec#011loss=7.138573\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22102.179050445557, \"sum\": 22102.179050445557, \"min\": 22102.179050445557}}, \"EndTime\": 1579312357.565744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312335.463246}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8611876438 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=131, train loss <loss>=7.13423187082\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:37 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:45 INFO 139837668939584] Epoch[132] Batch[0] avg_epoch_loss=7.214083\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=7.21408319473\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:52 INFO 139837668939584] Epoch[132] Batch[5] avg_epoch_loss=7.207864\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=7.20786436399\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:52 INFO 139837668939584] Epoch[132] Batch [5]#011Speed: 46.08 samples/sec#011loss=7.207864\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:57 INFO 139837668939584] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20383.461952209473, \"sum\": 20383.461952209473, \"min\": 20383.461952209473}}, \"EndTime\": 1579312377.949562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312357.565806}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:57 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.4657315568 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:57 INFO 139837668939584] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=132, train loss <loss>=7.19757781029\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:52:57 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:05 INFO 139837668939584] Epoch[133] Batch[0] avg_epoch_loss=7.238541\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=7.23854064941\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:12 INFO 139837668939584] Epoch[133] Batch[5] avg_epoch_loss=7.133073\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=7.13307325045\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:12 INFO 139837668939584] Epoch[133] Batch [5]#011Speed: 46.73 samples/sec#011loss=7.133073\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:18 INFO 139837668939584] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20141.03102684021, \"sum\": 20141.03102684021, \"min\": 20141.03102684021}}, \"EndTime\": 1579312398.09098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312377.949629}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:18 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8394380493 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:18 INFO 139837668939584] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=133, train loss <loss>=7.11980834007\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:18 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:25 INFO 139837668939584] Epoch[134] Batch[0] avg_epoch_loss=7.175888\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=7.17588758469\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:32 INFO 139837668939584] Epoch[134] Batch[5] avg_epoch_loss=7.185868\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=7.18586778641\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:32 INFO 139837668939584] Epoch[134] Batch [5]#011Speed: 46.50 samples/sec#011loss=7.185868\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:38 INFO 139837668939584] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20275.344848632812, \"sum\": 20275.344848632812, \"min\": 20275.344848632812}}, \"EndTime\": 1579312418.366711, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312398.091048}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:38 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.8747756931 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:38 INFO 139837668939584] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=134, train loss <loss>=7.18532004356\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:38 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:45 INFO 139837668939584] Epoch[135] Batch[0] avg_epoch_loss=7.106792\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=7.10679244995\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:52 INFO 139837668939584] Epoch[135] Batch[5] avg_epoch_loss=7.146696\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=7.14669601123\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:52 INFO 139837668939584] Epoch[135] Batch [5]#011Speed: 46.27 samples/sec#011loss=7.146696\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:58 INFO 139837668939584] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20430.00817298889, \"sum\": 20430.00817298889, \"min\": 20430.00817298889}}, \"EndTime\": 1579312438.797113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312418.366786}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.1795034193 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=135, train loss <loss>=7.14409575462\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:53:58 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:06 INFO 139837668939584] Epoch[136] Batch[0] avg_epoch_loss=7.168369\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=7.16836929321\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:12 INFO 139837668939584] Epoch[136] Batch[5] avg_epoch_loss=7.167839\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=7.16783912977\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:12 INFO 139837668939584] Epoch[136] Batch [5]#011Speed: 46.76 samples/sec#011loss=7.167839\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:18 INFO 139837668939584] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20047.532081604004, \"sum\": 20047.532081604004, \"min\": 20047.532081604004}}, \"EndTime\": 1579312458.844995, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312438.797167}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:18 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1781292381 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:18 INFO 139837668939584] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=136, train loss <loss>=7.19522423744\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:18 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:26 INFO 139837668939584] Epoch[137] Batch[0] avg_epoch_loss=7.192492\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=7.19249153137\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:33 INFO 139837668939584] Epoch[137] Batch[5] avg_epoch_loss=7.118349\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=7.11834891637\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:33 INFO 139837668939584] Epoch[137] Batch [5]#011Speed: 46.58 samples/sec#011loss=7.118349\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] Epoch[137] Batch[10] avg_epoch_loss=7.117248\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=7.11592731476\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] Epoch[137] Batch [10]#011Speed: 44.52 samples/sec#011loss=7.115927\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22086.441040039062, \"sum\": 22086.441040039062, \"min\": 22086.441040039062}}, \"EndTime\": 1579312480.931821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312458.845063}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1541227253 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=137, train loss <loss>=7.11724818837\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:40 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:41 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_60e7b76e-3dc3-4ac4-ba5a-c870097ff58e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 250.72884559631348, \"sum\": 250.72884559631348, \"min\": 250.72884559631348}}, \"EndTime\": 1579312481.182938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312480.931883}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:48 INFO 139837668939584] Epoch[138] Batch[0] avg_epoch_loss=7.211714\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=7.21171426773\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:55 INFO 139837668939584] Epoch[138] Batch[5] avg_epoch_loss=7.148555\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=7.14855535825\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:54:55 INFO 139837668939584] Epoch[138] Batch [5]#011Speed: 46.18 samples/sec#011loss=7.148555\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:01 INFO 139837668939584] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20099.71308708191, \"sum\": 20099.71308708191, \"min\": 20099.71308708191}}, \"EndTime\": 1579312501.282755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312481.18299}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1993113903 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=138, train loss <loss>=7.13353652954\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:08 INFO 139837668939584] Epoch[139] Batch[0] avg_epoch_loss=7.161510\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=7.16151046753\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:15 INFO 139837668939584] Epoch[139] Batch[5] avg_epoch_loss=7.104711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=7.10471137365\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:15 INFO 139837668939584] Epoch[139] Batch [5]#011Speed: 46.18 samples/sec#011loss=7.104711\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:21 INFO 139837668939584] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20454.979181289673, \"sum\": 20454.979181289673, \"min\": 20454.979181289673}}, \"EndTime\": 1579312521.738115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312501.28281}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:21 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.2391864795 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:21 INFO 139837668939584] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=139, train loss <loss>=7.11948533058\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:21 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:29 INFO 139837668939584] Epoch[140] Batch[0] avg_epoch_loss=7.192025\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=7.19202470779\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:36 INFO 139837668939584] Epoch[140] Batch[5] avg_epoch_loss=7.160160\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=7.16015982628\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:36 INFO 139837668939584] Epoch[140] Batch [5]#011Speed: 46.76 samples/sec#011loss=7.160160\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:41 INFO 139837668939584] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20185.810089111328, \"sum\": 20185.810089111328, \"min\": 20185.810089111328}}, \"EndTime\": 1579312541.924309, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312521.738184}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:41 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.9126543901 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:41 INFO 139837668939584] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=140, train loss <loss>=7.15665946007\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:41 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:49 INFO 139837668939584] Epoch[141] Batch[0] avg_epoch_loss=7.165459\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=7.1654586792\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:56 INFO 139837668939584] Epoch[141] Batch[5] avg_epoch_loss=7.181058\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=7.18105769157\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:55:56 INFO 139837668939584] Epoch[141] Batch [5]#011Speed: 47.10 samples/sec#011loss=7.181058\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] Epoch[141] Batch[10] avg_epoch_loss=7.164656\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=7.14497356415\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] Epoch[141] Batch [10]#011Speed: 43.35 samples/sec#011loss=7.144974\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22198.628187179565, \"sum\": 22198.628187179565, \"min\": 22198.628187179565}}, \"EndTime\": 1579312564.123313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312541.924377}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0378426356 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=141, train loss <loss>=7.16465581547\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:04 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:11 INFO 139837668939584] Epoch[142] Batch[0] avg_epoch_loss=7.192904\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=7.19290447235\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:18 INFO 139837668939584] Epoch[142] Batch[5] avg_epoch_loss=7.165065\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=7.16506481171\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:18 INFO 139837668939584] Epoch[142] Batch [5]#011Speed: 46.88 samples/sec#011loss=7.165065\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:23 INFO 139837668939584] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19823.467016220093, \"sum\": 19823.467016220093, \"min\": 19823.467016220093}}, \"EndTime\": 1579312583.947108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312564.123367}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:23 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.2848345148 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:23 INFO 139837668939584] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=142, train loss <loss>=7.14178757668\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:23 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:31 INFO 139837668939584] Epoch[143] Batch[0] avg_epoch_loss=7.212848\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=7.21284818649\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:38 INFO 139837668939584] Epoch[143] Batch[5] avg_epoch_loss=7.174315\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=7.17431473732\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:38 INFO 139837668939584] Epoch[143] Batch [5]#011Speed: 46.65 samples/sec#011loss=7.174315\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:44 INFO 139837668939584] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20192.080974578857, \"sum\": 20192.080974578857, \"min\": 20192.080974578857}}, \"EndTime\": 1579312604.139514, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312583.947164}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:44 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.2497355745 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:44 INFO 139837668939584] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=143, train loss <loss>=7.14386496544\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:44 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:51 INFO 139837668939584] Epoch[144] Batch[0] avg_epoch_loss=7.097137\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=7.09713697433\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:58 INFO 139837668939584] Epoch[144] Batch[5] avg_epoch_loss=7.158675\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=7.15867471695\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:56:58 INFO 139837668939584] Epoch[144] Batch [5]#011Speed: 46.68 samples/sec#011loss=7.158675\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:04 INFO 139837668939584] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20278.900146484375, \"sum\": 20278.900146484375, \"min\": 20278.900146484375}}, \"EndTime\": 1579312624.418785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312604.139577}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:04 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0172946213 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:04 INFO 139837668939584] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=144, train loss <loss>=7.14767394066\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:04 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:11 INFO 139837668939584] Epoch[145] Batch[0] avg_epoch_loss=7.055180\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=7.05517959595\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:18 INFO 139837668939584] Epoch[145] Batch[5] avg_epoch_loss=7.089720\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=7.08972024918\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:18 INFO 139837668939584] Epoch[145] Batch [5]#011Speed: 46.82 samples/sec#011loss=7.089720\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:24 INFO 139837668939584] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20031.46505355835, \"sum\": 20031.46505355835, \"min\": 20031.46505355835}}, \"EndTime\": 1579312644.450642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312624.418864}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:24 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.3521268618 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:24 INFO 139837668939584] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=145, train loss <loss>=7.15651659966\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:24 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:31 INFO 139837668939584] Epoch[146] Batch[0] avg_epoch_loss=7.040005\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=7.04000473022\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:38 INFO 139837668939584] Epoch[146] Batch[5] avg_epoch_loss=7.106687\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=7.10668683052\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:38 INFO 139837668939584] Epoch[146] Batch [5]#011Speed: 47.26 samples/sec#011loss=7.106687\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:44 INFO 139837668939584] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20075.83713531494, \"sum\": 20075.83713531494, \"min\": 20075.83713531494}}, \"EndTime\": 1579312664.526803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312644.450695}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:44 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0321769081 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:44 INFO 139837668939584] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=146, train loss <loss>=7.11550779343\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:44 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:44 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_a6db7131-c2c3-43bc-8a59-ae2fb5c7aad9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 249.38702583312988, \"sum\": 249.38702583312988, \"min\": 249.38702583312988}}, \"EndTime\": 1579312664.776621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312664.526871}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:52 INFO 139837668939584] Epoch[147] Batch[0] avg_epoch_loss=7.083101\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=7.08310079575\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:59 INFO 139837668939584] Epoch[147] Batch[5] avg_epoch_loss=7.088378\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=7.08837811152\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:57:59 INFO 139837668939584] Epoch[147] Batch [5]#011Speed: 47.45 samples/sec#011loss=7.088378\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:04 INFO 139837668939584] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19795.881986618042, \"sum\": 19795.881986618042, \"min\": 19795.881986618042}}, \"EndTime\": 1579312684.572633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312664.776687}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:04 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.3298188734 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:04 INFO 139837668939584] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=147, train loss <loss>=7.10656065941\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:04 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:04 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_dcba0e90-b4b9-442a-a7c9-cc4cf924a93d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 242.09189414978027, \"sum\": 242.09189414978027, \"min\": 242.09189414978027}}, \"EndTime\": 1579312684.815078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312684.572689}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:13 INFO 139837668939584] Epoch[148] Batch[0] avg_epoch_loss=7.049065\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=7.04906511307\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:19 INFO 139837668939584] Epoch[148] Batch[5] avg_epoch_loss=7.120061\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=7.12006092072\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:19 INFO 139837668939584] Epoch[148] Batch [5]#011Speed: 47.79 samples/sec#011loss=7.120061\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] Epoch[148] Batch[10] avg_epoch_loss=7.122845\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=7.12618646622\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] Epoch[148] Batch [10]#011Speed: 44.25 samples/sec#011loss=7.126186\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22158.0650806427, \"sum\": 22158.0650806427, \"min\": 22158.0650806427}}, \"EndTime\": 1579312706.973245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312684.815129}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.7264709829 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=148, train loss <loss>=7.12284525958\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:26 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:34 INFO 139837668939584] Epoch[149] Batch[0] avg_epoch_loss=7.180397\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=7.18039703369\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:41 INFO 139837668939584] Epoch[149] Batch[5] avg_epoch_loss=7.140151\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=7.14015142123\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:41 INFO 139837668939584] Epoch[149] Batch [5]#011Speed: 47.46 samples/sec#011loss=7.140151\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:47 INFO 139837668939584] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20206.47883415222, \"sum\": 20206.47883415222, \"min\": 20206.47883415222}}, \"EndTime\": 1579312727.180071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312706.973305}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:47 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.6233687536 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:47 INFO 139837668939584] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=149, train loss <loss>=7.14860720634\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:47 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:55 INFO 139837668939584] Epoch[150] Batch[0] avg_epoch_loss=7.149132\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:58:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=7.1491317749\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:01 INFO 139837668939584] Epoch[150] Batch[5] avg_epoch_loss=7.153129\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=7.15312878291\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:01 INFO 139837668939584] Epoch[150] Batch [5]#011Speed: 48.23 samples/sec#011loss=7.153129\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] Epoch[150] Batch[10] avg_epoch_loss=7.079946\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=6.99212684631\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] Epoch[150] Batch [10]#011Speed: 44.89 samples/sec#011loss=6.992127\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21877.805948257446, \"sum\": 21877.805948257446, \"min\": 21877.805948257446}}, \"EndTime\": 1579312749.058255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312727.180137}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.1273048576 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=150, train loss <loss>=7.07994608446\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:09 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_74c761a5-a4c0-40ff-b568-1302ee72ee51-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 251.69801712036133, \"sum\": 251.69801712036133, \"min\": 251.69801712036133}}, \"EndTime\": 1579312749.310368, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312749.058317}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:17 INFO 139837668939584] Epoch[151] Batch[0] avg_epoch_loss=7.268750\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=7.26875019073\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:23 INFO 139837668939584] Epoch[151] Batch[5] avg_epoch_loss=7.130265\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=7.13026483854\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:23 INFO 139837668939584] Epoch[151] Batch [5]#011Speed: 47.57 samples/sec#011loss=7.130265\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] Epoch[151] Batch[10] avg_epoch_loss=7.106099\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=7.07710094452\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] Epoch[151] Batch [10]#011Speed: 45.42 samples/sec#011loss=7.077101\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21472.055912017822, \"sum\": 21472.055912017822, \"min\": 21472.055912017822}}, \"EndTime\": 1579312770.782531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312749.310425}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.2717930398 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=151, train loss <loss>=7.10609943216\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:30 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:38 INFO 139837668939584] Epoch[152] Batch[0] avg_epoch_loss=7.144419\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=7.14441871643\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:44 INFO 139837668939584] Epoch[152] Batch[5] avg_epoch_loss=7.158124\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=7.15812373161\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:44 INFO 139837668939584] Epoch[152] Batch [5]#011Speed: 48.07 samples/sec#011loss=7.158124\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:50 INFO 139837668939584] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19920.295000076294, \"sum\": 19920.295000076294, \"min\": 19920.295000076294}}, \"EndTime\": 1579312790.703134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312770.782581}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:50 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.6258866376 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=152, train loss <loss>=7.12529330254\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:50 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:58 INFO 139837668939584] Epoch[153] Batch[0] avg_epoch_loss=7.199769\u001b[0m\n",
      "\u001b[34m[01/18/2020 01:59:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=7.19976854324\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:04 INFO 139837668939584] Epoch[153] Batch[5] avg_epoch_loss=7.136879\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=7.1368792057\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:04 INFO 139837668939584] Epoch[153] Batch [5]#011Speed: 48.37 samples/sec#011loss=7.136879\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:10 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19868.70789527893, \"sum\": 19868.70789527893, \"min\": 19868.70789527893}}, \"EndTime\": 1579312810.572222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312790.703201}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:10 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9093172529 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:10 INFO 139837668939584] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=153, train loss <loss>=7.13955950737\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:10 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:18 INFO 139837668939584] Epoch[154] Batch[0] avg_epoch_loss=7.181208\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=7.1812081337\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:25 INFO 139837668939584] Epoch[154] Batch[5] avg_epoch_loss=7.102807\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=7.10280688604\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:25 INFO 139837668939584] Epoch[154] Batch [5]#011Speed: 48.30 samples/sec#011loss=7.102807\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] Epoch[154] Batch[10] avg_epoch_loss=7.103840\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=7.10508050919\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] Epoch[154] Batch [10]#011Speed: 44.86 samples/sec#011loss=7.105081\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21774.79386329651, \"sum\": 21774.79386329651, \"min\": 21774.79386329651}}, \"EndTime\": 1579312832.347389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312810.572289}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.0552798566 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=154, train loss <loss>=7.1038403511\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:32 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:39 INFO 139837668939584] Epoch[155] Batch[0] avg_epoch_loss=7.088143\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=7.08814334869\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:46 INFO 139837668939584] Epoch[155] Batch[5] avg_epoch_loss=7.147779\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=7.14777859052\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:46 INFO 139837668939584] Epoch[155] Batch [5]#011Speed: 48.28 samples/sec#011loss=7.147779\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:52 INFO 139837668939584] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19747.24292755127, \"sum\": 19747.24292755127, \"min\": 19747.24292755127}}, \"EndTime\": 1579312852.094976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312832.347451}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:52 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.2319213586 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:52 INFO 139837668939584] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=155, train loss <loss>=7.18862133026\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:52 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:59 INFO 139837668939584] Epoch[156] Batch[0] avg_epoch_loss=7.242947\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:00:59 INFO 139837668939584] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=7.24294710159\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:06 INFO 139837668939584] Epoch[156] Batch[5] avg_epoch_loss=7.186333\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=7.18633333842\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:06 INFO 139837668939584] Epoch[156] Batch [5]#011Speed: 48.58 samples/sec#011loss=7.186333\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:11 INFO 139837668939584] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19686.532020568848, \"sum\": 19686.532020568848, \"min\": 19686.532020568848}}, \"EndTime\": 1579312871.781884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312852.095043}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.5442491712 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=156, train loss <loss>=7.17564001083\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:19 INFO 139837668939584] Epoch[157] Batch[0] avg_epoch_loss=7.165184\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=7.165184021\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:26 INFO 139837668939584] Epoch[157] Batch[5] avg_epoch_loss=7.103189\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=7.10318875313\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:26 INFO 139837668939584] Epoch[157] Batch [5]#011Speed: 49.07 samples/sec#011loss=7.103189\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] Epoch[157] Batch[10] avg_epoch_loss=7.121706\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=7.14392776489\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] Epoch[157] Batch [10]#011Speed: 45.66 samples/sec#011loss=7.143928\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21260.936975479126, \"sum\": 21260.936975479126, \"min\": 21260.936975479126}}, \"EndTime\": 1579312893.043203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312871.781953}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.5723706109 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=157, train loss <loss>=7.12170648575\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:33 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:40 INFO 139837668939584] Epoch[158] Batch[0] avg_epoch_loss=7.115194\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=7.11519384384\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:47 INFO 139837668939584] Epoch[158] Batch[5] avg_epoch_loss=7.094510\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=7.09451047579\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:47 INFO 139837668939584] Epoch[158] Batch [5]#011Speed: 49.22 samples/sec#011loss=7.094510\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] Epoch[158] Batch[10] avg_epoch_loss=7.138153\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=7.19052505493\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] Epoch[158] Batch [10]#011Speed: 45.71 samples/sec#011loss=7.190525\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21255.950927734375, \"sum\": 21255.950927734375, \"min\": 21255.950927734375}}, \"EndTime\": 1579312914.299507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312893.043265}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.2381796888 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=158, train loss <loss>=7.13815346631\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:01:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:01 INFO 139837668939584] Epoch[159] Batch[0] avg_epoch_loss=7.240105\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=7.24010467529\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:08 INFO 139837668939584] Epoch[159] Batch[5] avg_epoch_loss=7.214026\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=7.21402645111\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:08 INFO 139837668939584] Epoch[159] Batch [5]#011Speed: 49.20 samples/sec#011loss=7.214026\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] Epoch[159] Batch[10] avg_epoch_loss=7.216468\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=7.2193977356\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] Epoch[159] Batch [10]#011Speed: 45.75 samples/sec#011loss=7.219398\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21092.591047286987, \"sum\": 21092.591047286987, \"min\": 21092.591047286987}}, \"EndTime\": 1579312935.392456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312914.299566}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.7689873534 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=159, train loss <loss>=7.21646794406\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:15 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:22 INFO 139837668939584] Epoch[160] Batch[0] avg_epoch_loss=7.142017\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=7.1420173645\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:29 INFO 139837668939584] Epoch[160] Batch[5] avg_epoch_loss=7.214042\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=7.21404155095\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:29 INFO 139837668939584] Epoch[160] Batch [5]#011Speed: 48.57 samples/sec#011loss=7.214042\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:35 INFO 139837668939584] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19806.774139404297, \"sum\": 19806.774139404297, \"min\": 19806.774139404297}}, \"EndTime\": 1579312955.199523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312935.392508}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:35 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.7566516974 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:35 INFO 139837668939584] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=160, train loss <loss>=7.20373649597\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:35 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:42 INFO 139837668939584] Epoch[161] Batch[0] avg_epoch_loss=7.056886\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=7.0568857193\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:49 INFO 139837668939584] Epoch[161] Batch[5] avg_epoch_loss=7.138807\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=7.13880721728\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:49 INFO 139837668939584] Epoch[161] Batch [5]#011Speed: 48.94 samples/sec#011loss=7.138807\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:54 INFO 139837668939584] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19793.078899383545, \"sum\": 19793.078899383545, \"min\": 19793.078899383545}}, \"EndTime\": 1579312974.99298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312955.199591}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9807237379 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=161, train loss <loss>=7.1522767067\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:02:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:02 INFO 139837668939584] Epoch[162] Batch[0] avg_epoch_loss=7.052433\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=7.05243253708\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:09 INFO 139837668939584] Epoch[162] Batch[5] avg_epoch_loss=7.116181\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=7.11618073781\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:09 INFO 139837668939584] Epoch[162] Batch [5]#011Speed: 49.18 samples/sec#011loss=7.116181\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:14 INFO 139837668939584] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19865.80801010132, \"sum\": 19865.80801010132, \"min\": 19865.80801010132}}, \"EndTime\": 1579312994.859156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312974.993046}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.7629660808 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=162, train loss <loss>=7.1107694149\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:14 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:21 INFO 139837668939584] Epoch[163] Batch[0] avg_epoch_loss=7.166694\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=7.16669416428\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:28 INFO 139837668939584] Epoch[163] Batch[5] avg_epoch_loss=7.115459\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=7.11545928319\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:28 INFO 139837668939584] Epoch[163] Batch [5]#011Speed: 49.51 samples/sec#011loss=7.115459\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:34 INFO 139837668939584] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19144.837856292725, \"sum\": 19144.837856292725, \"min\": 19144.837856292725}}, \"EndTime\": 1579313014.004366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579312994.859222}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:34 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.1309474924 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:34 INFO 139837668939584] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=163, train loss <loss>=7.0949236393\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:34 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:41 INFO 139837668939584] Epoch[164] Batch[0] avg_epoch_loss=7.137581\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=7.13758134842\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:48 INFO 139837668939584] Epoch[164] Batch[5] avg_epoch_loss=7.087644\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=7.0876437823\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:48 INFO 139837668939584] Epoch[164] Batch [5]#011Speed: 49.20 samples/sec#011loss=7.087644\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:53 INFO 139837668939584] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19612.361192703247, \"sum\": 19612.361192703247, \"min\": 19612.361192703247}}, \"EndTime\": 1579313033.617133, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313014.004434}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:53 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.9185024884 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:53 INFO 139837668939584] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=164, train loss <loss>=7.12613925934\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:03:53 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:01 INFO 139837668939584] Epoch[165] Batch[0] avg_epoch_loss=7.154150\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=7.15415048599\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:07 INFO 139837668939584] Epoch[165] Batch[5] avg_epoch_loss=7.096960\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:07 INFO 139837668939584] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=7.09696030617\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:07 INFO 139837668939584] Epoch[165] Batch [5]#011Speed: 50.00 samples/sec#011loss=7.096960\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] Epoch[165] Batch[10] avg_epoch_loss=7.058507\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=7.01236333847\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] Epoch[165] Batch [10]#011Speed: 47.11 samples/sec#011loss=7.012363\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20989.373922348022, \"sum\": 20989.373922348022, \"min\": 20989.373922348022}}, \"EndTime\": 1579313054.606911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313033.617192}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.3490615388 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=165, train loss <loss>=7.05850713903\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:14 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_a37c10f0-bebd-4ad0-a32e-4222268aa27b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 249.17197227478027, \"sum\": 249.17197227478027, \"min\": 249.17197227478027}}, \"EndTime\": 1579313054.856469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313054.606972}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:22 INFO 139837668939584] Epoch[166] Batch[0] avg_epoch_loss=7.138759\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=7.1387591362\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:29 INFO 139837668939584] Epoch[166] Batch[5] avg_epoch_loss=7.186942\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=7.18694210052\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:29 INFO 139837668939584] Epoch[166] Batch [5]#011Speed: 49.58 samples/sec#011loss=7.186942\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] Epoch[166] Batch[10] avg_epoch_loss=7.243495\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=7.31135950089\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] Epoch[166] Batch [10]#011Speed: 46.71 samples/sec#011loss=7.311360\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21073.878049850464, \"sum\": 21073.878049850464, \"min\": 21073.878049850464}}, \"EndTime\": 1579313075.930449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313054.85652}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0809969613 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=166, train loss <loss>=7.24349546432\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:35 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:43 INFO 139837668939584] Epoch[167] Batch[0] avg_epoch_loss=7.207998\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=7.20799779892\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:49 INFO 139837668939584] Epoch[167] Batch[5] avg_epoch_loss=7.143586\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=7.14358607928\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:49 INFO 139837668939584] Epoch[167] Batch [5]#011Speed: 49.88 samples/sec#011loss=7.143586\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:55 INFO 139837668939584] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19572.892904281616, \"sum\": 19572.892904281616, \"min\": 19572.892904281616}}, \"EndTime\": 1579313095.503704, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313075.930511}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:55 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.1361223762 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:55 INFO 139837668939584] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=167, train loss <loss>=7.14775280952\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:04:55 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:03 INFO 139837668939584] Epoch[168] Batch[0] avg_epoch_loss=7.179404\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=7.17940425873\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:09 INFO 139837668939584] Epoch[168] Batch[5] avg_epoch_loss=7.121238\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=7.12123791377\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:09 INFO 139837668939584] Epoch[168] Batch [5]#011Speed: 49.39 samples/sec#011loss=7.121238\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:15 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19587.846994400024, \"sum\": 19587.846994400024, \"min\": 19587.846994400024}}, \"EndTime\": 1579313115.09193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313095.50377}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.3668476387 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=168, train loss <loss>=7.09472780228\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:15 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:22 INFO 139837668939584] Epoch[169] Batch[0] avg_epoch_loss=7.121540\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=7.12153959274\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:29 INFO 139837668939584] Epoch[169] Batch[5] avg_epoch_loss=7.083475\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=7.08347463608\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:29 INFO 139837668939584] Epoch[169] Batch [5]#011Speed: 50.06 samples/sec#011loss=7.083475\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] Epoch[169] Batch[10] avg_epoch_loss=7.087199\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=7.09166717529\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] Epoch[169] Batch [10]#011Speed: 46.68 samples/sec#011loss=7.091667\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21126.77502632141, \"sum\": 21126.77502632141, \"min\": 21126.77502632141}}, \"EndTime\": 1579313136.219083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313115.091997}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.6125037507 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=169, train loss <loss>=7.08719851754\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:36 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:43 INFO 139837668939584] Epoch[170] Batch[0] avg_epoch_loss=7.077912\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=7.07791233063\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:50 INFO 139837668939584] Epoch[170] Batch[5] avg_epoch_loss=7.076468\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=7.07646799088\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:50 INFO 139837668939584] Epoch[170] Batch [5]#011Speed: 49.80 samples/sec#011loss=7.076468\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] Epoch[170] Batch[10] avg_epoch_loss=7.035199\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=6.9856762886\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] Epoch[170] Batch [10]#011Speed: 47.07 samples/sec#011loss=6.985676\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20880.412101745605, \"sum\": 20880.412101745605, \"min\": 20880.412101745605}}, \"EndTime\": 1579313157.099839, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313136.219146}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.4168871501 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=170, train loss <loss>=7.0351990353\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:05:57 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_56592fd1-9cbb-438a-a573-d80ac8ff3d5b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 242.79212951660156, \"sum\": 242.79212951660156, \"min\": 242.79212951660156}}, \"EndTime\": 1579313157.342956, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313157.099891}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:05 INFO 139837668939584] Epoch[171] Batch[0] avg_epoch_loss=7.115886\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=7.1158862114\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:11 INFO 139837668939584] Epoch[171] Batch[5] avg_epoch_loss=7.132136\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=7.13213610649\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:11 INFO 139837668939584] Epoch[171] Batch [5]#011Speed: 50.26 samples/sec#011loss=7.132136\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:16 INFO 139837668939584] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19655.14612197876, \"sum\": 19655.14612197876, \"min\": 19655.14612197876}}, \"EndTime\": 1579313176.998206, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313157.343008}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:16 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.5104081185 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:16 INFO 139837668939584] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=171, train loss <loss>=7.11560835838\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:16 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:24 INFO 139837668939584] Epoch[172] Batch[0] avg_epoch_loss=7.069902\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=7.06990194321\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:31 INFO 139837668939584] Epoch[172] Batch[5] avg_epoch_loss=7.102828\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=7.10282786687\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:31 INFO 139837668939584] Epoch[172] Batch [5]#011Speed: 49.83 samples/sec#011loss=7.102828\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] Epoch[172] Batch[10] avg_epoch_loss=7.073103\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=7.0374332428\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] Epoch[172] Batch [10]#011Speed: 46.33 samples/sec#011loss=7.037433\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21278.883934020996, \"sum\": 21278.883934020996, \"min\": 21278.883934020996}}, \"EndTime\": 1579313198.277471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313176.998274}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.56735814 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=172, train loss <loss>=7.07310303775\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:38 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:45 INFO 139837668939584] Epoch[173] Batch[0] avg_epoch_loss=7.135474\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=7.13547420502\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:52 INFO 139837668939584] Epoch[173] Batch[5] avg_epoch_loss=7.089583\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=7.08958347638\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:52 INFO 139837668939584] Epoch[173] Batch [5]#011Speed: 49.23 samples/sec#011loss=7.089583\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:57 INFO 139837668939584] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19443.495988845825, \"sum\": 19443.495988845825, \"min\": 19443.495988845825}}, \"EndTime\": 1579313217.721315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313198.277534}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:57 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.578540309 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:57 INFO 139837668939584] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=173, train loss <loss>=7.06547040939\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:06:57 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:04 INFO 139837668939584] Epoch[174] Batch[0] avg_epoch_loss=7.030150\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=7.03014993668\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:11 INFO 139837668939584] Epoch[174] Batch[5] avg_epoch_loss=7.058033\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=7.05803346634\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:11 INFO 139837668939584] Epoch[174] Batch [5]#011Speed: 49.32 samples/sec#011loss=7.058033\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:16 INFO 139837668939584] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19224.29895401001, \"sum\": 19224.29895401001, \"min\": 19224.29895401001}}, \"EndTime\": 1579313236.94597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313217.721376}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:16 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.3664242652 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:16 INFO 139837668939584] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=174, train loss <loss>=7.07250204086\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:16 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:24 INFO 139837668939584] Epoch[175] Batch[0] avg_epoch_loss=7.098775\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=7.09877490997\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:30 INFO 139837668939584] Epoch[175] Batch[5] avg_epoch_loss=7.052568\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=7.05256843567\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:30 INFO 139837668939584] Epoch[175] Batch [5]#011Speed: 49.54 samples/sec#011loss=7.052568\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:36 INFO 139837668939584] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19500.518083572388, \"sum\": 19500.518083572388, \"min\": 19500.518083572388}}, \"EndTime\": 1579313256.446861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313236.946024}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:36 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.3579552223 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:36 INFO 139837668939584] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=175, train loss <loss>=7.05376868248\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:36 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:43 INFO 139837668939584] Epoch[176] Batch[0] avg_epoch_loss=7.096977\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=7.09697675705\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:50 INFO 139837668939584] Epoch[176] Batch[5] avg_epoch_loss=7.069790\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=7.06979012489\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:50 INFO 139837668939584] Epoch[176] Batch [5]#011Speed: 49.38 samples/sec#011loss=7.069790\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:55 INFO 139837668939584] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19456.488847732544, \"sum\": 19456.488847732544, \"min\": 19456.488847732544}}, \"EndTime\": 1579313275.903725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313256.446927}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:55 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=32.1741956029 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:55 INFO 139837668939584] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=176, train loss <loss>=7.10114722252\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:07:55 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:03 INFO 139837668939584] Epoch[177] Batch[0] avg_epoch_loss=7.155332\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=7.15533208847\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:09 INFO 139837668939584] Epoch[177] Batch[5] avg_epoch_loss=7.099725\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=7.09972476959\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:09 INFO 139837668939584] Epoch[177] Batch [5]#011Speed: 48.99 samples/sec#011loss=7.099725\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:15 INFO 139837668939584] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19554.982900619507, \"sum\": 19554.982900619507, \"min\": 19554.982900619507}}, \"EndTime\": 1579313295.45909, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313275.903792}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.8587309482 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=177, train loss <loss>=7.07045607567\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:15 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:23 INFO 139837668939584] Epoch[178] Batch[0] avg_epoch_loss=6.967743\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=6.96774339676\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:29 INFO 139837668939584] Epoch[178] Batch[5] avg_epoch_loss=7.065910\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=7.06590978305\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:29 INFO 139837668939584] Epoch[178] Batch [5]#011Speed: 48.12 samples/sec#011loss=7.065910\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] Epoch[178] Batch[10] avg_epoch_loss=7.106804\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=7.15587778091\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] Epoch[178] Batch [10]#011Speed: 45.90 samples/sec#011loss=7.155878\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21372.738122940063, \"sum\": 21372.738122940063, \"min\": 21372.738122940063}}, \"EndTime\": 1579313316.83221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313295.459155}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.0674812489 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=178, train loss <loss>=7.10680432753\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:36 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:44 INFO 139837668939584] Epoch[179] Batch[0] avg_epoch_loss=7.137214\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=7.13721370697\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:50 INFO 139837668939584] Epoch[179] Batch[5] avg_epoch_loss=7.091186\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=7.09118628502\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:50 INFO 139837668939584] Epoch[179] Batch [5]#011Speed: 48.90 samples/sec#011loss=7.091186\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:56 INFO 139837668939584] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19655.53307533264, \"sum\": 19655.53307533264, \"min\": 19655.53307533264}}, \"EndTime\": 1579313336.48809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313316.832271}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:56 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.6448756049 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:56 INFO 139837668939584] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=179, train loss <loss>=7.06991057396\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:08:56 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:04 INFO 139837668939584] Epoch[180] Batch[0] avg_epoch_loss=7.136611\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=7.13661146164\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:10 INFO 139837668939584] Epoch[180] Batch[5] avg_epoch_loss=7.032080\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=7.03208001455\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:10 INFO 139837668939584] Epoch[180] Batch [5]#011Speed: 47.84 samples/sec#011loss=7.032080\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] Epoch[180] Batch[10] avg_epoch_loss=7.022474\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=180, batch=10 train loss <loss>=7.0109457016\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] Epoch[180] Batch [10]#011Speed: 44.98 samples/sec#011loss=7.010946\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21561.75398826599, \"sum\": 21561.75398826599, \"min\": 21561.75398826599}}, \"EndTime\": 1579313358.050228, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313336.488157}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.4704893754 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=180, train loss <loss>=7.02247350866\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:18 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_c57af795-bed6-4cc8-8579-f7ed813923a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 270.582914352417, \"sum\": 270.582914352417, \"min\": 270.582914352417}}, \"EndTime\": 1579313358.321215, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313358.050291}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:25 INFO 139837668939584] Epoch[181] Batch[0] avg_epoch_loss=7.048664\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=7.04866409302\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:32 INFO 139837668939584] Epoch[181] Batch[5] avg_epoch_loss=7.053309\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=7.05330880483\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:32 INFO 139837668939584] Epoch[181] Batch [5]#011Speed: 48.27 samples/sec#011loss=7.053309\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:38 INFO 139837668939584] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19768.60809326172, \"sum\": 19768.60809326172, \"min\": 19768.60809326172}}, \"EndTime\": 1579313378.089925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313358.321267}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:38 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.1603758704 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:38 INFO 139837668939584] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=181, train loss <loss>=7.08426876068\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:38 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:45 INFO 139837668939584] Epoch[182] Batch[0] avg_epoch_loss=7.108719\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=7.10871887207\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:52 INFO 139837668939584] Epoch[182] Batch[5] avg_epoch_loss=7.133779\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:52 INFO 139837668939584] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=7.13377896945\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:52 INFO 139837668939584] Epoch[182] Batch [5]#011Speed: 47.80 samples/sec#011loss=7.133779\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:58 INFO 139837668939584] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20008.847951889038, \"sum\": 20008.847951889038, \"min\": 20008.847951889038}}, \"EndTime\": 1579313398.099154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313378.089983}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.6358493764 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=182, train loss <loss>=7.11095757484\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:09:58 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:05 INFO 139837668939584] Epoch[183] Batch[0] avg_epoch_loss=6.974324\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=6.97432422638\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:12 INFO 139837668939584] Epoch[183] Batch[5] avg_epoch_loss=7.034547\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=7.03454709053\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:12 INFO 139837668939584] Epoch[183] Batch [5]#011Speed: 47.76 samples/sec#011loss=7.034547\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] Epoch[183] Batch[10] avg_epoch_loss=7.068067\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=7.10828990936\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] Epoch[183] Batch [10]#011Speed: 44.58 samples/sec#011loss=7.108290\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21743.98684501648, \"sum\": 21743.98684501648, \"min\": 21743.98684501648}}, \"EndTime\": 1579313419.84352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313398.099223}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.6290447605 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=183, train loss <loss>=7.06806655364\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:19 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:27 INFO 139837668939584] Epoch[184] Batch[0] avg_epoch_loss=7.122209\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=7.12220859528\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:34 INFO 139837668939584] Epoch[184] Batch[5] avg_epoch_loss=7.110315\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=7.11031468709\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:34 INFO 139837668939584] Epoch[184] Batch [5]#011Speed: 47.46 samples/sec#011loss=7.110315\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] Epoch[184] Batch[10] avg_epoch_loss=7.104001\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=7.09642543793\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] Epoch[184] Batch [10]#011Speed: 44.43 samples/sec#011loss=7.096425\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21901.077032089233, \"sum\": 21901.077032089233, \"min\": 21901.077032089233}}, \"EndTime\": 1579313441.744976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313419.843572}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.2723579277 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=184, train loss <loss>=7.10400139202\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:41 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:49 INFO 139837668939584] Epoch[185] Batch[0] avg_epoch_loss=7.104004\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:49 INFO 139837668939584] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=7.10400390625\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:56 INFO 139837668939584] Epoch[185] Batch[5] avg_epoch_loss=7.099998\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=7.09999775887\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:10:56 INFO 139837668939584] Epoch[185] Batch [5]#011Speed: 47.09 samples/sec#011loss=7.099998\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:02 INFO 139837668939584] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20326.519012451172, \"sum\": 20326.519012451172, \"min\": 20326.519012451172}}, \"EndTime\": 1579313462.071838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313441.745038}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:02 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.4366166971 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:02 INFO 139837668939584] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=185, train loss <loss>=7.0810177803\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:02 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:09 INFO 139837668939584] Epoch[186] Batch[0] avg_epoch_loss=6.992074\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=6.99207353592\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:16 INFO 139837668939584] Epoch[186] Batch[5] avg_epoch_loss=7.043179\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=7.04317927361\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:16 INFO 139837668939584] Epoch[186] Batch [5]#011Speed: 47.24 samples/sec#011loss=7.043179\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] Epoch[186] Batch[10] avg_epoch_loss=7.016037\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=6.98346700668\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] Epoch[186] Batch [10]#011Speed: 44.51 samples/sec#011loss=6.983467\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21708.476066589355, \"sum\": 21708.476066589355, \"min\": 21708.476066589355}}, \"EndTime\": 1579313483.780694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313462.071906}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.7578313275 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=186, train loss <loss>=7.0160373341\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:23 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:24 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_db8d78b8-b24a-42e1-a3d1-ae322bd4f9bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 257.2929859161377, \"sum\": 257.2929859161377, \"min\": 257.2929859161377}}, \"EndTime\": 1579313484.0384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313483.78076}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:31 INFO 139837668939584] Epoch[187] Batch[0] avg_epoch_loss=6.947078\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=6.94707775116\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:38 INFO 139837668939584] Epoch[187] Batch[5] avg_epoch_loss=7.065919\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=7.06591931979\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:38 INFO 139837668939584] Epoch[187] Batch [5]#011Speed: 47.49 samples/sec#011loss=7.065919\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:43 INFO 139837668939584] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19883.168935775757, \"sum\": 19883.168935775757, \"min\": 19883.168935775757}}, \"EndTime\": 1579313503.921685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313484.038461}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:43 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.8299437053 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:43 INFO 139837668939584] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=187, train loss <loss>=7.06237406731\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:43 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:51 INFO 139837668939584] Epoch[188] Batch[0] avg_epoch_loss=7.114680\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=7.11468029022\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:58 INFO 139837668939584] Epoch[188] Batch[5] avg_epoch_loss=7.077077\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=7.07707722982\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:11:58 INFO 139837668939584] Epoch[188] Batch [5]#011Speed: 47.27 samples/sec#011loss=7.077077\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] Epoch[188] Batch[10] avg_epoch_loss=7.080193\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=7.08393297195\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] Epoch[188] Batch [10]#011Speed: 44.18 samples/sec#011loss=7.083933\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21951.27296447754, \"sum\": 21951.27296447754, \"min\": 21951.27296447754}}, \"EndTime\": 1579313525.873325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313503.921753}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.7042422858 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=188, train loss <loss>=7.08019347624\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:05 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:13 INFO 139837668939584] Epoch[189] Batch[0] avg_epoch_loss=7.114673\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=7.11467266083\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:20 INFO 139837668939584] Epoch[189] Batch[5] avg_epoch_loss=7.077019\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=7.07701865832\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:20 INFO 139837668939584] Epoch[189] Batch [5]#011Speed: 47.34 samples/sec#011loss=7.077019\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] Epoch[189] Batch[10] avg_epoch_loss=7.035772\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=6.98627529144\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] Epoch[189] Batch [10]#011Speed: 44.47 samples/sec#011loss=6.986275\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21731.49299621582, \"sum\": 21731.49299621582, \"min\": 21731.49299621582}}, \"EndTime\": 1579313547.605177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313525.873388}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.5882859882 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=189, train loss <loss>=7.03577167338\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:27 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:35 INFO 139837668939584] Epoch[190] Batch[0] avg_epoch_loss=7.152652\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=7.1526517868\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:41 INFO 139837668939584] Epoch[190] Batch[5] avg_epoch_loss=7.095271\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:41 INFO 139837668939584] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=7.09527142843\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:41 INFO 139837668939584] Epoch[190] Batch [5]#011Speed: 47.32 samples/sec#011loss=7.095271\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:47 INFO 139837668939584] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20101.81498527527, \"sum\": 20101.81498527527, \"min\": 20101.81498527527}}, \"EndTime\": 1579313567.70737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313547.605228}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:47 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=31.5392883681 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:47 INFO 139837668939584] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=190, train loss <loss>=7.08189191818\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:47 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:55 INFO 139837668939584] Epoch[191] Batch[0] avg_epoch_loss=7.048367\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:12:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=7.04836702347\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:02 INFO 139837668939584] Epoch[191] Batch[5] avg_epoch_loss=7.015545\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=7.01554473241\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:02 INFO 139837668939584] Epoch[191] Batch [5]#011Speed: 46.99 samples/sec#011loss=7.015545\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] Epoch[191] Batch[10] avg_epoch_loss=6.998497\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=6.97804069519\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] Epoch[191] Batch [10]#011Speed: 43.86 samples/sec#011loss=6.978041\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21835.84213256836, \"sum\": 21835.84213256836, \"min\": 21835.84213256836}}, \"EndTime\": 1579313589.543611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313567.707437}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0422228391 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=191, train loss <loss>=6.99849744277\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:09 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_aac98b5c-b3e3-4aec-bac4-19a4250d57e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 272.78804779052734, \"sum\": 272.78804779052734, \"min\": 272.78804779052734}}, \"EndTime\": 1579313589.816784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313589.543673}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:17 INFO 139837668939584] Epoch[192] Batch[0] avg_epoch_loss=7.069073\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=7.06907320023\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:24 INFO 139837668939584] Epoch[192] Batch[5] avg_epoch_loss=7.044872\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=7.04487164815\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:24 INFO 139837668939584] Epoch[192] Batch [5]#011Speed: 47.11 samples/sec#011loss=7.044872\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:30 INFO 139837668939584] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20256.222009658813, \"sum\": 20256.222009658813, \"min\": 20256.222009658813}}, \"EndTime\": 1579313610.073116, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313589.816841}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:30 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.8545649535 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:30 INFO 139837668939584] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=192, train loss <loss>=7.05085721016\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:30 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:37 INFO 139837668939584] Epoch[193] Batch[0] avg_epoch_loss=7.152332\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=7.15233230591\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:44 INFO 139837668939584] Epoch[193] Batch[5] avg_epoch_loss=7.078081\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=7.07808121045\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:44 INFO 139837668939584] Epoch[193] Batch [5]#011Speed: 46.34 samples/sec#011loss=7.078081\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:50 INFO 139837668939584] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20383.870840072632, \"sum\": 20383.870840072632, \"min\": 20383.870840072632}}, \"EndTime\": 1579313630.457379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313610.073185}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:50 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.6613452059 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=193, train loss <loss>=7.03939738274\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:50 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:58 INFO 139837668939584] Epoch[194] Batch[0] avg_epoch_loss=7.154144\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:13:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=7.15414428711\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:05 INFO 139837668939584] Epoch[194] Batch[5] avg_epoch_loss=7.082137\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=7.08213710785\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:05 INFO 139837668939584] Epoch[194] Batch [5]#011Speed: 45.74 samples/sec#011loss=7.082137\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] Epoch[194] Batch[10] avg_epoch_loss=7.058708\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=7.0305932045\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] Epoch[194] Batch [10]#011Speed: 42.91 samples/sec#011loss=7.030593\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22327.77714729309, \"sum\": 22327.77714729309, \"min\": 22327.77714729309}}, \"EndTime\": 1579313652.785546, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313630.457449}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.9625613495 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=194, train loss <loss>=7.05870806087\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:12 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:20 INFO 139837668939584] Epoch[195] Batch[0] avg_epoch_loss=7.158830\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:20 INFO 139837668939584] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=7.15883016586\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:27 INFO 139837668939584] Epoch[195] Batch[5] avg_epoch_loss=7.064355\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:27 INFO 139837668939584] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=7.06435457865\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:27 INFO 139837668939584] Epoch[195] Batch [5]#011Speed: 45.37 samples/sec#011loss=7.064355\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] Epoch[195] Batch[10] avg_epoch_loss=7.028498\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=195, batch=10 train loss <loss>=6.98547067642\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] Epoch[195] Batch [10]#011Speed: 42.20 samples/sec#011loss=6.985471\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22678.853034973145, \"sum\": 22678.853034973145, \"min\": 22678.853034973145}}, \"EndTime\": 1579313675.464739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313652.785604}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.3223503185 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=195, train loss <loss>=7.02849825946\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:35 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:43 INFO 139837668939584] Epoch[196] Batch[0] avg_epoch_loss=7.034254\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=7.0342540741\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:50 INFO 139837668939584] Epoch[196] Batch[5] avg_epoch_loss=7.081720\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=7.08172019323\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:50 INFO 139837668939584] Epoch[196] Batch [5]#011Speed: 44.65 samples/sec#011loss=7.081720\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] Epoch[196] Batch[10] avg_epoch_loss=7.035752\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=6.98059043884\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] Epoch[196] Batch [10]#011Speed: 41.52 samples/sec#011loss=6.980590\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22615.9930229187, \"sum\": 22615.9930229187, \"min\": 22615.9930229187}}, \"EndTime\": 1579313698.081091, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313675.464802}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.5637411825 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=196, train loss <loss>=7.03575212305\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:14:58 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:06 INFO 139837668939584] Epoch[197] Batch[0] avg_epoch_loss=7.065666\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=7.06566572189\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:13 INFO 139837668939584] Epoch[197] Batch[5] avg_epoch_loss=7.013740\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=7.01374046008\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:13 INFO 139837668939584] Epoch[197] Batch [5]#011Speed: 43.96 samples/sec#011loss=7.013740\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] Epoch[197] Batch[10] avg_epoch_loss=6.999453\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=6.98230829239\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] Epoch[197] Batch [10]#011Speed: 41.08 samples/sec#011loss=6.982308\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23069.199085235596, \"sum\": 23069.199085235596, \"min\": 23069.199085235596}}, \"EndTime\": 1579313721.150638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313698.081152}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.1296364141 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=197, train loss <loss>=6.99945311113\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:21 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:28 INFO 139837668939584] Epoch[198] Batch[0] avg_epoch_loss=6.992307\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=6.99230718613\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:36 INFO 139837668939584] Epoch[198] Batch[5] avg_epoch_loss=7.062271\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=7.06227111816\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:36 INFO 139837668939584] Epoch[198] Batch [5]#011Speed: 43.34 samples/sec#011loss=7.062271\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:42 INFO 139837668939584] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21259.936094284058, \"sum\": 21259.936094284058, \"min\": 21259.936094284058}}, \"EndTime\": 1579313742.410925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313721.150699}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:42 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.3508469706 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:42 INFO 139837668939584] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=198, train loss <loss>=7.03802819252\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:42 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:50 INFO 139837668939584] Epoch[199] Batch[0] avg_epoch_loss=6.907350\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=6.90734958649\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:57 INFO 139837668939584] Epoch[199] Batch[5] avg_epoch_loss=7.021160\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=7.02115972837\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:15:57 INFO 139837668939584] Epoch[199] Batch [5]#011Speed: 43.16 samples/sec#011loss=7.021160\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] Epoch[199] Batch[10] avg_epoch_loss=6.994328\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=199, batch=10 train loss <loss>=6.96212949753\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] Epoch[199] Batch [10]#011Speed: 40.71 samples/sec#011loss=6.962129\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23347.64790534973, \"sum\": 23347.64790534973, \"min\": 23347.64790534973}}, \"EndTime\": 1579313765.758949, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313742.410993}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.8250609852 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=199, train loss <loss>=6.99432780526\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:05 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:06 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_f2aeafc1-7114-4855-b4c4-48d2866205e0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 252.81000137329102, \"sum\": 252.81000137329102, \"min\": 252.81000137329102}}, \"EndTime\": 1579313766.012159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313765.759012}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:13 INFO 139837668939584] Epoch[200] Batch[0] avg_epoch_loss=7.082057\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=7.08205652237\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:21 INFO 139837668939584] Epoch[200] Batch[5] avg_epoch_loss=7.037465\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=7.03746501605\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:21 INFO 139837668939584] Epoch[200] Batch [5]#011Speed: 43.28 samples/sec#011loss=7.037465\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] Epoch[200] Batch[10] avg_epoch_loss=7.096811\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=7.16802549362\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] Epoch[200] Batch [10]#011Speed: 40.96 samples/sec#011loss=7.168025\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22983.095169067383, \"sum\": 22983.095169067383, \"min\": 22983.095169067383}}, \"EndTime\": 1579313788.995355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313766.01221}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.2815419555 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=200, train loss <loss>=7.09681068767\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:28 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:36 INFO 139837668939584] Epoch[201] Batch[0] avg_epoch_loss=7.061368\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=7.06136798859\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:43 INFO 139837668939584] Epoch[201] Batch[5] avg_epoch_loss=7.065565\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=7.06556463242\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:43 INFO 139837668939584] Epoch[201] Batch [5]#011Speed: 43.44 samples/sec#011loss=7.065565\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:50 INFO 139837668939584] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21326.828956604004, \"sum\": 21326.828956604004, \"min\": 21326.828956604004}}, \"EndTime\": 1579313810.322539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313788.995419}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:50 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.305650905 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=201, train loss <loss>=7.09583201408\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:50 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:58 INFO 139837668939584] Epoch[202] Batch[0] avg_epoch_loss=7.053861\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:16:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=7.0538611412\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:05 INFO 139837668939584] Epoch[202] Batch[5] avg_epoch_loss=7.091917\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=7.09191679955\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:05 INFO 139837668939584] Epoch[202] Batch [5]#011Speed: 43.07 samples/sec#011loss=7.091917\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:11 INFO 139837668939584] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21487.653017044067, \"sum\": 21487.653017044067, \"min\": 21487.653017044067}}, \"EndTime\": 1579313831.810614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313810.322625}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.2259325235 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=202, train loss <loss>=7.09200997353\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:19 INFO 139837668939584] Epoch[203] Batch[0] avg_epoch_loss=7.064502\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=7.06450176239\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:26 INFO 139837668939584] Epoch[203] Batch[5] avg_epoch_loss=7.015495\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=7.01549490293\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:26 INFO 139837668939584] Epoch[203] Batch [5]#011Speed: 43.03 samples/sec#011loss=7.015495\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:33 INFO 139837668939584] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21442.75212287903, \"sum\": 21442.75212287903, \"min\": 21442.75212287903}}, \"EndTime\": 1579313853.253745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313831.810683}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:33 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.2871544378 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:33 INFO 139837668939584] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:33 INFO 139837668939584] #quality_metric: host=algo-1, epoch=203, train loss <loss>=7.0443107605\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:33 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:40 INFO 139837668939584] Epoch[204] Batch[0] avg_epoch_loss=7.008411\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=7.00841093063\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:48 INFO 139837668939584] Epoch[204] Batch[5] avg_epoch_loss=7.055741\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:48 INFO 139837668939584] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=7.05574075381\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:48 INFO 139837668939584] Epoch[204] Batch [5]#011Speed: 43.29 samples/sec#011loss=7.055741\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:54 INFO 139837668939584] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21375.576972961426, \"sum\": 21375.576972961426, \"min\": 21375.576972961426}}, \"EndTime\": 1579313874.629696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313853.253811}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.6306949887 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=204, train loss <loss>=7.03067779541\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:17:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:02 INFO 139837668939584] Epoch[205] Batch[0] avg_epoch_loss=6.953588\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=6.95358800888\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:09 INFO 139837668939584] Epoch[205] Batch[5] avg_epoch_loss=7.004837\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=7.00483679771\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:09 INFO 139837668939584] Epoch[205] Batch [5]#011Speed: 42.90 samples/sec#011loss=7.004837\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:15 INFO 139837668939584] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21367.50102043152, \"sum\": 21367.50102043152, \"min\": 21367.50102043152}}, \"EndTime\": 1579313895.997524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313874.629753}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.0626943952 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=205, train loss <loss>=6.93354072571\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:15 INFO 139837668939584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:16 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/state_afca4b78-8c50-4bf5-a59b-5ff737f85a8d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 258.6479187011719, \"sum\": 258.6479187011719, \"min\": 258.6479187011719}}, \"EndTime\": 1579313896.256592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313895.997592}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:24 INFO 139837668939584] Epoch[206] Batch[0] avg_epoch_loss=7.086087\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=7.08608722687\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:31 INFO 139837668939584] Epoch[206] Batch[5] avg_epoch_loss=7.019283\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=7.0192832152\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:31 INFO 139837668939584] Epoch[206] Batch [5]#011Speed: 42.77 samples/sec#011loss=7.019283\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] Epoch[206] Batch[10] avg_epoch_loss=6.990817\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=206, batch=10 train loss <loss>=6.95665740967\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] Epoch[206] Batch [10]#011Speed: 40.70 samples/sec#011loss=6.956657\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23364.29214477539, \"sum\": 23364.29214477539, \"min\": 23364.29214477539}}, \"EndTime\": 1579313919.620995, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313896.25665}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.8473283469 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=206, train loss <loss>=6.99081693996\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:39 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:47 INFO 139837668939584] Epoch[207] Batch[0] avg_epoch_loss=7.079614\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=7.07961416245\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:54 INFO 139837668939584] Epoch[207] Batch[5] avg_epoch_loss=7.015887\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=7.01588749886\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:18:54 INFO 139837668939584] Epoch[207] Batch [5]#011Speed: 42.69 samples/sec#011loss=7.015887\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:01 INFO 139837668939584] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21477.355003356934, \"sum\": 21477.355003356934, \"min\": 21477.355003356934}}, \"EndTime\": 1579313941.098688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313919.621055}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.4950005085 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=207, train loss <loss>=6.98678517342\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:08 INFO 139837668939584] Epoch[208] Batch[0] avg_epoch_loss=7.051704\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=7.0517039299\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:16 INFO 139837668939584] Epoch[208] Batch[5] avg_epoch_loss=7.051459\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=7.05145947138\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:16 INFO 139837668939584] Epoch[208] Batch [5]#011Speed: 42.90 samples/sec#011loss=7.051459\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:22 INFO 139837668939584] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21405.690908432007, \"sum\": 21405.690908432007, \"min\": 21405.690908432007}}, \"EndTime\": 1579313962.504758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313941.098756}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:22 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.8239798316 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:22 INFO 139837668939584] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=208, train loss <loss>=7.04135489464\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:22 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:30 INFO 139837668939584] Epoch[209] Batch[0] avg_epoch_loss=7.097467\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=7.09746694565\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:37 INFO 139837668939584] Epoch[209] Batch[5] avg_epoch_loss=7.073983\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=7.07398271561\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:37 INFO 139837668939584] Epoch[209] Batch [5]#011Speed: 43.09 samples/sec#011loss=7.073983\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] Epoch[209] Batch[10] avg_epoch_loss=7.032284\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=6.98224506378\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] Epoch[209] Batch [10]#011Speed: 40.99 samples/sec#011loss=6.982245\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23008.486032485962, \"sum\": 23008.486032485962, \"min\": 23008.486032485962}}, \"EndTime\": 1579313985.51362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313962.504824}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.3807136151 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=209, train loss <loss>=7.03228378296\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:45 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:53 INFO 139837668939584] Epoch[210] Batch[0] avg_epoch_loss=7.019514\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:19:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=7.01951408386\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:00 INFO 139837668939584] Epoch[210] Batch[5] avg_epoch_loss=7.050356\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=7.05035583178\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:00 INFO 139837668939584] Epoch[210] Batch [5]#011Speed: 43.46 samples/sec#011loss=7.050356\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:06 INFO 139837668939584] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21289.523124694824, \"sum\": 21289.523124694824, \"min\": 21289.523124694824}}, \"EndTime\": 1579314006.803502, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579313985.513688}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:06 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.2630866726 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:06 INFO 139837668939584] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=210, train loss <loss>=7.05497546196\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:06 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:14 INFO 139837668939584] Epoch[211] Batch[0] avg_epoch_loss=7.087380\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:14 INFO 139837668939584] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=7.08738040924\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:21 INFO 139837668939584] Epoch[211] Batch[5] avg_epoch_loss=7.027623\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=7.02762254079\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:22 INFO 139837668939584] Epoch[211] Batch [5]#011Speed: 43.44 samples/sec#011loss=7.027623\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] Epoch[211] Batch[10] avg_epoch_loss=6.979694\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=6.92218036652\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] Epoch[211] Batch [10]#011Speed: 41.38 samples/sec#011loss=6.922180\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22930.023908615112, \"sum\": 22930.023908615112, \"min\": 22930.023908615112}}, \"EndTime\": 1579314029.733908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314006.80357}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.5214408581 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=211, train loss <loss>=6.97969427976\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:29 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:37 INFO 139837668939584] Epoch[212] Batch[0] avg_epoch_loss=7.036124\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:37 INFO 139837668939584] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=7.03612375259\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:44 INFO 139837668939584] Epoch[212] Batch[5] avg_epoch_loss=6.996673\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=6.99667270978\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:44 INFO 139837668939584] Epoch[212] Batch [5]#011Speed: 44.19 samples/sec#011loss=6.996673\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:50 INFO 139837668939584] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21049.77297782898, \"sum\": 21049.77297782898, \"min\": 21049.77297782898}}, \"EndTime\": 1579314050.784049, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314029.733972}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:50 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6914142539 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=212, train loss <loss>=7.00084719658\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:50 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:58 INFO 139837668939584] Epoch[213] Batch[0] avg_epoch_loss=7.051489\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:20:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=7.05148887634\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:06 INFO 139837668939584] Epoch[213] Batch[5] avg_epoch_loss=6.981608\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=6.98160823186\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:06 INFO 139837668939584] Epoch[213] Batch [5]#011Speed: 44.29 samples/sec#011loss=6.981608\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] Epoch[213] Batch[10] avg_epoch_loss=6.977720\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=213, batch=10 train loss <loss>=6.97305488586\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] Epoch[213] Batch [10]#011Speed: 42.36 samples/sec#011loss=6.973055\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22865.241050720215, \"sum\": 22865.241050720215, \"min\": 22865.241050720215}}, \"EndTime\": 1579314073.649648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314050.784106}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6956121888 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=213, train loss <loss>=6.97772034732\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:13 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:21 INFO 139837668939584] Epoch[214] Batch[0] avg_epoch_loss=7.029218\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=7.02921819687\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:28 INFO 139837668939584] Epoch[214] Batch[5] avg_epoch_loss=7.038023\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=7.03802251816\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:28 INFO 139837668939584] Epoch[214] Batch [5]#011Speed: 45.25 samples/sec#011loss=7.038023\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] Epoch[214] Batch[10] avg_epoch_loss=7.120739\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=214, batch=10 train loss <loss>=7.21999883652\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] Epoch[214] Batch [10]#011Speed: 42.34 samples/sec#011loss=7.219999\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22424.36385154724, \"sum\": 22424.36385154724, \"min\": 22424.36385154724}}, \"EndTime\": 1579314096.074358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314073.649709}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.0308047362 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=214, train loss <loss>=7.1207390265\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:36 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:43 INFO 139837668939584] Epoch[215] Batch[0] avg_epoch_loss=7.069734\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=7.06973409653\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:50 INFO 139837668939584] Epoch[215] Batch[5] avg_epoch_loss=7.097751\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=7.09775137901\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:50 INFO 139837668939584] Epoch[215] Batch [5]#011Speed: 44.80 samples/sec#011loss=7.097751\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:56 INFO 139837668939584] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20652.320861816406, \"sum\": 20652.320861816406, \"min\": 20652.320861816406}}, \"EndTime\": 1579314116.727059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314096.074421}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:56 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.4396491407 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:56 INFO 139837668939584] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:56 INFO 139837668939584] #quality_metric: host=algo-1, epoch=215, train loss <loss>=7.23131895065\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:21:56 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:04 INFO 139837668939584] Epoch[216] Batch[0] avg_epoch_loss=7.360229\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=7.36022853851\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:11 INFO 139837668939584] Epoch[216] Batch[5] avg_epoch_loss=7.325508\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=7.32550795873\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:11 INFO 139837668939584] Epoch[216] Batch [5]#011Speed: 45.08 samples/sec#011loss=7.325508\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] Epoch[216] Batch[10] avg_epoch_loss=7.261540\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=7.18477745056\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] Epoch[216] Batch [10]#011Speed: 42.69 samples/sec#011loss=7.184777\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22463.78493309021, \"sum\": 22463.78493309021, \"min\": 22463.78493309021}}, \"EndTime\": 1579314139.191227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314116.727127}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.2914681754 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=216, train loss <loss>=7.26153954593\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:19 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:26 INFO 139837668939584] Epoch[217] Batch[0] avg_epoch_loss=7.230807\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=7.23080682755\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:34 INFO 139837668939584] Epoch[217] Batch[5] avg_epoch_loss=7.182372\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:34 INFO 139837668939584] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=7.18237233162\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:34 INFO 139837668939584] Epoch[217] Batch [5]#011Speed: 44.51 samples/sec#011loss=7.182372\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:40 INFO 139837668939584] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20980.12900352478, \"sum\": 20980.12900352478, \"min\": 20980.12900352478}}, \"EndTime\": 1579314160.171706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314139.191292}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:40 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.1236259321 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:40 INFO 139837668939584] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=217, train loss <loss>=7.17356009483\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:40 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:47 INFO 139837668939584] Epoch[218] Batch[0] avg_epoch_loss=7.149879\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=7.14987850189\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:54 INFO 139837668939584] Epoch[218] Batch[5] avg_epoch_loss=7.184062\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=7.18406224251\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:22:54 INFO 139837668939584] Epoch[218] Batch [5]#011Speed: 44.97 samples/sec#011loss=7.184062\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:01 INFO 139837668939584] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20832.471132278442, \"sum\": 20832.471132278442, \"min\": 20832.471132278442}}, \"EndTime\": 1579314181.004494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314160.171761}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0971085037 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=218, train loss <loss>=7.16028008461\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:08 INFO 139837668939584] Epoch[219] Batch[0] avg_epoch_loss=7.112592\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=7.11259174347\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:16 INFO 139837668939584] Epoch[219] Batch[5] avg_epoch_loss=7.141415\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:16 INFO 139837668939584] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=7.14141527812\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:16 INFO 139837668939584] Epoch[219] Batch [5]#011Speed: 45.04 samples/sec#011loss=7.141415\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] Epoch[219] Batch[10] avg_epoch_loss=7.123552\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=219, batch=10 train loss <loss>=7.10211582184\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] Epoch[219] Batch [10]#011Speed: 42.52 samples/sec#011loss=7.102116\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22600.57282447815, \"sum\": 22600.57282447815, \"min\": 22600.57282447815}}, \"EndTime\": 1579314203.605441, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314181.004562}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.7779060935 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=219, train loss <loss>=7.1235518889\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:23 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:31 INFO 139837668939584] Epoch[220] Batch[0] avg_epoch_loss=7.141422\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=7.14142179489\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:38 INFO 139837668939584] Epoch[220] Batch[5] avg_epoch_loss=7.150138\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=7.15013750394\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:38 INFO 139837668939584] Epoch[220] Batch [5]#011Speed: 45.40 samples/sec#011loss=7.150138\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] Epoch[220] Batch[10] avg_epoch_loss=7.128640\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=220, batch=10 train loss <loss>=7.10284233093\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] Epoch[220] Batch [10]#011Speed: 42.76 samples/sec#011loss=7.102842\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22218.367099761963, \"sum\": 22218.367099761963, \"min\": 22218.367099761963}}, \"EndTime\": 1579314225.824125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314203.605492}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.8498869638 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] #quality_metric: host=algo-1, epoch=220, train loss <loss>=7.12863969803\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:45 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:53 INFO 139837668939584] Epoch[221] Batch[0] avg_epoch_loss=7.109019\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:23:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=7.10901880264\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:00 INFO 139837668939584] Epoch[221] Batch[5] avg_epoch_loss=7.093025\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=7.09302504857\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:00 INFO 139837668939584] Epoch[221] Batch [5]#011Speed: 45.37 samples/sec#011loss=7.093025\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] Epoch[221] Batch[10] avg_epoch_loss=7.144329\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=7.20589408875\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] Epoch[221] Batch [10]#011Speed: 42.83 samples/sec#011loss=7.205894\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22222.959995269775, \"sum\": 22222.959995269775, \"min\": 22222.959995269775}}, \"EndTime\": 1579314248.047429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314225.824187}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.1589293178 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=221, train loss <loss>=7.14432915774\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:08 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:15 INFO 139837668939584] Epoch[222] Batch[0] avg_epoch_loss=7.056196\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=7.05619573593\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:22 INFO 139837668939584] Epoch[222] Batch[5] avg_epoch_loss=7.102435\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=7.10243455569\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:22 INFO 139837668939584] Epoch[222] Batch [5]#011Speed: 45.55 samples/sec#011loss=7.102435\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:28 INFO 139837668939584] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20577.264070510864, \"sum\": 20577.264070510864, \"min\": 20577.264070510864}}, \"EndTime\": 1579314268.625026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314248.047481}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:28 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6928231075 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:28 INFO 139837668939584] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=222, train loss <loss>=7.07201104164\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:28 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:36 INFO 139837668939584] Epoch[223] Batch[0] avg_epoch_loss=7.077270\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=7.07727003098\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:43 INFO 139837668939584] Epoch[223] Batch[5] avg_epoch_loss=7.101070\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=7.10107008616\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:43 INFO 139837668939584] Epoch[223] Batch [5]#011Speed: 45.18 samples/sec#011loss=7.101070\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] Epoch[223] Batch[10] avg_epoch_loss=7.052113\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=6.99336509705\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] Epoch[223] Batch [10]#011Speed: 42.72 samples/sec#011loss=6.993365\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22484.54213142395, \"sum\": 22484.54213142395, \"min\": 22484.54213142395}}, \"EndTime\": 1579314291.109948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314268.625096}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.3533994448 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=223, train loss <loss>=7.05211327293\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:51 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:58 INFO 139837668939584] Epoch[224] Batch[0] avg_epoch_loss=7.054412\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:24:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=7.05441188812\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:05 INFO 139837668939584] Epoch[224] Batch[5] avg_epoch_loss=7.061687\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:05 INFO 139837668939584] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=7.06168746948\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:05 INFO 139837668939584] Epoch[224] Batch [5]#011Speed: 45.11 samples/sec#011loss=7.061687\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:11 INFO 139837668939584] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20764.288902282715, \"sum\": 20764.288902282715, \"min\": 20764.288902282715}}, \"EndTime\": 1579314311.874527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314291.109999}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:11 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.5811884283 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:11 INFO 139837668939584] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=224, train loss <loss>=7.08094549179\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:11 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:19 INFO 139837668939584] Epoch[225] Batch[0] avg_epoch_loss=7.022594\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=7.0225944519\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:26 INFO 139837668939584] Epoch[225] Batch[5] avg_epoch_loss=7.068838\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=7.06883811951\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:26 INFO 139837668939584] Epoch[225] Batch [5]#011Speed: 45.73 samples/sec#011loss=7.068838\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:32 INFO 139837668939584] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20669.7039604187, \"sum\": 20669.7039604187, \"min\": 20669.7039604187}}, \"EndTime\": 1579314332.544635, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314311.874593}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:32 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.3341077724 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:32 INFO 139837668939584] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=225, train loss <loss>=7.06813573837\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:32 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:40 INFO 139837668939584] Epoch[226] Batch[0] avg_epoch_loss=7.098734\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=7.09873437881\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:47 INFO 139837668939584] Epoch[226] Batch[5] avg_epoch_loss=7.100861\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=7.10086083412\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:47 INFO 139837668939584] Epoch[226] Batch [5]#011Speed: 45.55 samples/sec#011loss=7.100861\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:53 INFO 139837668939584] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20615.592002868652, \"sum\": 20615.592002868652, \"min\": 20615.592002868652}}, \"EndTime\": 1579314353.160605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314332.544701}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:53 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.0741855478 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:53 INFO 139837668939584] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=226, train loss <loss>=7.09544343948\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:25:53 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:00 INFO 139837668939584] Epoch[227] Batch[0] avg_epoch_loss=7.065507\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:00 INFO 139837668939584] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=7.06550693512\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:08 INFO 139837668939584] Epoch[227] Batch[5] avg_epoch_loss=7.072744\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=7.07274405162\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:08 INFO 139837668939584] Epoch[227] Batch [5]#011Speed: 45.26 samples/sec#011loss=7.072744\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] Epoch[227] Batch[10] avg_epoch_loss=7.032738\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=227, batch=10 train loss <loss>=6.98473167419\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] Epoch[227] Batch [10]#011Speed: 42.57 samples/sec#011loss=6.984732\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22401.65686607361, \"sum\": 22401.65686607361, \"min\": 22401.65686607361}}, \"EndTime\": 1579314375.562632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314353.160671}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.1941594712 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=227, train loss <loss>=7.03273842551\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:15 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:23 INFO 139837668939584] Epoch[228] Batch[0] avg_epoch_loss=7.069615\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:23 INFO 139837668939584] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=7.06961488724\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:30 INFO 139837668939584] Epoch[228] Batch[5] avg_epoch_loss=7.103788\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:30 INFO 139837668939584] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=7.10378774007\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:30 INFO 139837668939584] Epoch[228] Batch [5]#011Speed: 45.67 samples/sec#011loss=7.103788\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:36 INFO 139837668939584] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20598.6111164093, \"sum\": 20598.6111164093, \"min\": 20598.6111164093}}, \"EndTime\": 1579314396.161583, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314375.56269}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:36 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.9047983654 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:36 INFO 139837668939584] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=228, train loss <loss>=7.07701716423\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:36 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:44 INFO 139837668939584] Epoch[229] Batch[0] avg_epoch_loss=7.011606\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:44 INFO 139837668939584] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=7.01160621643\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:51 INFO 139837668939584] Epoch[229] Batch[5] avg_epoch_loss=7.047116\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:51 INFO 139837668939584] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=7.0471162796\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:51 INFO 139837668939584] Epoch[229] Batch [5]#011Speed: 45.22 samples/sec#011loss=7.047116\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] Epoch[229] Batch[10] avg_epoch_loss=7.020948\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=6.98954668045\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] Epoch[229] Batch [10]#011Speed: 42.88 samples/sec#011loss=6.989547\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22544.493198394775, \"sum\": 22544.493198394775, \"min\": 22544.493198394775}}, \"EndTime\": 1579314418.706486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314396.161649}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.4084106665 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] #quality_metric: host=algo-1, epoch=229, train loss <loss>=7.02094827999\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:26:58 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:06 INFO 139837668939584] Epoch[230] Batch[0] avg_epoch_loss=7.009182\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:06 INFO 139837668939584] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=7.00918245316\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:13 INFO 139837668939584] Epoch[230] Batch[5] avg_epoch_loss=7.054400\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:13 INFO 139837668939584] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=7.05439980825\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:13 INFO 139837668939584] Epoch[230] Batch [5]#011Speed: 45.14 samples/sec#011loss=7.054400\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] Epoch[230] Batch[10] avg_epoch_loss=7.056395\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=7.0587896347\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] Epoch[230] Batch [10]#011Speed: 42.32 samples/sec#011loss=7.058790\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22426.389932632446, \"sum\": 22426.389932632446, \"min\": 22426.389932632446}}, \"EndTime\": 1579314441.133193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314418.706538}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.8944204834 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] #quality_metric: host=algo-1, epoch=230, train loss <loss>=7.05639518391\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:21 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:28 INFO 139837668939584] Epoch[231] Batch[0] avg_epoch_loss=7.042755\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:28 INFO 139837668939584] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=7.04275512695\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:35 INFO 139837668939584] Epoch[231] Batch[5] avg_epoch_loss=7.017950\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=7.01794950167\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:35 INFO 139837668939584] Epoch[231] Batch [5]#011Speed: 45.35 samples/sec#011loss=7.017950\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] Epoch[231] Batch[10] avg_epoch_loss=6.997221\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=6.97234592438\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] Epoch[231] Batch [10]#011Speed: 42.51 samples/sec#011loss=6.972346\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22331.095933914185, \"sum\": 22331.095933914185, \"min\": 22331.095933914185}}, \"EndTime\": 1579314463.464643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314441.133252}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.7042511318 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=231, train loss <loss>=6.9972206029\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:43 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:50 INFO 139837668939584] Epoch[232] Batch[0] avg_epoch_loss=7.200867\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=7.20086717606\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:57 INFO 139837668939584] Epoch[232] Batch[5] avg_epoch_loss=7.066042\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=7.06604202588\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:27:57 INFO 139837668939584] Epoch[232] Batch [5]#011Speed: 45.34 samples/sec#011loss=7.066042\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:03 INFO 139837668939584] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20533.87212753296, \"sum\": 20533.87212753296, \"min\": 20533.87212753296}}, \"EndTime\": 1579314483.99886, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314463.464705}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:03 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8042763916 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:03 INFO 139837668939584] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:03 INFO 139837668939584] #quality_metric: host=algo-1, epoch=232, train loss <loss>=7.05061497688\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:03 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:11 INFO 139837668939584] Epoch[233] Batch[0] avg_epoch_loss=7.066637\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:11 INFO 139837668939584] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=7.06663703918\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:18 INFO 139837668939584] Epoch[233] Batch[5] avg_epoch_loss=7.057748\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:18 INFO 139837668939584] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=7.05774776141\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:18 INFO 139837668939584] Epoch[233] Batch [5]#011Speed: 45.00 samples/sec#011loss=7.057748\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:24 INFO 139837668939584] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20808.499097824097, \"sum\": 20808.499097824097, \"min\": 20808.499097824097}}, \"EndTime\": 1579314504.80773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314483.998924}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:24 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.8914855346 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:24 INFO 139837668939584] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=233, train loss <loss>=7.05621256828\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:24 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:32 INFO 139837668939584] Epoch[234] Batch[0] avg_epoch_loss=6.982336\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=6.98233604431\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:40 INFO 139837668939584] Epoch[234] Batch[5] avg_epoch_loss=7.019159\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:40 INFO 139837668939584] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=7.01915947596\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:40 INFO 139837668939584] Epoch[234] Batch [5]#011Speed: 44.58 samples/sec#011loss=7.019159\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] Epoch[234] Batch[10] avg_epoch_loss=7.014409\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=234, batch=10 train loss <loss>=7.00870876312\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] Epoch[234] Batch [10]#011Speed: 41.82 samples/sec#011loss=7.008709\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22878.072023391724, \"sum\": 22878.072023391724, \"min\": 22878.072023391724}}, \"EndTime\": 1579314527.686188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314504.8078}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.6789654543 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] #quality_metric: host=algo-1, epoch=234, train loss <loss>=7.01440915194\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:47 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:55 INFO 139837668939584] Epoch[235] Batch[0] avg_epoch_loss=6.928212\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:28:55 INFO 139837668939584] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=6.92821216583\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:02 INFO 139837668939584] Epoch[235] Batch[5] avg_epoch_loss=7.022163\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=7.02216275533\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:02 INFO 139837668939584] Epoch[235] Batch [5]#011Speed: 44.33 samples/sec#011loss=7.022163\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] Epoch[235] Batch[10] avg_epoch_loss=7.073043\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=235, batch=10 train loss <loss>=7.13409996033\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] Epoch[235] Batch [10]#011Speed: 42.04 samples/sec#011loss=7.134100\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22721.601009368896, \"sum\": 22721.601009368896, \"min\": 22721.601009368896}}, \"EndTime\": 1579314550.408143, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314527.686246}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.8710831865 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] #quality_metric: host=algo-1, epoch=235, train loss <loss>=7.07304330306\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:10 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:17 INFO 139837668939584] Epoch[236] Batch[0] avg_epoch_loss=7.106737\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=7.10673666\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:25 INFO 139837668939584] Epoch[236] Batch[5] avg_epoch_loss=7.091017\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:25 INFO 139837668939584] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=7.09101700783\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:25 INFO 139837668939584] Epoch[236] Batch [5]#011Speed: 44.39 samples/sec#011loss=7.091017\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:31 INFO 139837668939584] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20792.036056518555, \"sum\": 20792.036056518555, \"min\": 20792.036056518555}}, \"EndTime\": 1579314571.200536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314550.408208}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:31 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.2418319963 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:31 INFO 139837668939584] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:31 INFO 139837668939584] #quality_metric: host=algo-1, epoch=236, train loss <loss>=7.09838476181\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:31 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:39 INFO 139837668939584] Epoch[237] Batch[0] avg_epoch_loss=7.062487\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:39 INFO 139837668939584] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=7.0624871254\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:46 INFO 139837668939584] Epoch[237] Batch[5] avg_epoch_loss=7.068264\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=7.06826400757\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:46 INFO 139837668939584] Epoch[237] Batch [5]#011Speed: 44.28 samples/sec#011loss=7.068264\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] Epoch[237] Batch[10] avg_epoch_loss=7.087377\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=7.11031246185\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] Epoch[237] Batch [10]#011Speed: 41.79 samples/sec#011loss=7.110312\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22816.39814376831, \"sum\": 22816.39814376831, \"min\": 22816.39814376831}}, \"EndTime\": 1579314594.017309, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314571.200602}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.3647295408 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] #quality_metric: host=algo-1, epoch=237, train loss <loss>=7.08737694133\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:29:54 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:02 INFO 139837668939584] Epoch[238] Batch[0] avg_epoch_loss=7.055175\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:02 INFO 139837668939584] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=7.05517530441\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:09 INFO 139837668939584] Epoch[238] Batch[5] avg_epoch_loss=7.036184\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:09 INFO 139837668939584] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=7.03618415197\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:09 INFO 139837668939584] Epoch[238] Batch [5]#011Speed: 43.54 samples/sec#011loss=7.036184\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] Epoch[238] Batch[10] avg_epoch_loss=7.046049\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=238, batch=10 train loss <loss>=7.05788679123\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] Epoch[238] Batch [10]#011Speed: 41.34 samples/sec#011loss=7.057887\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23225.633144378662, \"sum\": 23225.633144378662, \"min\": 23225.633144378662}}, \"EndTime\": 1579314617.243285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314594.017371}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.7084376016 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] #quality_metric: host=algo-1, epoch=238, train loss <loss>=7.046048988\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:17 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:24 INFO 139837668939584] Epoch[239] Batch[0] avg_epoch_loss=6.911030\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:24 INFO 139837668939584] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=6.91102981567\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:32 INFO 139837668939584] Epoch[239] Batch[5] avg_epoch_loss=7.011897\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:32 INFO 139837668939584] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=7.01189661026\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:32 INFO 139837668939584] Epoch[239] Batch [5]#011Speed: 43.94 samples/sec#011loss=7.011897\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:38 INFO 139837668939584] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21101.8168926239, \"sum\": 21101.8168926239, \"min\": 21101.8168926239}}, \"EndTime\": 1579314638.345454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314617.243349}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:38 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.5707927921 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:38 INFO 139837668939584] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:38 INFO 139837668939584] #quality_metric: host=algo-1, epoch=239, train loss <loss>=7.01514787674\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:38 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:46 INFO 139837668939584] Epoch[240] Batch[0] avg_epoch_loss=6.944886\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:46 INFO 139837668939584] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=6.94488573074\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:53 INFO 139837668939584] Epoch[240] Batch[5] avg_epoch_loss=7.066100\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:53 INFO 139837668939584] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=7.06610027949\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:30:53 INFO 139837668939584] Epoch[240] Batch [5]#011Speed: 44.01 samples/sec#011loss=7.066100\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] Epoch[240] Batch[10] avg_epoch_loss=7.018792\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=240, batch=10 train loss <loss>=6.96202259064\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] Epoch[240] Batch [10]#011Speed: 41.22 samples/sec#011loss=6.962023\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22842.117071151733, \"sum\": 22842.117071151733, \"min\": 22842.117071151733}}, \"EndTime\": 1579314661.187993, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314638.345512}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.0620844146 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] #quality_metric: host=algo-1, epoch=240, train loss <loss>=7.0187922391\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:01 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:08 INFO 139837668939584] Epoch[241] Batch[0] avg_epoch_loss=7.136539\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:08 INFO 139837668939584] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=7.13653898239\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:15 INFO 139837668939584] Epoch[241] Batch[5] avg_epoch_loss=7.043714\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:15 INFO 139837668939584] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=7.04371356964\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:15 INFO 139837668939584] Epoch[241] Batch [5]#011Speed: 44.11 samples/sec#011loss=7.043714\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:22 INFO 139837668939584] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21019.044160842896, \"sum\": 21019.044160842896, \"min\": 21019.044160842896}}, \"EndTime\": 1579314682.207392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314661.188056}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:22 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.1638914062 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:22 INFO 139837668939584] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:22 INFO 139837668939584] #quality_metric: host=algo-1, epoch=241, train loss <loss>=7.06799631119\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:22 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:29 INFO 139837668939584] Epoch[242] Batch[0] avg_epoch_loss=7.022887\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:29 INFO 139837668939584] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=7.02288675308\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:36 INFO 139837668939584] Epoch[242] Batch[5] avg_epoch_loss=6.964683\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:36 INFO 139837668939584] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=6.96468273799\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:36 INFO 139837668939584] Epoch[242] Batch [5]#011Speed: 44.12 samples/sec#011loss=6.964683\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:43 INFO 139837668939584] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21013.592958450317, \"sum\": 21013.592958450317, \"min\": 21013.592958450317}}, \"EndTime\": 1579314703.221367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314682.207461}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:43 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.3142162743 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:43 INFO 139837668939584] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:43 INFO 139837668939584] #quality_metric: host=algo-1, epoch=242, train loss <loss>=6.96630010605\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:43 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:50 INFO 139837668939584] Epoch[243] Batch[0] avg_epoch_loss=7.042165\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=7.04216480255\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:57 INFO 139837668939584] Epoch[243] Batch[5] avg_epoch_loss=6.999870\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:57 INFO 139837668939584] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=6.99987022082\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:31:57 INFO 139837668939584] Epoch[243] Batch [5]#011Speed: 44.23 samples/sec#011loss=6.999870\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:04 INFO 139837668939584] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20961.70997619629, \"sum\": 20961.70997619629, \"min\": 20961.70997619629}}, \"EndTime\": 1579314724.18351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314703.221437}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:04 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=29.4344812648 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:04 INFO 139837668939584] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:04 INFO 139837668939584] #quality_metric: host=algo-1, epoch=243, train loss <loss>=7.02388048172\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:04 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:12 INFO 139837668939584] Epoch[244] Batch[0] avg_epoch_loss=6.941170\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:12 INFO 139837668939584] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=6.94116973877\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:19 INFO 139837668939584] Epoch[244] Batch[5] avg_epoch_loss=7.015602\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:19 INFO 139837668939584] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=7.01560242971\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:19 INFO 139837668939584] Epoch[244] Batch [5]#011Speed: 44.10 samples/sec#011loss=7.015602\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] Epoch[244] Batch[10] avg_epoch_loss=6.983198\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=244, batch=10 train loss <loss>=6.94431314468\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] Epoch[244] Batch [10]#011Speed: 41.83 samples/sec#011loss=6.944313\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22728.517055511475, \"sum\": 22728.517055511475, \"min\": 22728.517055511475}}, \"EndTime\": 1579314746.912408, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314724.183578}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=28.6863132521 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] #quality_metric: host=algo-1, epoch=244, train loss <loss>=6.98319820924\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:26 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:35 INFO 139837668939584] Epoch[245] Batch[0] avg_epoch_loss=6.945891\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:35 INFO 139837668939584] #quality_metric: host=algo-1, epoch=245, batch=0 train loss <loss>=6.94589090347\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:42 INFO 139837668939584] Epoch[245] Batch[5] avg_epoch_loss=7.034556\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:42 INFO 139837668939584] #quality_metric: host=algo-1, epoch=245, batch=5 train loss <loss>=7.03455583254\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:42 INFO 139837668939584] Epoch[245] Batch [5]#011Speed: 44.46 samples/sec#011loss=7.034556\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] Epoch[245] Batch[10] avg_epoch_loss=7.010859\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=245, batch=10 train loss <loss>=6.98242263794\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] Epoch[245] Batch [10]#011Speed: 41.70 samples/sec#011loss=6.982423\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] processed a total of 702 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23118.533849716187, \"sum\": 23118.533849716187, \"min\": 23118.533849716187}}, \"EndTime\": 1579314770.031291, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314746.912472}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] #throughput_metric: host=algo-1, train throughput=30.3651302868 records/second\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] #quality_metric: host=algo-1, epoch=245, train loss <loss>=7.01085892591\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] loss did not improve\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] Loading parameters from best epoch (205)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 148.73194694519043, \"sum\": 148.73194694519043, \"min\": 148.73194694519043}}, \"EndTime\": 1579314770.180415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314770.031351}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] stopping training now\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] Final loss: 6.93354072571 (occurred at epoch 205)\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] #quality_metric: host=algo-1, train final_loss <loss>=6.93354072571\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 WARNING 139837668939584] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 WARNING 139837668939584] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:32:50 INFO 139837668939584] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 198609.20310020447, \"sum\": 198609.20310020447, \"min\": 198609.20310020447}}, \"EndTime\": 1579314968.79026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314770.180487}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:10 INFO 139837668939584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 200027.95600891113, \"sum\": 200027.95600891113, \"min\": 200027.95600891113}}, \"EndTime\": 1579314970.208967, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314968.791368}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:10 INFO 139837668939584] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:10 INFO 139837668939584] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 160.9787940979004, \"sum\": 160.9787940979004, \"min\": 160.9787940979004}}, \"EndTime\": 1579314970.370032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314970.209017}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:10 INFO 139837668939584] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:10 INFO 139837668939584] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.032901763916015625, \"sum\": 0.032901763916015625, \"min\": 0.032901763916015625}}, \"EndTime\": 1579314970.370995, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314970.370078}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 9866.491079330444, \"sum\": 9866.491079330444, \"min\": 9866.491079330444}}, \"EndTime\": 1579314980.237462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314970.371045}\n",
      "\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, RMSE): 268.866326569\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, mean_wQuantileLoss): 0.060121723\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.1]): 0.032049876\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.2]): 0.05016402\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.3]): 0.06332151\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.4]): 0.07182847\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.5]): 0.07551545\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.6]): 0.07534737\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.7]): 0.07072153\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.8]): 0.060663443\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #test_score (algo-1, wQuantileLoss[0.9]): 0.04148385\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.0601217225194\u001b[0m\n",
      "\u001b[34m[01/18/2020 02:36:20 INFO 139837668939584] #quality_metric: host=algo-1, test RMSE <loss>=268.866326569\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 5426552.716970444, \"sum\": 5426552.716970444, \"min\": 5426552.716970444}, \"setuptime\": {\"count\": 1, \"max\": 6.9580078125, \"sum\": 6.9580078125, \"min\": 6.9580078125}}, \"EndTime\": 1579314980.692864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1579314980.237534}\n",
      "\u001b[0m\n",
      "\n",
      "2020-01-18 02:37:02 Uploading - Uploading generated training model\n",
      "2020-01-18 02:37:23 Completed - Training job completed\n",
      "Training seconds: 5528\n",
      "Billable seconds: 5528\n"
     ]
    }
   ],
   "source": [
    "# This step takes around 35 minutes to train the model with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type='ml.t2.medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-080dcb44d6f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.t2.medium'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdeployment_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_job\u001b[0;34m(self, job_name, initial_instance_count, instance_type, deployment_image, name, role, wait, model_environment_vars, vpc_config_override, accelerator_type, data_capture_config)\u001b[0m\n\u001b[1;32m   2669\u001b[0m             \u001b[0mmodel_vpc_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvpc_config_override\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m             \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m         )\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_model_data\u001b[0;34m(self, model_s3_location, deployment_image, initial_instance_count, instance_type, name, role, wait, model_environment_vars, model_vpc_config, accelerator_type, data_capture_config)\u001b[0m\n\u001b[1;32m   2758\u001b[0m             )\n\u001b[1;32m   2759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2760\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2761\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m-> 2338\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m         )\n\u001b[1;32m   2340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.m4.xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::758438347448:role/service-role/AmazonSageMaker-ExecutionRole-20200112T192035'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?E2ed!r\\75#<SD\\B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, encoding, num_samples, quantiles):\n",
    "        \n",
    "        instances = [series_to_obj(ts[k], \n",
    "                                   cat[k] if cat else None,\n",
    "                                   dynamic_feat) \n",
    "                     for k in range(len(ts))]\n",
    "        \n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'endpoint_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-389499b44577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m predictor = DeepARPredictor(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msagemaker_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'endpoint_name' is not defined"
     ]
    }
   ],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
